{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gwo9bpaVgxXF"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qUmV93fif6S"
   },
   "source": [
    "# Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 (CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run multiple experiments with the PG algorithm on the discrete CartPole-v0 environment, using the following commands:\n",
    "* The best configuration of CartPole in both the large and small batch cases should converge to a maximum score of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='CartPole-v0', exp_name='cartpole', n_iter=100, use_reward_to_go=False, use_baseline=False, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=False, batch_size=1000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp1\\q2_pg_cartpole_CartPole-v0\n",
      "########################\n",
      "Using CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\tensorboardX\\summary.py:153: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  scalar = float(scalar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 29.214284896850586\n",
      "Eval_StdReturn : 16.772335052490234\n",
      "Eval_MaxReturn : 68.0\n",
      "Eval_MinReturn : 10.0\n",
      "Eval_AverageEpLen : 29.214285714285715\n",
      "Train_AverageReturn : 23.0\n",
      "Train_StdReturn : 11.139610290527344\n",
      "Train_MaxReturn : 54.0\n",
      "Train_MinReturn : 10.0\n",
      "Train_AverageEpLen : 23.0\n",
      "Actor Loss : 19.53040313720703\n",
      "Train_EnvstepsSoFar : 1012\n",
      "TimeSinceStart : 0.2225208282470703\n",
      "Initial_DataCollection_AverageReturn : 23.0\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 63.28571319580078\n",
      "Eval_StdReturn : 14.027668952941895\n",
      "Eval_MaxReturn : 88.0\n",
      "Eval_MinReturn : 46.0\n",
      "Eval_AverageEpLen : 63.285714285714285\n",
      "Train_AverageReturn : 80.69230651855469\n",
      "Train_StdReturn : 27.791488647460938\n",
      "Train_MaxReturn : 134.0\n",
      "Train_MinReturn : 50.0\n",
      "Train_AverageEpLen : 80.6923076923077\n",
      "Actor Loss : 48.04361343383789\n",
      "Train_EnvstepsSoFar : 10286\n",
      "TimeSinceStart : 2.3459248542785645\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 72.0\n",
      "Eval_StdReturn : 42.78239440917969\n",
      "Eval_MaxReturn : 167.0\n",
      "Eval_MinReturn : 44.0\n",
      "Eval_AverageEpLen : 72.0\n",
      "Train_AverageReturn : 73.78571319580078\n",
      "Train_StdReturn : 33.939395904541016\n",
      "Train_MaxReturn : 160.0\n",
      "Train_MinReturn : 34.0\n",
      "Train_AverageEpLen : 73.78571428571429\n",
      "Actor Loss : 35.90528106689453\n",
      "Train_EnvstepsSoFar : 20617\n",
      "TimeSinceStart : 4.639852523803711\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 136.6666717529297\n",
      "Eval_StdReturn : 27.53583335876465\n",
      "Eval_MaxReturn : 160.0\n",
      "Eval_MinReturn : 98.0\n",
      "Eval_AverageEpLen : 136.66666666666666\n",
      "Train_AverageReturn : 128.875\n",
      "Train_StdReturn : 43.99271774291992\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 64.0\n",
      "Train_AverageEpLen : 128.875\n",
      "Actor Loss : 44.17494201660156\n",
      "Train_EnvstepsSoFar : 30951\n",
      "TimeSinceStart : 6.961863994598389\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 138.3333282470703\n",
      "Eval_StdReturn : 9.392668724060059\n",
      "Eval_MaxReturn : 150.0\n",
      "Eval_MinReturn : 127.0\n",
      "Eval_AverageEpLen : 138.33333333333334\n",
      "Train_AverageReturn : 127.875\n",
      "Train_StdReturn : 11.072912216186523\n",
      "Train_MaxReturn : 149.0\n",
      "Train_MinReturn : 118.0\n",
      "Train_AverageEpLen : 127.875\n",
      "Actor Loss : 37.04319381713867\n",
      "Train_EnvstepsSoFar : 41735\n",
      "TimeSinceStart : 9.458765983581543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 130.5\n",
      "Eval_StdReturn : 8.986100196838379\n",
      "Eval_MaxReturn : 143.0\n",
      "Eval_MinReturn : 121.0\n",
      "Eval_AverageEpLen : 130.5\n",
      "Train_AverageReturn : 125.25\n",
      "Train_StdReturn : 9.161741256713867\n",
      "Train_MaxReturn : 137.0\n",
      "Train_MinReturn : 112.0\n",
      "Train_AverageEpLen : 125.25\n",
      "Actor Loss : 33.48737335205078\n",
      "Train_EnvstepsSoFar : 52324\n",
      "TimeSinceStart : 11.955652952194214\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 140.6666717529297\n",
      "Eval_StdReturn : 9.428091049194336\n",
      "Eval_MaxReturn : 154.0\n",
      "Eval_MinReturn : 134.0\n",
      "Eval_AverageEpLen : 140.66666666666666\n",
      "Train_AverageReturn : 159.42857360839844\n",
      "Train_StdReturn : 27.58289909362793\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 125.0\n",
      "Train_AverageEpLen : 159.42857142857142\n",
      "Actor Loss : 41.190467834472656\n",
      "Train_EnvstepsSoFar : 63126\n",
      "TimeSinceStart : 14.50130558013916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 171.6666717529297\n",
      "Eval_StdReturn : 10.530379295349121\n",
      "Eval_MaxReturn : 186.0\n",
      "Eval_MinReturn : 161.0\n",
      "Eval_AverageEpLen : 171.66666666666666\n",
      "Train_AverageReturn : 169.1666717529297\n",
      "Train_StdReturn : 27.43730354309082\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 125.0\n",
      "Train_AverageEpLen : 169.16666666666666\n",
      "Actor Loss : 40.15699768066406\n",
      "Train_EnvstepsSoFar : 74016\n",
      "TimeSinceStart : 17.41965651512146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 58.85714340209961\n",
      "Eval_StdReturn : 9.341590881347656\n",
      "Eval_MaxReturn : 71.0\n",
      "Eval_MinReturn : 48.0\n",
      "Eval_AverageEpLen : 58.857142857142854\n",
      "Train_AverageReturn : 58.94444274902344\n",
      "Train_StdReturn : 6.637203693389893\n",
      "Train_MaxReturn : 75.0\n",
      "Train_MinReturn : 49.0\n",
      "Train_AverageEpLen : 58.94444444444444\n",
      "Actor Loss : 11.210254669189453\n",
      "Train_EnvstepsSoFar : 84484\n",
      "TimeSinceStart : 20.2043936252594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 79.33333587646484\n",
      "Eval_StdReturn : 17.92267417907715\n",
      "Eval_MaxReturn : 104.0\n",
      "Eval_MinReturn : 50.0\n",
      "Eval_AverageEpLen : 79.33333333333333\n",
      "Train_AverageReturn : 61.0\n",
      "Train_StdReturn : 17.553279876708984\n",
      "Train_MaxReturn : 113.0\n",
      "Train_MinReturn : 41.0\n",
      "Train_AverageEpLen : 61.0\n",
      "Actor Loss : 11.777595520019531\n",
      "Train_EnvstepsSoFar : 94729\n",
      "TimeSinceStart : 22.820165634155273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 151.3333282470703\n",
      "Eval_StdReturn : 7.542471885681152\n",
      "Eval_MaxReturn : 162.0\n",
      "Eval_MinReturn : 146.0\n",
      "Eval_AverageEpLen : 151.33333333333334\n",
      "Train_AverageReturn : 142.875\n",
      "Train_StdReturn : 24.353836059570312\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 125.0\n",
      "Train_AverageEpLen : 142.875\n",
      "Actor Loss : 23.615367889404297\n",
      "Train_EnvstepsSoFar : 105202\n",
      "TimeSinceStart : 25.184083938598633\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use %run to show real-time outputs on the Jupyter\n",
    "%run run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 \\\n",
    "    --exp_name cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='CartPole-v0', exp_name='cartpole_rtg', n_iter=100, use_reward_to_go=True, use_baseline=False, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=False, batch_size=1000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp1\\q2_pg_cartpole_rtg_CartPole-v0\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 31.615385055541992\n",
      "Eval_StdReturn : 15.153260231018066\n",
      "Eval_MaxReturn : 67.0\n",
      "Eval_MinReturn : 13.0\n",
      "Eval_AverageEpLen : 31.615384615384617\n",
      "Train_AverageReturn : 27.94444465637207\n",
      "Train_StdReturn : 14.94423770904541\n",
      "Train_MaxReturn : 77.0\n",
      "Train_MinReturn : 10.0\n",
      "Train_AverageEpLen : 27.944444444444443\n",
      "Actor Loss : 12.6674165725708\n",
      "Train_EnvstepsSoFar : 1006\n",
      "TimeSinceStart : 0.21887993812561035\n",
      "Initial_DataCollection_AverageReturn : 27.94444465637207\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 109.0\n",
      "Eval_StdReturn : 54.0509033203125\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 65.0\n",
      "Eval_AverageEpLen : 109.0\n",
      "Train_AverageReturn : 86.25\n",
      "Train_StdReturn : 40.472469329833984\n",
      "Train_MaxReturn : 175.0\n",
      "Train_MinReturn : 42.0\n",
      "Train_AverageEpLen : 86.25\n",
      "Actor Loss : 28.6323184967041\n",
      "Train_EnvstepsSoFar : 10513\n",
      "TimeSinceStart : 2.327364206314087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 121.5\n",
      "Eval_StdReturn : 13.238202095031738\n",
      "Eval_MaxReturn : 144.0\n",
      "Eval_MinReturn : 111.0\n",
      "Eval_AverageEpLen : 121.5\n",
      "Train_AverageReturn : 137.0\n",
      "Train_StdReturn : 40.277164459228516\n",
      "Train_MaxReturn : 175.0\n",
      "Train_MinReturn : 49.0\n",
      "Train_AverageEpLen : 137.0\n",
      "Actor Loss : 35.16377639770508\n",
      "Train_EnvstepsSoFar : 21246\n",
      "TimeSinceStart : 4.774886131286621\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 41.18832015991211\n",
      "Train_EnvstepsSoFar : 31910\n",
      "TimeSinceStart : 7.119380712509155\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 176.6666717529297\n",
      "Eval_StdReturn : 32.99831771850586\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 130.0\n",
      "Eval_AverageEpLen : 176.66666666666666\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 43.185691833496094\n",
      "Train_EnvstepsSoFar : 41910\n",
      "TimeSinceStart : 9.312055826187134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 188.0\n",
      "Eval_StdReturn : 10.230672836303711\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 175.0\n",
      "Eval_AverageEpLen : 188.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 49.31051254272461\n",
      "Train_EnvstepsSoFar : 52187\n",
      "TimeSinceStart : 11.517633199691772\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 184.0\n",
      "Train_StdReturn : 35.7770881652832\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 104.0\n",
      "Train_AverageEpLen : 184.0\n",
      "Actor Loss : 50.06452560424805\n",
      "Train_EnvstepsSoFar : 62791\n",
      "TimeSinceStart : 13.800406217575073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 51.474483489990234\n",
      "Train_EnvstepsSoFar : 73218\n",
      "TimeSinceStart : 16.032094955444336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 127.0\n",
      "Eval_StdReturn : 8.86002254486084\n",
      "Eval_MaxReturn : 140.0\n",
      "Eval_MinReturn : 115.0\n",
      "Eval_AverageEpLen : 127.0\n",
      "Train_AverageReturn : 102.69999694824219\n",
      "Train_StdReturn : 51.91156005859375\n",
      "Train_MaxReturn : 150.0\n",
      "Train_MinReturn : 19.0\n",
      "Train_AverageEpLen : 102.7\n",
      "Actor Loss : 31.00571632385254\n",
      "Train_EnvstepsSoFar : 83571\n",
      "TimeSinceStart : 18.34874987602234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 126.5\n",
      "Eval_StdReturn : 11.011357307434082\n",
      "Eval_MaxReturn : 137.0\n",
      "Eval_MinReturn : 110.0\n",
      "Eval_AverageEpLen : 126.5\n",
      "Train_AverageReturn : 124.0\n",
      "Train_StdReturn : 7.673909664154053\n",
      "Train_MaxReturn : 138.0\n",
      "Train_MinReturn : 112.0\n",
      "Train_AverageEpLen : 124.0\n",
      "Actor Loss : 23.682373046875\n",
      "Train_EnvstepsSoFar : 94132\n",
      "TimeSinceStart : 20.689682483673096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 189.0\n",
      "Train_StdReturn : 6.757711887359619\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 177.0\n",
      "Train_AverageEpLen : 189.0\n",
      "Actor Loss : 28.907466888427734\n",
      "Train_EnvstepsSoFar : 104953\n",
      "TimeSinceStart : 23.233315229415894\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 \\\n",
    "    -rtg --exp_name cartpole_rtg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='CartPole-v0', exp_name='cartpole_na', n_iter=100, use_reward_to_go=False, use_baseline=False, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=1000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp1\\q2_pg_cartpole_na_CartPole-v0\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 24.058822631835938\n",
      "Eval_StdReturn : 9.824762344360352\n",
      "Eval_MaxReturn : 40.0\n",
      "Eval_MinReturn : 12.0\n",
      "Eval_AverageEpLen : 24.058823529411764\n",
      "Train_AverageReturn : 23.0\n",
      "Train_StdReturn : 14.747880935668945\n",
      "Train_MaxReturn : 88.0\n",
      "Train_MinReturn : 10.0\n",
      "Train_AverageEpLen : 23.0\n",
      "Actor Loss : -0.0034240169916301966\n",
      "Train_EnvstepsSoFar : 1012\n",
      "TimeSinceStart : 0.2817659378051758\n",
      "Initial_DataCollection_AverageReturn : 23.0\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 145.3333282470703\n",
      "Eval_StdReturn : 21.853044509887695\n",
      "Eval_MaxReturn : 174.0\n",
      "Eval_MinReturn : 121.0\n",
      "Eval_AverageEpLen : 145.33333333333334\n",
      "Train_AverageReturn : 81.0\n",
      "Train_StdReturn : 40.867515563964844\n",
      "Train_MaxReturn : 197.0\n",
      "Train_MinReturn : 48.0\n",
      "Train_AverageEpLen : 81.0\n",
      "Actor Loss : -0.0011826815316453576\n",
      "Train_EnvstepsSoFar : 10300\n",
      "TimeSinceStart : 2.593017101287842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 163.0\n",
      "Eval_StdReturn : 48.83304977416992\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 94.0\n",
      "Eval_AverageEpLen : 163.0\n",
      "Train_AverageReturn : 143.0\n",
      "Train_StdReturn : 34.73830795288086\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 86.0\n",
      "Train_AverageEpLen : 143.0\n",
      "Actor Loss : 0.01184160728007555\n",
      "Train_EnvstepsSoFar : 21181\n",
      "TimeSinceStart : 5.014657258987427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 31884\n",
      "TimeSinceStart : 7.359760046005249\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 41884\n",
      "TimeSinceStart : 9.583131074905396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 51884\n",
      "TimeSinceStart : 11.833503246307373\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 61884\n",
      "TimeSinceStart : 14.076549053192139\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 71884\n",
      "TimeSinceStart : 16.28063440322876\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 81884\n",
      "TimeSinceStart : 18.48023271560669\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 91884\n",
      "TimeSinceStart : 20.66122555732727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 101884\n",
      "TimeSinceStart : 22.905576944351196\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 \\\n",
    "    -na --exp_name cartpole_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='CartPole-v0', exp_name='cartpole_rtg_na', n_iter=100, use_reward_to_go=True, use_baseline=False, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=1000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=50, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp1\\q2_pg_cartpole_rtg_na_CartPole-v0\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 32.153846740722656\n",
      "Eval_StdReturn : 16.594003677368164\n",
      "Eval_MaxReturn : 74.0\n",
      "Eval_MinReturn : 10.0\n",
      "Eval_AverageEpLen : 32.15384615384615\n",
      "Train_AverageReturn : 26.743589401245117\n",
      "Train_StdReturn : 13.641410827636719\n",
      "Train_MaxReturn : 63.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 26.743589743589745\n",
      "Actor Loss : -0.003974552266299725\n",
      "Train_EnvstepsSoFar : 1043\n",
      "TimeSinceStart : 0.2501058578491211\n",
      "Initial_DataCollection_AverageReturn : 26.743589401245117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 128.5\n",
      "Eval_StdReturn : 18.821529388427734\n",
      "Eval_MaxReturn : 153.0\n",
      "Eval_MinReturn : 102.0\n",
      "Eval_AverageEpLen : 128.5\n",
      "Train_AverageReturn : 146.0\n",
      "Train_StdReturn : 48.11964416503906\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 77.0\n",
      "Train_AverageEpLen : 146.0\n",
      "Actor Loss : -0.007449491415172815\n",
      "Train_EnvstepsSoFar : 10544\n",
      "TimeSinceStart : 2.6558544635772705\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 178.0\n",
      "Eval_StdReturn : 18.832595825195312\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 154.0\n",
      "Eval_AverageEpLen : 178.0\n",
      "Train_AverageReturn : 191.5\n",
      "Train_StdReturn : 12.024280548095703\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 174.0\n",
      "Train_AverageEpLen : 191.5\n",
      "Actor Loss : -0.010286251083016396\n",
      "Train_EnvstepsSoFar : 21305\n",
      "TimeSinceStart : 5.371530771255493\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 188.1666717529297\n",
      "Train_StdReturn : 16.856420516967773\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 161.0\n",
      "Train_AverageEpLen : 188.16666666666666\n",
      "Actor Loss : 0.021885599941015244\n",
      "Train_EnvstepsSoFar : 32373\n",
      "TimeSinceStart : 8.147447109222412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0009461495792493224\n",
      "Train_EnvstepsSoFar : 42965\n",
      "TimeSinceStart : 10.79515814781189\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 110.0\n",
      "Eval_StdReturn : 5.147815227508545\n",
      "Eval_MaxReturn : 115.0\n",
      "Eval_MinReturn : 102.0\n",
      "Eval_AverageEpLen : 110.0\n",
      "Train_AverageReturn : 104.5\n",
      "Train_StdReturn : 5.08428955078125\n",
      "Train_MaxReturn : 114.0\n",
      "Train_MinReturn : 96.0\n",
      "Train_AverageEpLen : 104.5\n",
      "Actor Loss : -0.026709692552685738\n",
      "Train_EnvstepsSoFar : 53197\n",
      "TimeSinceStart : 13.425541162490845\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.014779928140342236\n",
      "Train_EnvstepsSoFar : 63754\n",
      "TimeSinceStart : 17.60423994064331\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.005195687524974346\n",
      "Train_EnvstepsSoFar : 73754\n",
      "TimeSinceStart : 20.05642056465149\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0025171032175421715\n",
      "Train_EnvstepsSoFar : 83754\n",
      "TimeSinceStart : 22.56710124015808\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0022188301663845778\n",
      "Train_EnvstepsSoFar : 93754\n",
      "TimeSinceStart : 25.021673440933228\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.01834060810506344\n",
      "Train_EnvstepsSoFar : 103913\n",
      "TimeSinceStart : 27.53957772254944\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 \\\n",
    "    -rtg -na --exp_name cartpole_rtg_na \\\n",
    "    --video_log_freq 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='CartPole-v0', exp_name='cartpole_lb', n_iter=100, use_reward_to_go=False, use_baseline=False, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=False, batch_size=4000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp1\\q2_pg_cartpole_lb_CartPole-v0\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 40.400001525878906\n",
      "Eval_StdReturn : 20.47046661376953\n",
      "Eval_MaxReturn : 81.0\n",
      "Eval_MinReturn : 16.0\n",
      "Eval_AverageEpLen : 40.4\n",
      "Train_AverageReturn : 26.375\n",
      "Train_StdReturn : 16.110153198242188\n",
      "Train_MaxReturn : 91.0\n",
      "Train_MinReturn : 9.0\n",
      "Train_AverageEpLen : 26.375\n",
      "Actor Loss : 24.932889938354492\n",
      "Train_EnvstepsSoFar : 4009\n",
      "TimeSinceStart : 0.7180848121643066\n",
      "Initial_DataCollection_AverageReturn : 26.375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 109.75\n",
      "Eval_StdReturn : 23.941333770751953\n",
      "Eval_MaxReturn : 135.0\n",
      "Eval_MinReturn : 82.0\n",
      "Eval_AverageEpLen : 109.75\n",
      "Train_AverageReturn : 71.55357360839844\n",
      "Train_StdReturn : 28.091636657714844\n",
      "Train_MaxReturn : 165.0\n",
      "Train_MinReturn : 32.0\n",
      "Train_AverageEpLen : 71.55357142857143\n",
      "Actor Loss : 44.65800476074219\n",
      "Train_EnvstepsSoFar : 40293\n",
      "TimeSinceStart : 7.0865514278411865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 73.83333587646484\n",
      "Eval_StdReturn : 11.753250122070312\n",
      "Eval_MaxReturn : 90.0\n",
      "Eval_MinReturn : 54.0\n",
      "Eval_AverageEpLen : 73.83333333333333\n",
      "Train_AverageReturn : 115.85713958740234\n",
      "Train_StdReturn : 39.36433792114258\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 62.0\n",
      "Train_AverageEpLen : 115.85714285714286\n",
      "Actor Loss : 60.59613037109375\n",
      "Train_EnvstepsSoFar : 80885\n",
      "TimeSinceStart : 14.560380697250366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 170.0\n",
      "Eval_StdReturn : 42.42640686035156\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 110.0\n",
      "Eval_AverageEpLen : 170.0\n",
      "Train_AverageReturn : 195.42857360839844\n",
      "Train_StdReturn : 20.444046020507812\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 104.0\n",
      "Train_AverageEpLen : 195.42857142857142\n",
      "Actor Loss : 97.49348449707031\n",
      "Train_EnvstepsSoFar : 121454\n",
      "TimeSinceStart : 21.656475067138672\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 196.3333282470703\n",
      "Eval_StdReturn : 5.185449600219727\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 189.0\n",
      "Eval_AverageEpLen : 196.33333333333334\n",
      "Train_AverageReturn : 183.5454559326172\n",
      "Train_StdReturn : 34.07841873168945\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 88.0\n",
      "Train_AverageEpLen : 183.54545454545453\n",
      "Actor Loss : 90.34605407714844\n",
      "Train_EnvstepsSoFar : 162617\n",
      "TimeSinceStart : 28.863497018814087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 82.26315307617188\n",
      "Train_EnvstepsSoFar : 202956\n",
      "TimeSinceStart : 35.98833966255188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 85.69245147705078\n",
      "Train_EnvstepsSoFar : 243858\n",
      "TimeSinceStart : 43.11648178100586\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 194.6666717529297\n",
      "Train_StdReturn : 23.85139274597168\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 88.0\n",
      "Train_AverageEpLen : 194.66666666666666\n",
      "Actor Loss : 86.7912826538086\n",
      "Train_EnvstepsSoFar : 284492\n",
      "TimeSinceStart : 50.132753133773804\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 77.67219543457031\n",
      "Train_EnvstepsSoFar : 325289\n",
      "TimeSinceStart : 57.193007946014404\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 199.09524536132812\n",
      "Train_StdReturn : 2.244671106338501\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 192.0\n",
      "Train_AverageEpLen : 199.0952380952381\n",
      "Actor Loss : 71.53651428222656\n",
      "Train_EnvstepsSoFar : 366428\n",
      "TimeSinceStart : 64.40468215942383\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 67.94456481933594\n",
      "Train_EnvstepsSoFar : 406621\n",
      "TimeSinceStart : 71.35786128044128\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run run_hw2.py --env_name CartPole-v0 -n 100 -b 4000 \\\n",
    "    --exp_name cartpole_lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='CartPole-v0', exp_name='cartpole_lb_rtg', n_iter=100, use_reward_to_go=True, use_baseline=False, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=False, batch_size=4000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp1\\q2_pg_cartpole_lb_rtg_CartPole-v0\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 32.38461685180664\n",
      "Eval_StdReturn : 20.11711311340332\n",
      "Eval_MaxReturn : 92.0\n",
      "Eval_MinReturn : 14.0\n",
      "Eval_AverageEpLen : 32.38461538461539\n",
      "Train_AverageReturn : 22.994253158569336\n",
      "Train_StdReturn : 13.211583137512207\n",
      "Train_MaxReturn : 107.0\n",
      "Train_MinReturn : 8.0\n",
      "Train_AverageEpLen : 22.99425287356322\n",
      "Actor Loss : 10.8725004196167\n",
      "Train_EnvstepsSoFar : 4001\n",
      "TimeSinceStart : 0.7048914432525635\n",
      "Initial_DataCollection_AverageReturn : 22.994253158569336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 144.75\n",
      "Eval_StdReturn : 52.79855728149414\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 60.0\n",
      "Eval_AverageEpLen : 144.75\n",
      "Train_AverageReturn : 144.5357208251953\n",
      "Train_StdReturn : 32.270599365234375\n",
      "Train_MaxReturn : 194.0\n",
      "Train_MinReturn : 56.0\n",
      "Train_AverageEpLen : 144.53571428571428\n",
      "Actor Loss : 42.19361877441406\n",
      "Train_EnvstepsSoFar : 40356\n",
      "TimeSinceStart : 7.144176244735718\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 170.0\n",
      "Eval_StdReturn : 14.352700233459473\n",
      "Eval_MaxReturn : 183.0\n",
      "Eval_MinReturn : 150.0\n",
      "Eval_AverageEpLen : 170.0\n",
      "Train_AverageReturn : 190.31817626953125\n",
      "Train_StdReturn : 25.830541610717773\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 96.0\n",
      "Train_AverageEpLen : 190.3181818181818\n",
      "Actor Loss : 50.65365982055664\n",
      "Train_EnvstepsSoFar : 81095\n",
      "TimeSinceStart : 14.219073295593262\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 193.0\n",
      "Eval_StdReturn : 6.68331241607666\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 184.0\n",
      "Eval_AverageEpLen : 193.0\n",
      "Train_AverageReturn : 177.52174377441406\n",
      "Train_StdReturn : 12.175776481628418\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 156.0\n",
      "Train_AverageEpLen : 177.52173913043478\n",
      "Actor Loss : 44.005393981933594\n",
      "Train_EnvstepsSoFar : 121924\n",
      "TimeSinceStart : 21.38884997367859\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 45.228336334228516\n",
      "Train_EnvstepsSoFar : 162122\n",
      "TimeSinceStart : 28.320945024490356\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 43.30313491821289\n",
      "Train_EnvstepsSoFar : 202122\n",
      "TimeSinceStart : 35.33385181427002\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 196.0\n",
      "Train_StdReturn : 15.234672546386719\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 129.0\n",
      "Train_AverageEpLen : 196.0\n",
      "Actor Loss : 38.67666244506836\n",
      "Train_EnvstepsSoFar : 242898\n",
      "TimeSinceStart : 42.30232357978821\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 176.6086883544922\n",
      "Train_StdReturn : 23.205583572387695\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 126.0\n",
      "Train_AverageEpLen : 176.6086956521739\n",
      "Actor Loss : 35.121131896972656\n",
      "Train_EnvstepsSoFar : 283400\n",
      "TimeSinceStart : 49.26447415351868\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 38.94927978515625\n",
      "Train_EnvstepsSoFar : 323575\n",
      "TimeSinceStart : 56.17115139961243\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 36.59944152832031\n",
      "Train_EnvstepsSoFar : 363734\n",
      "TimeSinceStart : 62.91482186317444\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 199.23809814453125\n",
      "Train_StdReturn : 2.348348379135132\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 192.0\n",
      "Train_AverageEpLen : 199.23809523809524\n",
      "Actor Loss : 33.60546112060547\n",
      "Train_EnvstepsSoFar : 404241\n",
      "TimeSinceStart : 69.2046332359314\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run run_hw2.py --env_name CartPole-v0 -n 100 -b 4000 \\\n",
    "    -rtg --exp_name cartpole_lb_rtg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='CartPole-v0', exp_name='cartpole_lb_na', n_iter=100, use_reward_to_go=False, use_baseline=False, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=4000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp1\\q2_pg_cartpole_lb_na_CartPole-v0\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 38.181819915771484\n",
      "Eval_StdReturn : 26.343252182006836\n",
      "Eval_MaxReturn : 99.0\n",
      "Eval_MinReturn : 14.0\n",
      "Eval_AverageEpLen : 38.18181818181818\n",
      "Train_AverageReturn : 25.737178802490234\n",
      "Train_StdReturn : 13.833308219909668\n",
      "Train_MaxReturn : 75.0\n",
      "Train_MinReturn : 9.0\n",
      "Train_AverageEpLen : 25.737179487179485\n",
      "Actor Loss : -0.00497256126254797\n",
      "Train_EnvstepsSoFar : 4015\n",
      "TimeSinceStart : 0.6288909912109375\n",
      "Initial_DataCollection_AverageReturn : 25.737178802490234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 57.71428680419922\n",
      "Eval_StdReturn : 13.812150955200195\n",
      "Eval_MaxReturn : 75.0\n",
      "Eval_MinReturn : 29.0\n",
      "Eval_AverageEpLen : 57.714285714285715\n",
      "Train_AverageReturn : 81.87754821777344\n",
      "Train_StdReturn : 28.148754119873047\n",
      "Train_MaxReturn : 156.0\n",
      "Train_MinReturn : 35.0\n",
      "Train_AverageEpLen : 81.87755102040816\n",
      "Actor Loss : 0.0017705888021737337\n",
      "Train_EnvstepsSoFar : 40332\n",
      "TimeSinceStart : 6.44861626625061\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 141.0\n",
      "Eval_StdReturn : 10.198039054870605\n",
      "Eval_MaxReturn : 151.0\n",
      "Eval_MinReturn : 127.0\n",
      "Eval_AverageEpLen : 141.0\n",
      "Train_AverageReturn : 174.2608642578125\n",
      "Train_StdReturn : 27.818069458007812\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 117.0\n",
      "Train_AverageEpLen : 174.2608695652174\n",
      "Actor Loss : -0.019639067351818085\n",
      "Train_EnvstepsSoFar : 80727\n",
      "TimeSinceStart : 12.859955549240112\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 121535\n",
      "TimeSinceStart : 19.31067967414856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 161535\n",
      "TimeSinceStart : 25.57344913482666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 201535\n",
      "TimeSinceStart : 31.903966426849365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 241535\n",
      "TimeSinceStart : 38.193856716156006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 281535\n",
      "TimeSinceStart : 44.4728639125824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 321535\n",
      "TimeSinceStart : 50.68710160255432\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 361535\n",
      "TimeSinceStart : 57.005982637405396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0\n",
      "Train_EnvstepsSoFar : 401535\n",
      "TimeSinceStart : 63.29997658729553\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run run_hw2.py --env_name CartPole-v0 -n 100 -b 4000 \\\n",
    "    -na --exp_name cartpole_lb_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='CartPole-v0', exp_name='cartpole_lb_rtg_na', n_iter=100, use_reward_to_go=True, use_baseline=False, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=4000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=50, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp1\\q2_pg_cartpole_lb_rtg_na_CartPole-v0\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 24.235294342041016\n",
      "Eval_StdReturn : 11.026862144470215\n",
      "Eval_MaxReturn : 62.0\n",
      "Eval_MinReturn : 12.0\n",
      "Eval_AverageEpLen : 24.235294117647058\n",
      "Train_AverageReturn : 23.58823585510254\n",
      "Train_StdReturn : 13.4035062789917\n",
      "Train_MaxReturn : 124.0\n",
      "Train_MinReturn : 8.0\n",
      "Train_AverageEpLen : 23.58823529411765\n",
      "Actor Loss : -0.004336353857070208\n",
      "Train_EnvstepsSoFar : 4010\n",
      "TimeSinceStart : 0.6288118362426758\n",
      "Initial_DataCollection_AverageReturn : 23.58823585510254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 171.6666717529297\n",
      "Eval_StdReturn : 0.471404492855072\n",
      "Eval_MaxReturn : 172.0\n",
      "Eval_MinReturn : 171.0\n",
      "Eval_AverageEpLen : 171.66666666666666\n",
      "Train_AverageReturn : 154.8076934814453\n",
      "Train_StdReturn : 41.7676887512207\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 47.0\n",
      "Train_AverageEpLen : 154.80769230769232\n",
      "Actor Loss : -0.010071922093629837\n",
      "Train_EnvstepsSoFar : 40341\n",
      "TimeSinceStart : 6.397403240203857\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 197.14285278320312\n",
      "Train_StdReturn : 12.777531623840332\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 140.0\n",
      "Train_AverageEpLen : 197.14285714285714\n",
      "Actor Loss : -0.0038214107044041157\n",
      "Train_EnvstepsSoFar : 81494\n",
      "TimeSinceStart : 12.879552602767944\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.004445735365152359\n",
      "Train_EnvstepsSoFar : 121690\n",
      "TimeSinceStart : 19.127530574798584\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 197.8095245361328\n",
      "Train_StdReturn : 9.358567237854004\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 156.0\n",
      "Train_AverageEpLen : 197.8095238095238\n",
      "Actor Loss : 0.010497622191905975\n",
      "Train_EnvstepsSoFar : 162235\n",
      "TimeSinceStart : 25.471891164779663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 195.4761962890625\n",
      "Train_StdReturn : 11.561921119689941\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 156.0\n",
      "Train_AverageEpLen : 195.47619047619048\n",
      "Actor Loss : 0.004506790079176426\n",
      "Train_EnvstepsSoFar : 202725\n",
      "TimeSinceStart : 31.784491777420044\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 186.0\n",
      "Eval_StdReturn : 11.430952072143555\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 172.0\n",
      "Eval_AverageEpLen : 186.0\n",
      "Train_AverageReturn : 199.8095245361328\n",
      "Train_StdReturn : 0.8518353700637817\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 196.0\n",
      "Train_AverageEpLen : 199.8095238095238\n",
      "Actor Loss : -0.018769513815641403\n",
      "Train_EnvstepsSoFar : 243844\n",
      "TimeSinceStart : 41.185955286026\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.009960129857063293\n",
      "Train_EnvstepsSoFar : 284171\n",
      "TimeSinceStart : 47.59969663619995\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 196.14285278320312\n",
      "Train_StdReturn : 11.585236549377441\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 147.0\n",
      "Train_AverageEpLen : 196.14285714285714\n",
      "Actor Loss : 0.010073916055262089\n",
      "Train_EnvstepsSoFar : 325139\n",
      "TimeSinceStart : 54.091280460357666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0021275749895721674\n",
      "Train_EnvstepsSoFar : 366135\n",
      "TimeSinceStart : 60.48264503479004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0003841252182610333\n",
      "Train_EnvstepsSoFar : 407127\n",
      "TimeSinceStart : 66.89242482185364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run run_hw2.py --env_name CartPole-v0 -n 100 -b 4000 \\\n",
    "    -rtg -na --exp_name cartpole_lb_rtg_na \\\n",
    "    --video_log_freq 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Answer the following questions briefly:\n",
    "\n",
    "    * Which value estimator has better performance without advantage normalization: the trajectorycentric one, or the one using reward-to-go?  \n",
    "    The one using reward-to-go.\n",
    "    * Did advantage normalization help?   \n",
    "    Yes, normalization stabilizes the training.\n",
    "    * Did the batch size make an impact?  \n",
    "    Yes, the settings with the larger batch size perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "km7LlYvhqKTl"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../../run_logs/exp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Neural Network Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 (HalfCheetah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will use your baselined policy gradient implementation to learn a controller for HalfCheetah-v4. Run the following commands:\n",
    "* You should expect to achieve an average return over 300 forthe baselined version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='HalfCheetah-v4', exp_name='cheetah', n_iter=100, use_reward_to_go=True, use_baseline=False, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=False, batch_size=5000, eval_batch_size=400, discount=0.95, learning_rate=0.01, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp2\\q2_pg_cheetah_HalfCheetah-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -827.1619262695312\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -827.1619262695312\n",
      "Eval_MinReturn : -827.1619262695312\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -703.1766357421875\n",
      "Train_StdReturn : 91.4741439819336\n",
      "Train_MaxReturn : -540.0187377929688\n",
      "Train_MinReturn : -795.40185546875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -118.6458969116211\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 1.4971425533294678\n",
      "Initial_DataCollection_AverageReturn : -703.1766357421875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -870.347900390625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -870.347900390625\n",
      "Eval_MinReturn : -870.347900390625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -916.8486328125\n",
      "Train_StdReturn : 24.2982120513916\n",
      "Train_MaxReturn : -872.3486328125\n",
      "Train_MinReturn : -944.1043701171875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -148.40171813964844\n",
      "Train_EnvstepsSoFar : 50000\n",
      "TimeSinceStart : 13.155847311019897\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -776.791748046875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -776.791748046875\n",
      "Eval_MinReturn : -776.791748046875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -834.4862060546875\n",
      "Train_StdReturn : 60.71469497680664\n",
      "Train_MaxReturn : -743.9251098632812\n",
      "Train_MinReturn : -923.1229248046875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -132.51913452148438\n",
      "Train_EnvstepsSoFar : 100000\n",
      "TimeSinceStart : 25.839374780654907\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -761.0855712890625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -761.0855712890625\n",
      "Eval_MinReturn : -761.0855712890625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -704.6331176757812\n",
      "Train_StdReturn : 46.45063400268555\n",
      "Train_MaxReturn : -647.0123291015625\n",
      "Train_MinReturn : -758.2212524414062\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -108.9517593383789\n",
      "Train_EnvstepsSoFar : 150000\n",
      "TimeSinceStart : 38.480820655822754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -589.3050537109375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -589.3050537109375\n",
      "Eval_MinReturn : -589.3050537109375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -593.1707763671875\n",
      "Train_StdReturn : 101.27203369140625\n",
      "Train_MaxReturn : -470.05908203125\n",
      "Train_MinReturn : -769.00830078125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -88.5616226196289\n",
      "Train_EnvstepsSoFar : 200000\n",
      "TimeSinceStart : 51.37895941734314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -557.1468505859375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -557.1468505859375\n",
      "Eval_MinReturn : -557.1468505859375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -481.5411682128906\n",
      "Train_StdReturn : 80.0116958618164\n",
      "Train_MaxReturn : -405.74151611328125\n",
      "Train_MinReturn : -619.4041137695312\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -69.2696762084961\n",
      "Train_EnvstepsSoFar : 250000\n",
      "TimeSinceStart : 64.30112433433533\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -429.8017272949219\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -429.8017272949219\n",
      "Eval_MinReturn : -429.8017272949219\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -463.9861755371094\n",
      "Train_StdReturn : 48.410003662109375\n",
      "Train_MaxReturn : -411.00787353515625\n",
      "Train_MinReturn : -538.6485595703125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -63.82777404785156\n",
      "Train_EnvstepsSoFar : 300000\n",
      "TimeSinceStart : 77.22376728057861\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -434.53619384765625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -434.53619384765625\n",
      "Eval_MinReturn : -434.53619384765625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -379.0657653808594\n",
      "Train_StdReturn : 43.58700942993164\n",
      "Train_MaxReturn : -292.7720947265625\n",
      "Train_MinReturn : -410.73529052734375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -50.69688034057617\n",
      "Train_EnvstepsSoFar : 350000\n",
      "TimeSinceStart : 89.88022994995117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -443.5589599609375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -443.5589599609375\n",
      "Eval_MinReturn : -443.5589599609375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -340.97137451171875\n",
      "Train_StdReturn : 36.94642639160156\n",
      "Train_MaxReturn : -273.9456481933594\n",
      "Train_MinReturn : -387.5863952636719\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -43.73338317871094\n",
      "Train_EnvstepsSoFar : 400000\n",
      "TimeSinceStart : 102.52235126495361\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -402.59820556640625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -402.59820556640625\n",
      "Eval_MinReturn : -402.59820556640625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -291.7097473144531\n",
      "Train_StdReturn : 57.369483947753906\n",
      "Train_MaxReturn : -203.85589599609375\n",
      "Train_MinReturn : -368.4624938964844\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -36.61524200439453\n",
      "Train_EnvstepsSoFar : 450000\n",
      "TimeSinceStart : 115.39214658737183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -363.85870361328125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -363.85870361328125\n",
      "Eval_MinReturn : -363.85870361328125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -359.30517578125\n",
      "Train_StdReturn : 42.29823303222656\n",
      "Train_MaxReturn : -301.0439758300781\n",
      "Train_MinReturn : -423.2928161621094\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -42.623687744140625\n",
      "Train_EnvstepsSoFar : 500000\n",
      "TimeSinceStart : 128.04388809204102\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No baseline\n",
    "%run run_hw2.py --env_name HalfCheetah-v4 \\\n",
    "    -n 100 -b 5000 -rtg --discount 0.95 -lr 0.01 \\\n",
    "    --exp_name cheetah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='HalfCheetah-v4', exp_name='cheetah_baseline', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.01, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=False, batch_size=5000, eval_batch_size=400, discount=0.95, learning_rate=0.01, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp2\\q2_pg_cheetah_baseline_HalfCheetah-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -705.6990966796875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -705.6990966796875\n",
      "Eval_MinReturn : -705.6990966796875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -698.0880737304688\n",
      "Train_StdReturn : 128.3528594970703\n",
      "Train_MaxReturn : -593.7640991210938\n",
      "Train_MinReturn : -949.9215087890625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -116.79462432861328\n",
      "Baseline Loss : 193.55795288085938\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 1.3181376457214355\n",
      "Initial_DataCollection_AverageReturn : -698.0880737304688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -642.52294921875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -642.52294921875\n",
      "Eval_MinReturn : -642.52294921875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -783.1107177734375\n",
      "Train_StdReturn : 43.019920349121094\n",
      "Train_MaxReturn : -709.2584228515625\n",
      "Train_MinReturn : -824.659423828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 9.256083488464355\n",
      "Baseline Loss : 27.483600616455078\n",
      "Train_EnvstepsSoFar : 50000\n",
      "TimeSinceStart : 13.212671279907227\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -471.33782958984375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -471.33782958984375\n",
      "Eval_MinReturn : -471.33782958984375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -491.3750915527344\n",
      "Train_StdReturn : 43.70899200439453\n",
      "Train_MaxReturn : -451.0376281738281\n",
      "Train_MinReturn : -567.3719482421875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.7058001756668091\n",
      "Baseline Loss : 17.424579620361328\n",
      "Train_EnvstepsSoFar : 100000\n",
      "TimeSinceStart : 26.17423701286316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -358.3057861328125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -358.3057861328125\n",
      "Eval_MinReturn : -358.3057861328125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -456.023681640625\n",
      "Train_StdReturn : 48.998443603515625\n",
      "Train_MaxReturn : -402.83154296875\n",
      "Train_MinReturn : -525.5819702148438\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 2.7494945526123047\n",
      "Baseline Loss : 23.059703826904297\n",
      "Train_EnvstepsSoFar : 150000\n",
      "TimeSinceStart : 38.99531054496765\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -302.1475830078125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -302.1475830078125\n",
      "Eval_MinReturn : -302.1475830078125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -361.83270263671875\n",
      "Train_StdReturn : 55.878482818603516\n",
      "Train_MaxReturn : -262.9602355957031\n",
      "Train_MinReturn : -420.97259521484375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -4.927827835083008\n",
      "Baseline Loss : 20.806026458740234\n",
      "Train_EnvstepsSoFar : 200000\n",
      "TimeSinceStart : 51.91201090812683\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -112.38858032226562\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -112.38858032226562\n",
      "Eval_MinReturn : -112.38858032226562\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -229.673828125\n",
      "Train_StdReturn : 45.34009552001953\n",
      "Train_MaxReturn : -177.53480529785156\n",
      "Train_MinReturn : -303.14581298828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 5.500471115112305\n",
      "Baseline Loss : 24.943103790283203\n",
      "Train_EnvstepsSoFar : 250000\n",
      "TimeSinceStart : 64.98793077468872\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -15.735481262207031\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -15.735481262207031\n",
      "Eval_MinReturn : -15.735481262207031\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -139.5234375\n",
      "Train_StdReturn : 33.56938934326172\n",
      "Train_MaxReturn : -93.88304138183594\n",
      "Train_MinReturn : -197.03530883789062\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 6.438183784484863\n",
      "Baseline Loss : 12.28654956817627\n",
      "Train_EnvstepsSoFar : 300000\n",
      "TimeSinceStart : 78.15718793869019\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -25.029563903808594\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -25.029563903808594\n",
      "Eval_MinReturn : -25.029563903808594\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -153.3917694091797\n",
      "Train_StdReturn : 119.6971664428711\n",
      "Train_MaxReturn : 51.51282501220703\n",
      "Train_MinReturn : -292.6280517578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -9.244553565979004\n",
      "Baseline Loss : 32.30146408081055\n",
      "Train_EnvstepsSoFar : 350000\n",
      "TimeSinceStart : 91.21743083000183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -3.0292186737060547\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -3.0292186737060547\n",
      "Eval_MinReturn : -3.0292186737060547\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -71.21124267578125\n",
      "Train_StdReturn : 40.0888557434082\n",
      "Train_MaxReturn : 2.822681427001953\n",
      "Train_MinReturn : -107.73365783691406\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -3.8196938037872314\n",
      "Baseline Loss : 21.135679244995117\n",
      "Train_EnvstepsSoFar : 400000\n",
      "TimeSinceStart : 104.34035611152649\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 224.8380584716797\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 224.8380584716797\n",
      "Eval_MinReturn : 224.8380584716797\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 320.73297119140625\n",
      "Train_StdReturn : 95.5526123046875\n",
      "Train_MaxReturn : 444.1305236816406\n",
      "Train_MinReturn : 202.66796875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -2.656303882598877\n",
      "Baseline Loss : 22.845102310180664\n",
      "Train_EnvstepsSoFar : 450000\n",
      "TimeSinceStart : 117.39532971382141\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 337.1311340332031\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 337.1311340332031\n",
      "Eval_MinReturn : 337.1311340332031\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 268.85906982421875\n",
      "Train_StdReturn : 80.93635559082031\n",
      "Train_MaxReturn : 373.27325439453125\n",
      "Train_MinReturn : 149.8053436279297\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 1.1890143156051636\n",
      "Baseline Loss : 21.12622833251953\n",
      "Train_EnvstepsSoFar : 500000\n",
      "TimeSinceStart : 130.1854157447815\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "%run run_hw2.py --env_name HalfCheetah-v4 \\\n",
    "    -n 100 -b 5000 -rtg --discount 0.95 -lr 0.01 \\\n",
    "    --use_baseline -blr 0.01 -bgs 5 --exp_name cheetah_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run another experiment with a decreased number of baseline gradient steps (-bgs) and/or baseline learning rate (-blr). How does this affect (a) the baseline learning curve and (b) the performance of the policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='HalfCheetah-v4', exp_name='cheetah_baseline_bgs=5_blr=0.01', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.01, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=False, batch_size=5000, eval_batch_size=400, discount=0.95, learning_rate=0.01, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp2\\q2_pg_cheetah_baseline_bgs=5_blr=0.01_HalfCheetah-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -791.7724609375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -791.7724609375\n",
      "Eval_MinReturn : -791.7724609375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -625.2374267578125\n",
      "Train_StdReturn : 27.968196868896484\n",
      "Train_MaxReturn : -595.23388671875\n",
      "Train_MinReturn : -678.2005615234375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -105.03706359863281\n",
      "Baseline Loss : 152.4749755859375\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 1.4719874858856201\n",
      "Initial_DataCollection_AverageReturn : -625.2374267578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -837.1446533203125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -837.1446533203125\n",
      "Eval_MinReturn : -837.1446533203125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -927.9500732421875\n",
      "Train_StdReturn : 33.03892517089844\n",
      "Train_MaxReturn : -881.0828247070312\n",
      "Train_MinReturn : -982.5501098632812\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 30.107505798339844\n",
      "Baseline Loss : 32.137054443359375\n",
      "Train_EnvstepsSoFar : 50000\n",
      "TimeSinceStart : 13.053974151611328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -458.4577941894531\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -458.4577941894531\n",
      "Eval_MinReturn : -458.4577941894531\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -535.4243774414062\n",
      "Train_StdReturn : 51.3392219543457\n",
      "Train_MaxReturn : -443.9987487792969\n",
      "Train_MinReturn : -581.7783203125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 7.322734355926514\n",
      "Baseline Loss : 22.071321487426758\n",
      "Train_EnvstepsSoFar : 100000\n",
      "TimeSinceStart : 25.837095975875854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -634.487548828125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -634.487548828125\n",
      "Eval_MinReturn : -634.487548828125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -514.17578125\n",
      "Train_StdReturn : 63.469627380371094\n",
      "Train_MaxReturn : -438.67999267578125\n",
      "Train_MinReturn : -618.9906005859375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -3.7651784420013428\n",
      "Baseline Loss : 33.46122741699219\n",
      "Train_EnvstepsSoFar : 150000\n",
      "TimeSinceStart : 38.749974966049194\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -263.04681396484375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -263.04681396484375\n",
      "Eval_MinReturn : -263.04681396484375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -412.28106689453125\n",
      "Train_StdReturn : 34.11153793334961\n",
      "Train_MaxReturn : -362.0289306640625\n",
      "Train_MinReturn : -450.20050048828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -5.469181060791016\n",
      "Baseline Loss : 16.037763595581055\n",
      "Train_EnvstepsSoFar : 200000\n",
      "TimeSinceStart : 51.597644329071045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -181.75674438476562\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -181.75674438476562\n",
      "Eval_MinReturn : -181.75674438476562\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -221.40115356445312\n",
      "Train_StdReturn : 79.29993438720703\n",
      "Train_MaxReturn : -127.88209533691406\n",
      "Train_MinReturn : -353.92938232421875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -3.9349417686462402\n",
      "Baseline Loss : 24.031835556030273\n",
      "Train_EnvstepsSoFar : 250000\n",
      "TimeSinceStart : 64.77971315383911\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 59.08253860473633\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 59.08253860473633\n",
      "Eval_MinReturn : 59.08253860473633\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -25.19527816772461\n",
      "Train_StdReturn : 36.90827560424805\n",
      "Train_MaxReturn : 34.7972412109375\n",
      "Train_MinReturn : -71.10310363769531\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -1.395694375038147\n",
      "Baseline Loss : 15.263209342956543\n",
      "Train_EnvstepsSoFar : 300000\n",
      "TimeSinceStart : 77.90155792236328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 137.50711059570312\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 137.50711059570312\n",
      "Eval_MinReturn : 137.50711059570312\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 148.48257446289062\n",
      "Train_StdReturn : 42.00783920288086\n",
      "Train_MaxReturn : 209.29661560058594\n",
      "Train_MinReturn : 101.06068420410156\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 6.511630058288574\n",
      "Baseline Loss : 15.372729301452637\n",
      "Train_EnvstepsSoFar : 350000\n",
      "TimeSinceStart : 90.90796446800232\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 220.9165802001953\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 220.9165802001953\n",
      "Eval_MinReturn : 220.9165802001953\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 109.6561508178711\n",
      "Train_StdReturn : 41.321563720703125\n",
      "Train_MaxReturn : 176.84170532226562\n",
      "Train_MinReturn : 56.47323989868164\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.11354886740446091\n",
      "Baseline Loss : 15.157196998596191\n",
      "Train_EnvstepsSoFar : 400000\n",
      "TimeSinceStart : 103.88514161109924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 325.605224609375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 325.605224609375\n",
      "Eval_MinReturn : 325.605224609375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 297.8412780761719\n",
      "Train_StdReturn : 140.4572296142578\n",
      "Train_MaxReturn : 440.9840087890625\n",
      "Train_MinReturn : 76.8423843383789\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.518695056438446\n",
      "Baseline Loss : 25.51409912109375\n",
      "Train_EnvstepsSoFar : 450000\n",
      "TimeSinceStart : 117.09708070755005\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 312.31781005859375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 312.31781005859375\n",
      "Eval_MinReturn : 312.31781005859375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 346.98101806640625\n",
      "Train_StdReturn : 58.44500732421875\n",
      "Train_MaxReturn : 392.42755126953125\n",
      "Train_MinReturn : 231.72547912597656\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.17489498853683472\n",
      "Baseline Loss : 16.622201919555664\n",
      "Train_EnvstepsSoFar : 500000\n",
      "TimeSinceStart : 130.12317180633545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='HalfCheetah-v4', exp_name='cheetah_baseline_bgs=5_blr=0.001', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.001, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=False, batch_size=5000, eval_batch_size=400, discount=0.95, learning_rate=0.01, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp2\\q2_pg_cheetah_baseline_bgs=5_blr=0.001_HalfCheetah-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -756.773193359375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -756.773193359375\n",
      "Eval_MinReturn : -756.773193359375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -671.8757934570312\n",
      "Train_StdReturn : 47.369056701660156\n",
      "Train_MaxReturn : -615.5830688476562\n",
      "Train_MinReturn : -741.5953369140625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -112.6360855102539\n",
      "Baseline Loss : 205.5643310546875\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 1.288975715637207\n",
      "Initial_DataCollection_AverageReturn : -671.8757934570312\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -1053.4478759765625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -1053.4478759765625\n",
      "Eval_MinReturn : -1053.4478759765625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -1128.3382568359375\n",
      "Train_StdReturn : 47.52375411987305\n",
      "Train_MaxReturn : -1068.235595703125\n",
      "Train_MinReturn : -1185.053955078125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -159.29049682617188\n",
      "Baseline Loss : 378.34765625\n",
      "Train_EnvstepsSoFar : 50000\n",
      "TimeSinceStart : 13.192615032196045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -828.3184814453125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -828.3184814453125\n",
      "Eval_MinReturn : -828.3184814453125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -768.5680541992188\n",
      "Train_StdReturn : 37.30990219116211\n",
      "Train_MaxReturn : -723.3182373046875\n",
      "Train_MinReturn : -811.2213745117188\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -71.63971710205078\n",
      "Baseline Loss : 139.90452575683594\n",
      "Train_EnvstepsSoFar : 100000\n",
      "TimeSinceStart : 26.335869789123535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -499.0494384765625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -499.0494384765625\n",
      "Eval_MinReturn : -499.0494384765625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -621.458740234375\n",
      "Train_StdReturn : 97.08397674560547\n",
      "Train_MaxReturn : -498.8814697265625\n",
      "Train_MinReturn : -745.033203125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -19.29022216796875\n",
      "Baseline Loss : 50.91404724121094\n",
      "Train_EnvstepsSoFar : 150000\n",
      "TimeSinceStart : 39.130048751831055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -421.22955322265625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -421.22955322265625\n",
      "Eval_MinReturn : -421.22955322265625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -479.997802734375\n",
      "Train_StdReturn : 40.21150588989258\n",
      "Train_MaxReturn : -421.5068054199219\n",
      "Train_MinReturn : -528.844970703125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 9.42457389831543\n",
      "Baseline Loss : 17.571147918701172\n",
      "Train_EnvstepsSoFar : 200000\n",
      "TimeSinceStart : 52.168041944503784\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -316.99774169921875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -316.99774169921875\n",
      "Eval_MinReturn : -316.99774169921875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -417.77264404296875\n",
      "Train_StdReturn : 45.398441314697266\n",
      "Train_MaxReturn : -349.9853210449219\n",
      "Train_MinReturn : -465.136474609375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 4.033548355102539\n",
      "Baseline Loss : 24.57275390625\n",
      "Train_EnvstepsSoFar : 250000\n",
      "TimeSinceStart : 64.9359450340271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -485.26220703125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -485.26220703125\n",
      "Eval_MinReturn : -485.26220703125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -309.45294189453125\n",
      "Train_StdReturn : 103.0843276977539\n",
      "Train_MaxReturn : -206.44137573242188\n",
      "Train_MinReturn : -500.3223876953125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 6.665632724761963\n",
      "Baseline Loss : 37.5554084777832\n",
      "Train_EnvstepsSoFar : 300000\n",
      "TimeSinceStart : 77.6087486743927\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -297.14886474609375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -297.14886474609375\n",
      "Eval_MinReturn : -297.14886474609375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -153.20944213867188\n",
      "Train_StdReturn : 87.05970001220703\n",
      "Train_MaxReturn : -81.60580444335938\n",
      "Train_MinReturn : -320.0048828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 14.807472229003906\n",
      "Baseline Loss : 37.27091979980469\n",
      "Train_EnvstepsSoFar : 350000\n",
      "TimeSinceStart : 90.5893964767456\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -160.45993041992188\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -160.45993041992188\n",
      "Eval_MinReturn : -160.45993041992188\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -82.08012390136719\n",
      "Train_StdReturn : 39.60193634033203\n",
      "Train_MaxReturn : -27.682323455810547\n",
      "Train_MinReturn : -133.21417236328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 9.536285400390625\n",
      "Baseline Loss : 19.489042282104492\n",
      "Train_EnvstepsSoFar : 400000\n",
      "TimeSinceStart : 103.42563509941101\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 100.85586547851562\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 100.85586547851562\n",
      "Eval_MinReturn : 100.85586547851562\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 61.900787353515625\n",
      "Train_StdReturn : 113.5451431274414\n",
      "Train_MaxReturn : 234.29974365234375\n",
      "Train_MinReturn : -98.11824035644531\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 9.043410301208496\n",
      "Baseline Loss : 39.46778869628906\n",
      "Train_EnvstepsSoFar : 450000\n",
      "TimeSinceStart : 116.391437292099\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 10.37057876586914\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 10.37057876586914\n",
      "Eval_MinReturn : 10.37057876586914\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -23.775127410888672\n",
      "Train_StdReturn : 135.7696075439453\n",
      "Train_MaxReturn : 122.86134338378906\n",
      "Train_MinReturn : -223.14605712890625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 2.2571654319763184\n",
      "Baseline Loss : 46.385597229003906\n",
      "Train_EnvstepsSoFar : 500000\n",
      "TimeSinceStart : 129.268981218338\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='HalfCheetah-v4', exp_name='cheetah_baseline_bgs=3_blr=0.01', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.01, baseline_gradient_steps=3, gae_lambda=None, normalize_advantages=False, batch_size=5000, eval_batch_size=400, discount=0.95, learning_rate=0.01, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp2\\q2_pg_cheetah_baseline_bgs=3_blr=0.01_HalfCheetah-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -677.9522705078125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -677.9522705078125\n",
      "Eval_MinReturn : -677.9522705078125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -657.5802612304688\n",
      "Train_StdReturn : 56.93351745605469\n",
      "Train_MaxReturn : -564.57958984375\n",
      "Train_MinReturn : -732.0308837890625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -109.93037414550781\n",
      "Baseline Loss : 173.39459228515625\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 1.2785420417785645\n",
      "Initial_DataCollection_AverageReturn : -657.5802612304688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -960.724609375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -960.724609375\n",
      "Eval_MinReturn : -960.724609375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -1008.3619384765625\n",
      "Train_StdReturn : 37.32831954956055\n",
      "Train_MaxReturn : -967.0350341796875\n",
      "Train_MinReturn : -1070.7386474609375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -4.4186553955078125\n",
      "Baseline Loss : 30.318378448486328\n",
      "Train_EnvstepsSoFar : 50000\n",
      "TimeSinceStart : 12.93123197555542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -656.1800537109375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -656.1800537109375\n",
      "Eval_MinReturn : -656.1800537109375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -630.4627075195312\n",
      "Train_StdReturn : 42.78825378417969\n",
      "Train_MaxReturn : -566.0333251953125\n",
      "Train_MinReturn : -697.9549560546875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 3.7893242835998535\n",
      "Baseline Loss : 28.179868698120117\n",
      "Train_EnvstepsSoFar : 100000\n",
      "TimeSinceStart : 25.67667031288147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -581.01611328125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -581.01611328125\n",
      "Eval_MinReturn : -581.01611328125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -463.52972412109375\n",
      "Train_StdReturn : 38.91630935668945\n",
      "Train_MaxReturn : -418.50958251953125\n",
      "Train_MinReturn : -522.3837280273438\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 4.438727378845215\n",
      "Baseline Loss : 14.25737190246582\n",
      "Train_EnvstepsSoFar : 150000\n",
      "TimeSinceStart : 38.70416212081909\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -484.80108642578125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -484.80108642578125\n",
      "Eval_MinReturn : -484.80108642578125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -447.17498779296875\n",
      "Train_StdReturn : 90.35542297363281\n",
      "Train_MaxReturn : -321.3446044921875\n",
      "Train_MinReturn : -582.0228271484375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -4.225268363952637\n",
      "Baseline Loss : 33.17988586425781\n",
      "Train_EnvstepsSoFar : 200000\n",
      "TimeSinceStart : 51.752034187316895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -367.07525634765625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -367.07525634765625\n",
      "Eval_MinReturn : -367.07525634765625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -301.9866638183594\n",
      "Train_StdReturn : 67.78974914550781\n",
      "Train_MaxReturn : -197.30670166015625\n",
      "Train_MinReturn : -379.7642517089844\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 5.164990425109863\n",
      "Baseline Loss : 17.530757904052734\n",
      "Train_EnvstepsSoFar : 250000\n",
      "TimeSinceStart : 64.53121042251587\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -259.1786804199219\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -259.1786804199219\n",
      "Eval_MinReturn : -259.1786804199219\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -244.9152069091797\n",
      "Train_StdReturn : 38.821327209472656\n",
      "Train_MaxReturn : -190.95375061035156\n",
      "Train_MinReturn : -305.28143310546875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 1.5539262294769287\n",
      "Baseline Loss : 14.263495445251465\n",
      "Train_EnvstepsSoFar : 300000\n",
      "TimeSinceStart : 77.27397441864014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -136.59963989257812\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -136.59963989257812\n",
      "Eval_MinReturn : -136.59963989257812\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -260.6453857421875\n",
      "Train_StdReturn : 159.96815490722656\n",
      "Train_MaxReturn : -128.4220428466797\n",
      "Train_MinReturn : -574.1744384765625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -9.253030776977539\n",
      "Baseline Loss : 26.67940902709961\n",
      "Train_EnvstepsSoFar : 350000\n",
      "TimeSinceStart : 90.13993310928345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -17.521278381347656\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -17.521278381347656\n",
      "Eval_MinReturn : -17.521278381347656\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 25.368541717529297\n",
      "Train_StdReturn : 67.86038970947266\n",
      "Train_MaxReturn : 146.40269470214844\n",
      "Train_MinReturn : -39.29999923706055\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 8.01949691772461\n",
      "Baseline Loss : 31.74991226196289\n",
      "Train_EnvstepsSoFar : 400000\n",
      "TimeSinceStart : 102.87642478942871\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 98.99491119384766\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 98.99491119384766\n",
      "Eval_MinReturn : 98.99491119384766\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 53.276771545410156\n",
      "Train_StdReturn : 47.019405364990234\n",
      "Train_MaxReturn : 124.75969696044922\n",
      "Train_MinReturn : -19.235118865966797\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 7.237971782684326\n",
      "Baseline Loss : 22.669574737548828\n",
      "Train_EnvstepsSoFar : 450000\n",
      "TimeSinceStart : 115.57012486457825\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 64.89290618896484\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 64.89290618896484\n",
      "Eval_MinReturn : 64.89290618896484\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 84.91851043701172\n",
      "Train_StdReturn : 76.18038177490234\n",
      "Train_MaxReturn : 213.29098510742188\n",
      "Train_MinReturn : 1.6243667602539062\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -1.0113693475723267\n",
      "Baseline Loss : 29.997472763061523\n",
      "Train_EnvstepsSoFar : 500000\n",
      "TimeSinceStart : 128.25992035865784\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='HalfCheetah-v4', exp_name='cheetah_baseline_bgs=3_blr=0.001', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.001, baseline_gradient_steps=3, gae_lambda=None, normalize_advantages=False, batch_size=5000, eval_batch_size=400, discount=0.95, learning_rate=0.01, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp2\\q2_pg_cheetah_baseline_bgs=3_blr=0.001_HalfCheetah-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -727.1510009765625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -727.1510009765625\n",
      "Eval_MinReturn : -727.1510009765625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -677.50830078125\n",
      "Train_StdReturn : 70.7365493774414\n",
      "Train_MaxReturn : -547.7343139648438\n",
      "Train_MinReturn : -734.8107299804688\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -113.31232452392578\n",
      "Baseline Loss : 212.1715545654297\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 1.2753245830535889\n",
      "Initial_DataCollection_AverageReturn : -677.50830078125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -963.4501953125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -963.4501953125\n",
      "Eval_MinReturn : -963.4501953125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -1105.2984619140625\n",
      "Train_StdReturn : 39.117549896240234\n",
      "Train_MaxReturn : -1051.0970458984375\n",
      "Train_MinReturn : -1139.3419189453125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -173.6648406982422\n",
      "Baseline Loss : 455.2456970214844\n",
      "Train_EnvstepsSoFar : 50000\n",
      "TimeSinceStart : 12.90476107597351\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -865.5361328125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -865.5361328125\n",
      "Eval_MinReturn : -865.5361328125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -994.3390502929688\n",
      "Train_StdReturn : 39.32232666015625\n",
      "Train_MaxReturn : -933.5643310546875\n",
      "Train_MinReturn : -1049.2366943359375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -123.48909759521484\n",
      "Baseline Loss : 257.81683349609375\n",
      "Train_EnvstepsSoFar : 100000\n",
      "TimeSinceStart : 25.636659383773804\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -676.7310791015625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -676.7310791015625\n",
      "Eval_MinReturn : -676.7310791015625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -710.2042846679688\n",
      "Train_StdReturn : 51.29244613647461\n",
      "Train_MaxReturn : -613.0142822265625\n",
      "Train_MinReturn : -753.5201416015625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -55.69207000732422\n",
      "Baseline Loss : 89.01068878173828\n",
      "Train_EnvstepsSoFar : 150000\n",
      "TimeSinceStart : 38.53231883049011\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -512.401611328125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -512.401611328125\n",
      "Eval_MinReturn : -512.401611328125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -541.9389038085938\n",
      "Train_StdReturn : 52.45017623901367\n",
      "Train_MaxReturn : -488.66802978515625\n",
      "Train_MinReturn : -619.3900756835938\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -8.53476619720459\n",
      "Baseline Loss : 35.20530700683594\n",
      "Train_EnvstepsSoFar : 200000\n",
      "TimeSinceStart : 51.27650237083435\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -434.832763671875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -434.832763671875\n",
      "Eval_MinReturn : -434.832763671875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -476.7120056152344\n",
      "Train_StdReturn : 69.54795837402344\n",
      "Train_MaxReturn : -425.05108642578125\n",
      "Train_MinReturn : -608.6474609375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 3.189021587371826\n",
      "Baseline Loss : 26.311466217041016\n",
      "Train_EnvstepsSoFar : 250000\n",
      "TimeSinceStart : 64.1121118068695\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -369.12103271484375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -369.12103271484375\n",
      "Eval_MinReturn : -369.12103271484375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -342.7494201660156\n",
      "Train_StdReturn : 48.96753692626953\n",
      "Train_MaxReturn : -255.37376403808594\n",
      "Train_MinReturn : -397.0198974609375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 10.468295097351074\n",
      "Baseline Loss : 27.49980354309082\n",
      "Train_EnvstepsSoFar : 300000\n",
      "TimeSinceStart : 76.92341780662537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -288.3046875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -288.3046875\n",
      "Eval_MinReturn : -288.3046875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -272.1943664550781\n",
      "Train_StdReturn : 64.97360229492188\n",
      "Train_MaxReturn : -202.45069885253906\n",
      "Train_MinReturn : -386.3166809082031\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 17.001178741455078\n",
      "Baseline Loss : 36.09636688232422\n",
      "Train_EnvstepsSoFar : 350000\n",
      "TimeSinceStart : 89.61399245262146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -254.35064697265625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -254.35064697265625\n",
      "Eval_MinReturn : -254.35064697265625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -209.78076171875\n",
      "Train_StdReturn : 64.53612518310547\n",
      "Train_MaxReturn : -120.00070190429688\n",
      "Train_MinReturn : -282.27227783203125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 15.014039039611816\n",
      "Baseline Loss : 29.896106719970703\n",
      "Train_EnvstepsSoFar : 400000\n",
      "TimeSinceStart : 102.55901193618774\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -172.15341186523438\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -172.15341186523438\n",
      "Eval_MinReturn : -172.15341186523438\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -127.40757751464844\n",
      "Train_StdReturn : 89.58589172363281\n",
      "Train_MaxReturn : -46.508758544921875\n",
      "Train_MinReturn : -286.9001159667969\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 17.5856876373291\n",
      "Baseline Loss : 34.44673156738281\n",
      "Train_EnvstepsSoFar : 450000\n",
      "TimeSinceStart : 115.50231146812439\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -115.6950912475586\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -115.6950912475586\n",
      "Eval_MinReturn : -115.6950912475586\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -85.22044372558594\n",
      "Train_StdReturn : 99.52987670898438\n",
      "Train_MaxReturn : 8.956804275512695\n",
      "Train_MinReturn : -276.54595947265625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 12.007326126098633\n",
      "Baseline Loss : 42.06264877319336\n",
      "Train_EnvstepsSoFar : 500000\n",
      "TimeSinceStart : 128.1615390777588\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='HalfCheetah-v4', exp_name='cheetah_baseline_bgs=1_blr=0.01', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.01, baseline_gradient_steps=1, gae_lambda=None, normalize_advantages=False, batch_size=5000, eval_batch_size=400, discount=0.95, learning_rate=0.01, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp2\\q2_pg_cheetah_baseline_bgs=1_blr=0.01_HalfCheetah-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -898.5200805664062\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -898.5200805664062\n",
      "Eval_MinReturn : -898.5200805664062\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -689.1084594726562\n",
      "Train_StdReturn : 36.018733978271484\n",
      "Train_MaxReturn : -638.9786376953125\n",
      "Train_MinReturn : -728.2109985351562\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -115.02156066894531\n",
      "Baseline Loss : 211.7958221435547\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 1.2618522644042969\n",
      "Initial_DataCollection_AverageReturn : -689.1084594726562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -1012.4949951171875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -1012.4949951171875\n",
      "Eval_MinReturn : -1012.4949951171875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -968.65576171875\n",
      "Train_StdReturn : 102.9505615234375\n",
      "Train_MaxReturn : -819.60498046875\n",
      "Train_MinReturn : -1137.6314697265625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -121.61430358886719\n",
      "Baseline Loss : 261.60601806640625\n",
      "Train_EnvstepsSoFar : 50000\n",
      "TimeSinceStart : 12.622417688369751\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -696.30224609375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -696.30224609375\n",
      "Eval_MinReturn : -696.30224609375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -681.5406494140625\n",
      "Train_StdReturn : 74.44773864746094\n",
      "Train_MaxReturn : -534.5462646484375\n",
      "Train_MinReturn : -731.67724609375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.5426251888275146\n",
      "Baseline Loss : 32.54816436767578\n",
      "Train_EnvstepsSoFar : 100000\n",
      "TimeSinceStart : 25.386573314666748\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -497.29229736328125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -497.29229736328125\n",
      "Eval_MinReturn : -497.29229736328125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -500.1123962402344\n",
      "Train_StdReturn : 54.3967399597168\n",
      "Train_MaxReturn : -428.88653564453125\n",
      "Train_MinReturn : -579.7259521484375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 45.04590606689453\n",
      "Baseline Loss : 60.326576232910156\n",
      "Train_EnvstepsSoFar : 150000\n",
      "TimeSinceStart : 38.14021182060242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -456.384521484375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -456.384521484375\n",
      "Eval_MinReturn : -456.384521484375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -461.8191833496094\n",
      "Train_StdReturn : 40.79630661010742\n",
      "Train_MaxReturn : -383.56732177734375\n",
      "Train_MinReturn : -501.32733154296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 25.495344161987305\n",
      "Baseline Loss : 33.18852996826172\n",
      "Train_EnvstepsSoFar : 200000\n",
      "TimeSinceStart : 51.11441969871521\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -421.79425048828125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -421.79425048828125\n",
      "Eval_MinReturn : -421.79425048828125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -349.34527587890625\n",
      "Train_StdReturn : 8.858776092529297\n",
      "Train_MaxReturn : -337.89715576171875\n",
      "Train_MinReturn : -361.50775146484375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 12.469099998474121\n",
      "Baseline Loss : 16.15261459350586\n",
      "Train_EnvstepsSoFar : 250000\n",
      "TimeSinceStart : 63.97847890853882\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -330.44708251953125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -330.44708251953125\n",
      "Eval_MinReturn : -330.44708251953125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -285.3706359863281\n",
      "Train_StdReturn : 121.58685302734375\n",
      "Train_MaxReturn : -154.04652404785156\n",
      "Train_MinReturn : -507.90283203125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -1.1645164489746094\n",
      "Baseline Loss : 30.763887405395508\n",
      "Train_EnvstepsSoFar : 300000\n",
      "TimeSinceStart : 76.6870846748352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -225.80126953125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -225.80126953125\n",
      "Eval_MinReturn : -225.80126953125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -285.77294921875\n",
      "Train_StdReturn : 26.816062927246094\n",
      "Train_MaxReturn : -248.8143768310547\n",
      "Train_MinReturn : -329.2186279296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -2.7316155433654785\n",
      "Baseline Loss : 26.610580444335938\n",
      "Train_EnvstepsSoFar : 350000\n",
      "TimeSinceStart : 89.5967926979065\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -97.0535659790039\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -97.0535659790039\n",
      "Eval_MinReturn : -97.0535659790039\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -117.48149108886719\n",
      "Train_StdReturn : 84.62861633300781\n",
      "Train_MaxReturn : -9.968788146972656\n",
      "Train_MinReturn : -238.6016845703125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 12.288036346435547\n",
      "Baseline Loss : 27.43109703063965\n",
      "Train_EnvstepsSoFar : 400000\n",
      "TimeSinceStart : 102.33958625793457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 18.007572174072266\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 18.007572174072266\n",
      "Eval_MinReturn : 18.007572174072266\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -62.52166748046875\n",
      "Train_StdReturn : 59.82937240600586\n",
      "Train_MaxReturn : 50.28443145751953\n",
      "Train_MinReturn : -127.22261810302734\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 11.813750267028809\n",
      "Baseline Loss : 24.878122329711914\n",
      "Train_EnvstepsSoFar : 450000\n",
      "TimeSinceStart : 114.94913363456726\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 233.71055603027344\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 233.71055603027344\n",
      "Eval_MinReturn : 233.71055603027344\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 94.42462158203125\n",
      "Train_StdReturn : 134.63804626464844\n",
      "Train_MaxReturn : 198.79498291015625\n",
      "Train_MinReturn : -167.4940643310547\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 12.683015823364258\n",
      "Baseline Loss : 63.97441864013672\n",
      "Train_EnvstepsSoFar : 500000\n",
      "TimeSinceStart : 127.63025522232056\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='HalfCheetah-v4', exp_name='cheetah_baseline_bgs=1_blr=0.001', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.001, baseline_gradient_steps=1, gae_lambda=None, normalize_advantages=False, batch_size=5000, eval_batch_size=400, discount=0.95, learning_rate=0.01, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp2\\q2_pg_cheetah_baseline_bgs=1_blr=0.001_HalfCheetah-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -704.0704956054688\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -704.0704956054688\n",
      "Eval_MinReturn : -704.0704956054688\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -693.9966430664062\n",
      "Train_StdReturn : 26.25629425048828\n",
      "Train_MaxReturn : -661.1858520507812\n",
      "Train_MinReturn : -732.95556640625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -115.90302276611328\n",
      "Baseline Loss : 222.08901977539062\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 1.272256851196289\n",
      "Initial_DataCollection_AverageReturn : -693.9966430664062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -1251.3980712890625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -1251.3980712890625\n",
      "Eval_MinReturn : -1251.3980712890625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -1297.0474853515625\n",
      "Train_StdReturn : 117.05533599853516\n",
      "Train_MaxReturn : -1095.031494140625\n",
      "Train_MinReturn : -1420.376220703125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -211.10292053222656\n",
      "Baseline Loss : 693.2552490234375\n",
      "Train_EnvstepsSoFar : 50000\n",
      "TimeSinceStart : 12.662036657333374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -916.7936401367188\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -916.7936401367188\n",
      "Eval_MinReturn : -916.7936401367188\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -829.1171875\n",
      "Train_StdReturn : 63.555397033691406\n",
      "Train_MaxReturn : -727.471435546875\n",
      "Train_MinReturn : -908.9827270507812\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -129.03591918945312\n",
      "Baseline Loss : 300.0794982910156\n",
      "Train_EnvstepsSoFar : 100000\n",
      "TimeSinceStart : 25.62950110435486\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -679.4181518554688\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -679.4181518554688\n",
      "Eval_MinReturn : -679.4181518554688\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -632.6138916015625\n",
      "Train_StdReturn : 153.28074645996094\n",
      "Train_MaxReturn : -339.18585205078125\n",
      "Train_MinReturn : -786.2710571289062\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -91.5959243774414\n",
      "Baseline Loss : 174.47244262695312\n",
      "Train_EnvstepsSoFar : 150000\n",
      "TimeSinceStart : 38.4966516494751\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -764.5408935546875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -764.5408935546875\n",
      "Eval_MinReturn : -764.5408935546875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -586.7794799804688\n",
      "Train_StdReturn : 58.43907165527344\n",
      "Train_MaxReturn : -518.7572021484375\n",
      "Train_MinReturn : -673.4948120117188\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -78.67945861816406\n",
      "Baseline Loss : 146.22044372558594\n",
      "Train_EnvstepsSoFar : 200000\n",
      "TimeSinceStart : 51.26175284385681\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -596.4517822265625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -596.4517822265625\n",
      "Eval_MinReturn : -596.4517822265625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -533.0402221679688\n",
      "Train_StdReturn : 44.947998046875\n",
      "Train_MaxReturn : -481.508544921875\n",
      "Train_MinReturn : -611.2183837890625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -63.256317138671875\n",
      "Baseline Loss : 102.6175765991211\n",
      "Train_EnvstepsSoFar : 250000\n",
      "TimeSinceStart : 64.30641055107117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -569.008056640625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -569.008056640625\n",
      "Eval_MinReturn : -569.008056640625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -561.67333984375\n",
      "Train_StdReturn : 46.5152587890625\n",
      "Train_MaxReturn : -512.2799072265625\n",
      "Train_MinReturn : -619.24658203125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -53.925750732421875\n",
      "Baseline Loss : 79.34735107421875\n",
      "Train_EnvstepsSoFar : 300000\n",
      "TimeSinceStart : 77.50460457801819\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -473.8216857910156\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -473.8216857910156\n",
      "Eval_MinReturn : -473.8216857910156\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -458.1678161621094\n",
      "Train_StdReturn : 63.225677490234375\n",
      "Train_MaxReturn : -343.17144775390625\n",
      "Train_MinReturn : -519.1079711914062\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -29.52790641784668\n",
      "Baseline Loss : 39.50379180908203\n",
      "Train_EnvstepsSoFar : 350000\n",
      "TimeSinceStart : 90.66602730751038\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -248.06097412109375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -248.06097412109375\n",
      "Eval_MinReturn : -248.06097412109375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -378.8773498535156\n",
      "Train_StdReturn : 26.116344451904297\n",
      "Train_MaxReturn : -344.00732421875\n",
      "Train_MinReturn : -424.46331787109375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -10.016514778137207\n",
      "Baseline Loss : 19.105018615722656\n",
      "Train_EnvstepsSoFar : 400000\n",
      "TimeSinceStart : 103.61008310317993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -329.28350830078125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -329.28350830078125\n",
      "Eval_MinReturn : -329.28350830078125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -242.8070526123047\n",
      "Train_StdReturn : 33.12187194824219\n",
      "Train_MaxReturn : -181.04347229003906\n",
      "Train_MinReturn : -279.41595458984375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 14.366820335388184\n",
      "Baseline Loss : 34.731204986572266\n",
      "Train_EnvstepsSoFar : 450000\n",
      "TimeSinceStart : 116.70764517784119\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -139.954833984375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -139.954833984375\n",
      "Eval_MinReturn : -139.954833984375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -234.0897979736328\n",
      "Train_StdReturn : 81.62483215332031\n",
      "Train_MaxReturn : -169.74258422851562\n",
      "Train_MinReturn : -392.14215087890625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 12.487543106079102\n",
      "Baseline Loss : 28.025409698486328\n",
      "Train_EnvstepsSoFar : 500000\n",
      "TimeSinceStart : 129.77079892158508\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "baseline_gradient_steps = [5, 3, 1]\n",
    "baseline_learning_rate = [0.01, 0.001]\n",
    "for bgs in baseline_gradient_steps:\n",
    "    for blr in baseline_learning_rate:\n",
    "        %run run_hw2.py --env_name HalfCheetah-v4 \\\n",
    "            -n 100 -b 5000 -rtg --discount 0.95 -lr 0.01 \\\n",
    "            --use_baseline -blr \"$blr\" -bgs \"$bgs\" --exp_name cheetah_baseline_bgs=\"$bgs\"_blr=\"$blr\"\n",
    "        print('*' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger baseline gradient steps (bgs=5) works better and blr=0.01 works better than blr=0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optional: Add -na back to see how much it improves things. Also, set video_log_freq 10, then open TensorBoard and go to the “Images” tab to see some videos of your HalfCheetah walking along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='HalfCheetah-v4', exp_name='cheetah_baseline_na', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.01, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=5000, eval_batch_size=400, discount=0.95, learning_rate=0.01, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=10, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp2\\q2_pg_cheetah_baseline_na_HalfCheetah-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -728.9951171875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -728.9951171875\n",
      "Eval_MinReturn : -728.9951171875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -673.6901245117188\n",
      "Train_StdReturn : 24.390350341796875\n",
      "Train_MaxReturn : -635.9420776367188\n",
      "Train_MinReturn : -708.2735595703125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.13115186989307404\n",
      "Baseline Loss : 160.03152465820312\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 1.6728346347808838\n",
      "Initial_DataCollection_AverageReturn : -673.6901245117188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -591.3040771484375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -591.3040771484375\n",
      "Eval_MinReturn : -591.3040771484375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -605.8554077148438\n",
      "Train_StdReturn : 30.02361488342285\n",
      "Train_MaxReturn : -580.9853515625\n",
      "Train_MinReturn : -663.1669921875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.11118888854980469\n",
      "Baseline Loss : 20.944007873535156\n",
      "Train_EnvstepsSoFar : 50000\n",
      "TimeSinceStart : 15.433513879776001\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -463.79638671875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -463.79638671875\n",
      "Eval_MinReturn : -463.79638671875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -493.83843994140625\n",
      "Train_StdReturn : 49.7720947265625\n",
      "Train_MaxReturn : -421.5867004394531\n",
      "Train_MinReturn : -547.124755859375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.10297204554080963\n",
      "Baseline Loss : 35.07945251464844\n",
      "Train_EnvstepsSoFar : 100000\n",
      "TimeSinceStart : 65.20294666290283\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -264.0831298828125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -264.0831298828125\n",
      "Eval_MinReturn : -264.0831298828125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -352.489501953125\n",
      "Train_StdReturn : 42.2669677734375\n",
      "Train_MaxReturn : -291.1361083984375\n",
      "Train_MinReturn : -411.1396179199219\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.08103494346141815\n",
      "Baseline Loss : 23.873682022094727\n",
      "Train_EnvstepsSoFar : 150000\n",
      "TimeSinceStart : 112.48579692840576\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -226.51402282714844\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -226.51402282714844\n",
      "Eval_MinReturn : -226.51402282714844\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -200.83505249023438\n",
      "Train_StdReturn : 24.877548217773438\n",
      "Train_MaxReturn : -159.42434692382812\n",
      "Train_MinReturn : -231.79896545410156\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.0982978418469429\n",
      "Baseline Loss : 22.98892593383789\n",
      "Train_EnvstepsSoFar : 200000\n",
      "TimeSinceStart : 158.78312826156616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -108.86298370361328\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -108.86298370361328\n",
      "Eval_MinReturn : -108.86298370361328\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -177.081787109375\n",
      "Train_StdReturn : 68.63737487792969\n",
      "Train_MaxReturn : -53.036834716796875\n",
      "Train_MinReturn : -239.41465759277344\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.07290481775999069\n",
      "Baseline Loss : 29.141075134277344\n",
      "Train_EnvstepsSoFar : 250000\n",
      "TimeSinceStart : 207.39855861663818\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -65.3198471069336\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -65.3198471069336\n",
      "Eval_MinReturn : -65.3198471069336\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -27.452045440673828\n",
      "Train_StdReturn : 53.81197738647461\n",
      "Train_MaxReturn : 77.3953857421875\n",
      "Train_MinReturn : -63.24920654296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.09508415311574936\n",
      "Baseline Loss : 18.025066375732422\n",
      "Train_EnvstepsSoFar : 300000\n",
      "TimeSinceStart : 256.34391593933105\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 301.3961181640625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 301.3961181640625\n",
      "Eval_MinReturn : 301.3961181640625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 135.26541137695312\n",
      "Train_StdReturn : 61.81783676147461\n",
      "Train_MaxReturn : 229.37269592285156\n",
      "Train_MinReturn : 36.379722595214844\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.14305730164051056\n",
      "Baseline Loss : 21.577070236206055\n",
      "Train_EnvstepsSoFar : 350000\n",
      "TimeSinceStart : 305.68849325180054\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 720.794921875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 720.794921875\n",
      "Eval_MinReturn : 720.794921875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 546.2984008789062\n",
      "Train_StdReturn : 249.53424072265625\n",
      "Train_MaxReturn : 947.3818969726562\n",
      "Train_MinReturn : 335.39776611328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.05221967771649361\n",
      "Baseline Loss : 33.92543029785156\n",
      "Train_EnvstepsSoFar : 400000\n",
      "TimeSinceStart : 351.7934904098511\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 507.56646728515625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 507.56646728515625\n",
      "Eval_MinReturn : 507.56646728515625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 495.6241760253906\n",
      "Train_StdReturn : 105.00432586669922\n",
      "Train_MaxReturn : 608.7405395507812\n",
      "Train_MinReturn : 369.07183837890625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.042794935405254364\n",
      "Baseline Loss : 23.339385986328125\n",
      "Train_EnvstepsSoFar : 450000\n",
      "TimeSinceStart : 397.1421067714691\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1038.0233154296875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1038.0233154296875\n",
      "Eval_MinReturn : 1038.0233154296875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 789.4653930664062\n",
      "Train_StdReturn : 330.98590087890625\n",
      "Train_MaxReturn : 1313.172119140625\n",
      "Train_MinReturn : 458.884521484375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.026793276891112328\n",
      "Baseline Loss : 33.2888069152832\n",
      "Train_EnvstepsSoFar : 500000\n",
      "TimeSinceStart : 442.9428391456604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run run_hw2.py --env_name HalfCheetah-v4 \\\n",
    "    -n 100 -b 5000 -rtg --discount 0.95 -lr 0.01 \\\n",
    "    --use_baseline -blr 0.01 -bgs 5 --exp_name cheetah_baseline_na \\\n",
    "    -na --video_log_freq 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../../run_logs/exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Generalized Advantage Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 (LunarLander-v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your implementation of policy gradient with generalized advantage estimation to learn a controller for a version of LunarLander-v2 with noisy actions. Search over λ ∈ [0, 0.95, 0.98, 0.99, 1] to replace <λ> below. **Do not** change any of the other hyperparameters (e.g. batch size, learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='LunarLander-v2', exp_name='lunar_lander_lambda0', n_iter=300, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=0.0, normalize_advantages=False, batch_size=2000, eval_batch_size=400, discount=0.99, learning_rate=0.001, n_layers=3, layer_size=128, ep_len=1000, seed=1, no_gpu=False, which_gpu=0, video_log_freq=100, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp3\\q2_pg_lunar_lander_lambda0_LunarLander-v2\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -276.6845397949219\n",
      "Eval_StdReturn : 136.70455932617188\n",
      "Eval_MaxReturn : -122.50342559814453\n",
      "Eval_MinReturn : -463.65399169921875\n",
      "Eval_AverageEpLen : 95.6\n",
      "Train_AverageReturn : -177.9470672607422\n",
      "Train_StdReturn : 89.45281219482422\n",
      "Train_MaxReturn : -20.923614501953125\n",
      "Train_MinReturn : -393.395751953125\n",
      "Train_AverageEpLen : 81.0\n",
      "Actor Loss : -2.986598253250122\n",
      "Baseline Loss : 11426.2060546875\n",
      "Train_EnvstepsSoFar : 2025\n",
      "TimeSinceStart : 0.6820456981658936\n",
      "Initial_DataCollection_AverageReturn : -177.9470672607422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -26.16534423828125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -26.16534423828125\n",
      "Eval_MinReturn : -26.16534423828125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -200.01824951171875\n",
      "Train_StdReturn : 75.84395599365234\n",
      "Train_MaxReturn : -6.5255889892578125\n",
      "Train_MinReturn : -272.9508056640625\n",
      "Train_AverageEpLen : 242.22222222222223\n",
      "Actor Loss : -0.3391324579715729\n",
      "Baseline Loss : 3779.3515625\n",
      "Train_EnvstepsSoFar : 61926\n",
      "TimeSinceStart : 24.56640601158142\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -198.07064819335938\n",
      "Eval_StdReturn : 71.34634399414062\n",
      "Eval_MaxReturn : -126.72429656982422\n",
      "Eval_MinReturn : -269.4169921875\n",
      "Eval_AverageEpLen : 213.5\n",
      "Train_AverageReturn : -128.70278930664062\n",
      "Train_StdReturn : 100.83245086669922\n",
      "Train_MaxReturn : 68.30927276611328\n",
      "Train_MinReturn : -255.6982879638672\n",
      "Train_AverageEpLen : 294.7142857142857\n",
      "Actor Loss : -0.1364409178495407\n",
      "Baseline Loss : 3370.58056640625\n",
      "Train_EnvstepsSoFar : 127565\n",
      "TimeSinceStart : 67.86595869064331\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -244.89791870117188\n",
      "Eval_StdReturn : 15.5455322265625\n",
      "Eval_MaxReturn : -229.35238647460938\n",
      "Eval_MinReturn : -260.4434509277344\n",
      "Eval_AverageEpLen : 315.0\n",
      "Train_AverageReturn : -79.15706634521484\n",
      "Train_StdReturn : 93.25584411621094\n",
      "Train_MaxReturn : 52.329132080078125\n",
      "Train_MinReturn : -225.91958618164062\n",
      "Train_AverageEpLen : 229.0\n",
      "Actor Loss : 0.09607067704200745\n",
      "Baseline Loss : 2384.429443359375\n",
      "Train_EnvstepsSoFar : 192420\n",
      "TimeSinceStart : 105.01842308044434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -149.0484619140625\n",
      "Eval_StdReturn : 30.82317543029785\n",
      "Eval_MaxReturn : -113.12300109863281\n",
      "Eval_MinReturn : -188.39163208007812\n",
      "Eval_AverageEpLen : 183.33333333333334\n",
      "Train_AverageReturn : -176.0482177734375\n",
      "Train_StdReturn : 78.23209381103516\n",
      "Train_MaxReturn : -41.50944519042969\n",
      "Train_MinReturn : -279.1923828125\n",
      "Train_AverageEpLen : 252.55555555555554\n",
      "Actor Loss : -0.12926076352596283\n",
      "Baseline Loss : 3026.66064453125\n",
      "Train_EnvstepsSoFar : 262331\n",
      "TimeSinceStart : 155.00885772705078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 11.880531311035156\n",
      "Eval_StdReturn : 179.621826171875\n",
      "Eval_MaxReturn : 191.5023651123047\n",
      "Eval_MinReturn : -167.74130249023438\n",
      "Eval_AverageEpLen : 341.0\n",
      "Train_AverageReturn : -11.40391731262207\n",
      "Train_StdReturn : 146.1759033203125\n",
      "Train_MaxReturn : 213.52767944335938\n",
      "Train_MinReturn : -220.67385864257812\n",
      "Train_AverageEpLen : 271.375\n",
      "Actor Loss : 0.09062007069587708\n",
      "Baseline Loss : 4964.2763671875\n",
      "Train_EnvstepsSoFar : 327829\n",
      "TimeSinceStart : 191.26579427719116\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -113.20274353027344\n",
      "Eval_StdReturn : 100.76055908203125\n",
      "Eval_MaxReturn : -12.442184448242188\n",
      "Eval_MinReturn : -213.9633026123047\n",
      "Eval_AverageEpLen : 204.5\n",
      "Train_AverageReturn : -46.50352478027344\n",
      "Train_StdReturn : 88.50078582763672\n",
      "Train_MaxReturn : 100.39636993408203\n",
      "Train_MinReturn : -173.54388427734375\n",
      "Train_AverageEpLen : 349.3333333333333\n",
      "Actor Loss : 0.07697609066963196\n",
      "Baseline Loss : 1744.5777587890625\n",
      "Train_EnvstepsSoFar : 395126\n",
      "TimeSinceStart : 232.2140827178955\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -147.71835327148438\n",
      "Eval_StdReturn : 65.22144317626953\n",
      "Eval_MaxReturn : -82.49691772460938\n",
      "Eval_MinReturn : -212.93980407714844\n",
      "Eval_AverageEpLen : 280.0\n",
      "Train_AverageReturn : -107.99024200439453\n",
      "Train_StdReturn : 111.81758117675781\n",
      "Train_MaxReturn : 37.95844268798828\n",
      "Train_MinReturn : -240.14817810058594\n",
      "Train_AverageEpLen : 263.375\n",
      "Actor Loss : -0.02429034374654293\n",
      "Baseline Loss : 2839.114013671875\n",
      "Train_EnvstepsSoFar : 460495\n",
      "TimeSinceStart : 273.05120754241943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -144.3104705810547\n",
      "Eval_StdReturn : 83.00830078125\n",
      "Eval_MaxReturn : -61.30217742919922\n",
      "Eval_MinReturn : -227.3187713623047\n",
      "Eval_AverageEpLen : 258.0\n",
      "Train_AverageReturn : -73.80873107910156\n",
      "Train_StdReturn : 93.55056762695312\n",
      "Train_MaxReturn : 47.605674743652344\n",
      "Train_MinReturn : -208.9342041015625\n",
      "Train_AverageEpLen : 256.75\n",
      "Actor Loss : -0.11522270739078522\n",
      "Baseline Loss : 2726.726806640625\n",
      "Train_EnvstepsSoFar : 526472\n",
      "TimeSinceStart : 310.90925312042236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 103.54307556152344\n",
      "Eval_StdReturn : 117.76180267333984\n",
      "Eval_MaxReturn : 221.30487060546875\n",
      "Eval_MinReturn : -14.218727111816406\n",
      "Eval_AverageEpLen : 329.5\n",
      "Train_AverageReturn : -94.67192077636719\n",
      "Train_StdReturn : 87.68466186523438\n",
      "Train_MaxReturn : -1.8590517044067383\n",
      "Train_MinReturn : -233.0499725341797\n",
      "Train_AverageEpLen : 267.5\n",
      "Actor Loss : 0.019194582477211952\n",
      "Baseline Loss : 2223.222900390625\n",
      "Train_EnvstepsSoFar : 592555\n",
      "TimeSinceStart : 347.16121554374695\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 176.14007568359375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 176.14007568359375\n",
      "Eval_MinReturn : 176.14007568359375\n",
      "Eval_AverageEpLen : 439.0\n",
      "Train_AverageReturn : -115.34097290039062\n",
      "Train_StdReturn : 98.01085662841797\n",
      "Train_MaxReturn : 19.84923553466797\n",
      "Train_MinReturn : -246.12725830078125\n",
      "Train_AverageEpLen : 341.0\n",
      "Actor Loss : -0.21686987578868866\n",
      "Baseline Loss : 2916.930908203125\n",
      "Train_EnvstepsSoFar : 657415\n",
      "TimeSinceStart : 384.20684719085693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='LunarLander-v2', exp_name='lunar_lander_lambda0.95', n_iter=300, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=0.95, normalize_advantages=False, batch_size=2000, eval_batch_size=400, discount=0.99, learning_rate=0.001, n_layers=3, layer_size=128, ep_len=1000, seed=1, no_gpu=False, which_gpu=0, video_log_freq=100, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp3\\q2_pg_lunar_lander_lambda0.95_LunarLander-v2\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -125.99720764160156\n",
      "Eval_StdReturn : 62.193153381347656\n",
      "Eval_MaxReturn : -64.9122314453125\n",
      "Eval_MinReturn : -244.75828552246094\n",
      "Eval_AverageEpLen : 99.2\n",
      "Train_AverageReturn : -209.60800170898438\n",
      "Train_StdReturn : 109.69744873046875\n",
      "Train_MaxReturn : -77.18550872802734\n",
      "Train_MinReturn : -486.2994689941406\n",
      "Train_AverageEpLen : 91.69565217391305\n",
      "Actor Loss : -53.176265716552734\n",
      "Baseline Loss : 15147.3876953125\n",
      "Train_EnvstepsSoFar : 2109\n",
      "TimeSinceStart : 0.6005711555480957\n",
      "Initial_DataCollection_AverageReturn : -209.60800170898438\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -34.02397155761719\n",
      "Eval_StdReturn : 39.705810546875\n",
      "Eval_MaxReturn : 21.451988220214844\n",
      "Eval_MinReturn : -69.28793334960938\n",
      "Eval_AverageEpLen : 186.66666666666666\n",
      "Train_AverageReturn : -23.51678466796875\n",
      "Train_StdReturn : 26.51264762878418\n",
      "Train_MaxReturn : 20.186939239501953\n",
      "Train_MinReturn : -76.027099609375\n",
      "Train_AverageEpLen : 149.42857142857142\n",
      "Actor Loss : 9.830214500427246\n",
      "Baseline Loss : 1761.089111328125\n",
      "Train_EnvstepsSoFar : 62947\n",
      "TimeSinceStart : 22.007856130599976\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 30.530529022216797\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 30.530529022216797\n",
      "Eval_MinReturn : 30.530529022216797\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -128.57977294921875\n",
      "Train_StdReturn : 32.699764251708984\n",
      "Train_MaxReturn : -78.79852294921875\n",
      "Train_MinReturn : -173.03392028808594\n",
      "Train_AverageEpLen : 398.0\n",
      "Actor Loss : -3.9983584880828857\n",
      "Baseline Loss : 1529.47607421875\n",
      "Train_EnvstepsSoFar : 131984\n",
      "TimeSinceStart : 82.27250385284424\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 86.6624984741211\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 86.6624984741211\n",
      "Eval_MinReturn : 86.6624984741211\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 94.49000549316406\n",
      "Train_StdReturn : 35.674678802490234\n",
      "Train_MaxReturn : 130.16468811035156\n",
      "Train_MinReturn : 58.815330505371094\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.5315660238265991\n",
      "Baseline Loss : 192.26657104492188\n",
      "Train_EnvstepsSoFar : 197900\n",
      "TimeSinceStart : 157.96952605247498\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -134.23202514648438\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -134.23202514648438\n",
      "Eval_MinReturn : -134.23202514648438\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -54.64881896972656\n",
      "Train_StdReturn : 33.3616943359375\n",
      "Train_MaxReturn : -21.28712272644043\n",
      "Train_MinReturn : -88.01051330566406\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -1.4541181325912476\n",
      "Baseline Loss : 80.60504150390625\n",
      "Train_EnvstepsSoFar : 260416\n",
      "TimeSinceStart : 268.91787338256836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -10.075443267822266\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -10.075443267822266\n",
      "Eval_MinReturn : -10.075443267822266\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -11.279547691345215\n",
      "Train_StdReturn : 9.11623477935791\n",
      "Train_MaxReturn : -2.1633129119873047\n",
      "Train_MinReturn : -20.395782470703125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.41721728444099426\n",
      "Baseline Loss : 104.41388702392578\n",
      "Train_EnvstepsSoFar : 321413\n",
      "TimeSinceStart : 362.67136001586914\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 8.853767395019531\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 8.853767395019531\n",
      "Eval_MinReturn : 8.853767395019531\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -30.789358139038086\n",
      "Train_StdReturn : 27.387727737426758\n",
      "Train_MaxReturn : -3.401630401611328\n",
      "Train_MinReturn : -58.177085876464844\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.23700861632823944\n",
      "Baseline Loss : 92.45398712158203\n",
      "Train_EnvstepsSoFar : 384801\n",
      "TimeSinceStart : 460.3859827518463\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -73.30980682373047\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -73.30980682373047\n",
      "Eval_MinReturn : -73.30980682373047\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -31.18337631225586\n",
      "Train_StdReturn : 2.2590408325195312\n",
      "Train_MaxReturn : -28.924335479736328\n",
      "Train_MinReturn : -33.44241714477539\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.07686085253953934\n",
      "Baseline Loss : 140.6731719970703\n",
      "Train_EnvstepsSoFar : 445799\n",
      "TimeSinceStart : 573.6354477405548\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -57.138763427734375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -57.138763427734375\n",
      "Eval_MinReturn : -57.138763427734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -54.33737564086914\n",
      "Train_StdReturn : 11.217998504638672\n",
      "Train_MaxReturn : -43.11937713623047\n",
      "Train_MinReturn : -65.55537414550781\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.12768368422985077\n",
      "Baseline Loss : 107.98828125\n",
      "Train_EnvstepsSoFar : 505799\n",
      "TimeSinceStart : 665.3884098529816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -43.49836730957031\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -43.49836730957031\n",
      "Eval_MinReturn : -43.49836730957031\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -40.231719970703125\n",
      "Train_StdReturn : 8.674985885620117\n",
      "Train_MaxReturn : -31.55673599243164\n",
      "Train_MinReturn : -48.906707763671875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.5416887402534485\n",
      "Baseline Loss : 173.90994262695312\n",
      "Train_EnvstepsSoFar : 568558\n",
      "TimeSinceStart : 753.3601188659668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -45.030208587646484\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -45.030208587646484\n",
      "Eval_MinReturn : -45.030208587646484\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -9.320846557617188\n",
      "Train_StdReturn : 10.454132080078125\n",
      "Train_MaxReturn : 1.1332855224609375\n",
      "Train_MinReturn : -19.774978637695312\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.1285170465707779\n",
      "Baseline Loss : 147.26258850097656\n",
      "Train_EnvstepsSoFar : 629531\n",
      "TimeSinceStart : 842.6834092140198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='LunarLander-v2', exp_name='lunar_lander_lambda0.98', n_iter=300, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=0.98, normalize_advantages=False, batch_size=2000, eval_batch_size=400, discount=0.99, learning_rate=0.001, n_layers=3, layer_size=128, ep_len=1000, seed=1, no_gpu=False, which_gpu=0, video_log_freq=100, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp3\\q2_pg_lunar_lander_lambda0.98_LunarLander-v2\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -151.43496704101562\n",
      "Eval_StdReturn : 68.97090148925781\n",
      "Eval_MaxReturn : -64.81558227539062\n",
      "Eval_MinReturn : -270.9161376953125\n",
      "Eval_AverageEpLen : 89.4\n",
      "Train_AverageReturn : -169.39797973632812\n",
      "Train_StdReturn : 121.95913696289062\n",
      "Train_MaxReturn : 11.730178833007812\n",
      "Train_MinReturn : -523.902587890625\n",
      "Train_AverageEpLen : 89.8695652173913\n",
      "Actor Loss : -87.0692138671875\n",
      "Baseline Loss : 12060.4423828125\n",
      "Train_EnvstepsSoFar : 2067\n",
      "TimeSinceStart : 0.605907678604126\n",
      "Initial_DataCollection_AverageReturn : -169.39797973632812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -40.590946197509766\n",
      "Eval_StdReturn : 29.501333236694336\n",
      "Eval_MaxReturn : -9.364133834838867\n",
      "Eval_MinReturn : -80.16593933105469\n",
      "Eval_AverageEpLen : 156.66666666666666\n",
      "Train_AverageReturn : -67.15032958984375\n",
      "Train_StdReturn : 33.501136779785156\n",
      "Train_MaxReturn : -2.98870849609375\n",
      "Train_MinReturn : -108.84297180175781\n",
      "Train_AverageEpLen : 131.3125\n",
      "Actor Loss : 15.391310691833496\n",
      "Baseline Loss : 1067.645263671875\n",
      "Train_EnvstepsSoFar : 62660\n",
      "TimeSinceStart : 22.069647312164307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 15.079413414001465\n",
      "Eval_StdReturn : 12.201323509216309\n",
      "Eval_MaxReturn : 28.553123474121094\n",
      "Eval_MinReturn : -0.993009090423584\n",
      "Eval_AverageEpLen : 148.0\n",
      "Train_AverageReturn : 12.951470375061035\n",
      "Train_StdReturn : 23.18451690673828\n",
      "Train_MaxReturn : 50.50160217285156\n",
      "Train_MinReturn : -36.77964401245117\n",
      "Train_AverageEpLen : 138.66666666666666\n",
      "Actor Loss : 10.72612190246582\n",
      "Baseline Loss : 1669.6370849609375\n",
      "Train_EnvstepsSoFar : 129198\n",
      "TimeSinceStart : 66.20809292793274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 143.065673828125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 143.065673828125\n",
      "Eval_MinReturn : 143.065673828125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 125.7031021118164\n",
      "Train_StdReturn : 72.51277923583984\n",
      "Train_MaxReturn : 184.584228515625\n",
      "Train_MinReturn : 23.551376342773438\n",
      "Train_AverageEpLen : 694.6666666666666\n",
      "Actor Loss : 4.873810768127441\n",
      "Baseline Loss : 841.1810913085938\n",
      "Train_EnvstepsSoFar : 196305\n",
      "TimeSinceStart : 139.7622435092926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -74.92471313476562\n",
      "Eval_StdReturn : 17.374874114990234\n",
      "Eval_MaxReturn : -57.549842834472656\n",
      "Eval_MinReturn : -92.29959106445312\n",
      "Eval_AverageEpLen : 380.5\n",
      "Train_AverageReturn : -38.3868293762207\n",
      "Train_StdReturn : 16.305912017822266\n",
      "Train_MaxReturn : -23.603710174560547\n",
      "Train_MinReturn : -61.10544967651367\n",
      "Train_AverageEpLen : 737.6666666666666\n",
      "Actor Loss : -0.07886680960655212\n",
      "Baseline Loss : 721.8155517578125\n",
      "Train_EnvstepsSoFar : 264447\n",
      "TimeSinceStart : 224.02494931221008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -12.76980972290039\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -12.76980972290039\n",
      "Eval_MinReturn : -12.76980972290039\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -30.96428108215332\n",
      "Train_StdReturn : 19.121591567993164\n",
      "Train_MaxReturn : -11.842689514160156\n",
      "Train_MinReturn : -50.085872650146484\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -1.3670834302902222\n",
      "Baseline Loss : 292.6993103027344\n",
      "Train_EnvstepsSoFar : 331607\n",
      "TimeSinceStart : 302.0763726234436\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 259.8425598144531\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 259.8425598144531\n",
      "Eval_MinReturn : 259.8425598144531\n",
      "Eval_AverageEpLen : 409.0\n",
      "Train_AverageReturn : 202.46214294433594\n",
      "Train_StdReturn : 40.589054107666016\n",
      "Train_MaxReturn : 264.90716552734375\n",
      "Train_MinReturn : 151.8230438232422\n",
      "Train_AverageEpLen : 517.75\n",
      "Actor Loss : -4.419976711273193\n",
      "Baseline Loss : 975.4304809570312\n",
      "Train_EnvstepsSoFar : 397964\n",
      "TimeSinceStart : 365.91073536872864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -20.35803985595703\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -20.35803985595703\n",
      "Eval_MinReturn : -20.35803985595703\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : -34.07521057128906\n",
      "Train_StdReturn : 0.8857479095458984\n",
      "Train_MaxReturn : -33.1894645690918\n",
      "Train_MinReturn : -34.960960388183594\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -2.462754249572754\n",
      "Baseline Loss : 88.27159118652344\n",
      "Train_EnvstepsSoFar : 465679\n",
      "TimeSinceStart : 448.97661447525024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 6.87518310546875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 6.87518310546875\n",
      "Eval_MinReturn : 6.87518310546875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 64.29411315917969\n",
      "Train_StdReturn : 135.60694885253906\n",
      "Train_MaxReturn : 180.80044555664062\n",
      "Train_MinReturn : -125.88165283203125\n",
      "Train_AverageEpLen : 830.6666666666666\n",
      "Actor Loss : -9.271087646484375\n",
      "Baseline Loss : 1957.0128173828125\n",
      "Train_EnvstepsSoFar : 534222\n",
      "TimeSinceStart : 504.5910151004791\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -193.64971923828125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -193.64971923828125\n",
      "Eval_MinReturn : -193.64971923828125\n",
      "Eval_AverageEpLen : 825.0\n",
      "Train_AverageReturn : -20.45281219482422\n",
      "Train_StdReturn : 3.8319168090820312\n",
      "Train_MaxReturn : -16.620895385742188\n",
      "Train_MinReturn : -24.28472900390625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 1.0152390003204346\n",
      "Baseline Loss : 280.3653259277344\n",
      "Train_EnvstepsSoFar : 595198\n",
      "TimeSinceStart : 603.5471515655518\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 261.0523986816406\n",
      "Eval_StdReturn : 20.811248779296875\n",
      "Eval_MaxReturn : 281.8636474609375\n",
      "Eval_MinReturn : 240.24114990234375\n",
      "Eval_AverageEpLen : 374.0\n",
      "Train_AverageReturn : 171.1039276123047\n",
      "Train_StdReturn : 110.44674682617188\n",
      "Train_MaxReturn : 266.6182861328125\n",
      "Train_MinReturn : -21.084793090820312\n",
      "Train_AverageEpLen : 290.0\n",
      "Actor Loss : -4.175097465515137\n",
      "Baseline Loss : 1443.03662109375\n",
      "Train_EnvstepsSoFar : 660885\n",
      "TimeSinceStart : 643.6180145740509\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='LunarLander-v2', exp_name='lunar_lander_lambda0.99', n_iter=300, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=0.99, normalize_advantages=False, batch_size=2000, eval_batch_size=400, discount=0.99, learning_rate=0.001, n_layers=3, layer_size=128, ep_len=1000, seed=1, no_gpu=False, which_gpu=0, video_log_freq=100, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp3\\q2_pg_lunar_lander_lambda0.99_LunarLander-v2\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -176.42738342285156\n",
      "Eval_StdReturn : 103.22578430175781\n",
      "Eval_MaxReturn : -66.7901840209961\n",
      "Eval_MinReturn : -371.21746826171875\n",
      "Eval_AverageEpLen : 79.33333333333333\n",
      "Train_AverageReturn : -169.0244903564453\n",
      "Train_StdReturn : 116.8520736694336\n",
      "Train_MaxReturn : -2.7479171752929688\n",
      "Train_MinReturn : -488.3636779785156\n",
      "Train_AverageEpLen : 86.95833333333333\n",
      "Actor Loss : -133.2552947998047\n",
      "Baseline Loss : 11827.34375\n",
      "Train_EnvstepsSoFar : 2087\n",
      "TimeSinceStart : 0.6199309825897217\n",
      "Initial_DataCollection_AverageReturn : -169.0244903564453\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -47.771915435791016\n",
      "Eval_StdReturn : 24.825090408325195\n",
      "Eval_MaxReturn : -7.8010101318359375\n",
      "Eval_MinReturn : -76.03717803955078\n",
      "Eval_AverageEpLen : 116.0\n",
      "Train_AverageReturn : -99.95323181152344\n",
      "Train_StdReturn : 73.85213470458984\n",
      "Train_MaxReturn : -15.928479194641113\n",
      "Train_MinReturn : -287.5807189941406\n",
      "Train_AverageEpLen : 129.75\n",
      "Actor Loss : 10.906599044799805\n",
      "Baseline Loss : 1308.517578125\n",
      "Train_EnvstepsSoFar : 62351\n",
      "TimeSinceStart : 19.890775442123413\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 23.311880111694336\n",
      "Eval_StdReturn : 25.054895401000977\n",
      "Eval_MaxReturn : 56.87904357910156\n",
      "Eval_MinReturn : -3.2979812622070312\n",
      "Eval_AverageEpLen : 161.66666666666666\n",
      "Train_AverageReturn : 8.855376243591309\n",
      "Train_StdReturn : 36.53670120239258\n",
      "Train_MaxReturn : 51.66120910644531\n",
      "Train_MinReturn : -54.171085357666016\n",
      "Train_AverageEpLen : 235.55555555555554\n",
      "Actor Loss : 18.33310317993164\n",
      "Baseline Loss : 1381.315185546875\n",
      "Train_EnvstepsSoFar : 125345\n",
      "TimeSinceStart : 53.20037770271301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 123.20779418945312\n",
      "Eval_StdReturn : 93.19123077392578\n",
      "Eval_MaxReturn : 216.39903259277344\n",
      "Eval_MinReturn : 30.016563415527344\n",
      "Eval_AverageEpLen : 225.5\n",
      "Train_AverageReturn : 37.79263687133789\n",
      "Train_StdReturn : 64.83647918701172\n",
      "Train_MaxReturn : 227.85379028320312\n",
      "Train_MinReturn : -30.187158584594727\n",
      "Train_AverageEpLen : 185.1818181818182\n",
      "Actor Loss : 10.1600923538208\n",
      "Baseline Loss : 2284.40869140625\n",
      "Train_EnvstepsSoFar : 194469\n",
      "TimeSinceStart : 115.09781002998352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 25.369857788085938\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 25.369857788085938\n",
      "Eval_MinReturn : 25.369857788085938\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 14.556921005249023\n",
      "Train_StdReturn : 79.39012908935547\n",
      "Train_MaxReturn : 153.32028198242188\n",
      "Train_MinReturn : -67.2914810180664\n",
      "Train_AverageEpLen : 580.0\n",
      "Actor Loss : 2.7051877975463867\n",
      "Baseline Loss : 1670.9451904296875\n",
      "Train_EnvstepsSoFar : 257525\n",
      "TimeSinceStart : 212.18069314956665\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 190.07522583007812\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 190.07522583007812\n",
      "Eval_MinReturn : 190.07522583007812\n",
      "Eval_AverageEpLen : 774.0\n",
      "Train_AverageReturn : 132.5734100341797\n",
      "Train_StdReturn : 102.07013702392578\n",
      "Train_MaxReturn : 209.75299072265625\n",
      "Train_MinReturn : -11.657066345214844\n",
      "Train_AverageEpLen : 791.0\n",
      "Actor Loss : 1.6202071905136108\n",
      "Baseline Loss : 839.2640380859375\n",
      "Train_EnvstepsSoFar : 326882\n",
      "TimeSinceStart : 272.2292094230652\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 69.81932830810547\n",
      "Eval_StdReturn : 53.71710968017578\n",
      "Eval_MaxReturn : 123.53643798828125\n",
      "Eval_MinReturn : 16.102218627929688\n",
      "Eval_AverageEpLen : 454.5\n",
      "Train_AverageReturn : 121.58761596679688\n",
      "Train_StdReturn : 115.8746337890625\n",
      "Train_MaxReturn : 229.86529541015625\n",
      "Train_MinReturn : -44.99083709716797\n",
      "Train_AverageEpLen : 359.8333333333333\n",
      "Actor Loss : 4.487782001495361\n",
      "Baseline Loss : 1828.2099609375\n",
      "Train_EnvstepsSoFar : 398234\n",
      "TimeSinceStart : 343.93627882003784\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 135.89967346191406\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 135.89967346191406\n",
      "Eval_MinReturn : 135.89967346191406\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 93.89861297607422\n",
      "Train_StdReturn : 37.15868377685547\n",
      "Train_MaxReturn : 131.0572967529297\n",
      "Train_MinReturn : 56.73992919921875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.04612625017762184\n",
      "Baseline Loss : 325.3963623046875\n",
      "Train_EnvstepsSoFar : 467249\n",
      "TimeSinceStart : 424.01040959358215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -101.4623794555664\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : -101.4623794555664\n",
      "Eval_MinReturn : -101.4623794555664\n",
      "Eval_AverageEpLen : 575.0\n",
      "Train_AverageReturn : -35.36310958862305\n",
      "Train_StdReturn : 25.065340042114258\n",
      "Train_MaxReturn : -3.0859603881835938\n",
      "Train_MinReturn : -64.19207763671875\n",
      "Train_AverageEpLen : 793.6666666666666\n",
      "Actor Loss : 0.12421216815710068\n",
      "Baseline Loss : 555.2013549804688\n",
      "Train_EnvstepsSoFar : 531819\n",
      "TimeSinceStart : 502.5986649990082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 133.20765686035156\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 133.20765686035156\n",
      "Eval_MinReturn : 133.20765686035156\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 137.0707550048828\n",
      "Train_StdReturn : 16.482067108154297\n",
      "Train_MaxReturn : 153.55282592773438\n",
      "Train_MinReturn : 120.58869171142578\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 2.0611813068389893\n",
      "Baseline Loss : 404.58502197265625\n",
      "Train_EnvstepsSoFar : 598694\n",
      "TimeSinceStart : 595.1228196620941\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 137.39105224609375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 137.39105224609375\n",
      "Eval_MinReturn : 137.39105224609375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 79.49779510498047\n",
      "Train_StdReturn : 16.96942901611328\n",
      "Train_MaxReturn : 96.46722412109375\n",
      "Train_MinReturn : 62.52836608886719\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.04362112283706665\n",
      "Baseline Loss : 169.5315399169922\n",
      "Train_EnvstepsSoFar : 665198\n",
      "TimeSinceStart : 674.7544219493866\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='LunarLander-v2', exp_name='lunar_lander_lambda1', n_iter=300, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=1.0, normalize_advantages=False, batch_size=2000, eval_batch_size=400, discount=0.99, learning_rate=0.001, n_layers=3, layer_size=128, ep_len=1000, seed=1, no_gpu=False, which_gpu=0, video_log_freq=100, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp3\\q2_pg_lunar_lander_lambda1_LunarLander-v2\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -139.70262145996094\n",
      "Eval_StdReturn : 60.52817153930664\n",
      "Eval_MaxReturn : -81.14521789550781\n",
      "Eval_MinReturn : -253.4558868408203\n",
      "Eval_AverageEpLen : 91.6\n",
      "Train_AverageReturn : -159.66424560546875\n",
      "Train_StdReturn : 92.46004486083984\n",
      "Train_MaxReturn : 34.90504455566406\n",
      "Train_MinReturn : -360.8289489746094\n",
      "Train_AverageEpLen : 84.83333333333333\n",
      "Actor Loss : -249.9574432373047\n",
      "Baseline Loss : 9879.2802734375\n",
      "Train_EnvstepsSoFar : 2036\n",
      "TimeSinceStart : 0.5942733287811279\n",
      "Initial_DataCollection_AverageReturn : -159.66424560546875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -211.7406768798828\n",
      "Eval_StdReturn : 175.9287109375\n",
      "Eval_MaxReturn : -58.70466613769531\n",
      "Eval_MinReturn : -507.08184814453125\n",
      "Eval_AverageEpLen : 100.5\n",
      "Train_AverageReturn : -125.50618743896484\n",
      "Train_StdReturn : 44.86887741088867\n",
      "Train_MaxReturn : -46.36199188232422\n",
      "Train_MinReturn : -217.18109130859375\n",
      "Train_AverageEpLen : 84.79166666666667\n",
      "Actor Loss : -12.195743560791016\n",
      "Baseline Loss : 668.8255615234375\n",
      "Train_EnvstepsSoFar : 61417\n",
      "TimeSinceStart : 17.192375421524048\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -66.24876403808594\n",
      "Eval_StdReturn : 25.112537384033203\n",
      "Eval_MaxReturn : -22.981163024902344\n",
      "Eval_MinReturn : -84.85603332519531\n",
      "Eval_AverageEpLen : 123.75\n",
      "Train_AverageReturn : -58.674560546875\n",
      "Train_StdReturn : 34.21254348754883\n",
      "Train_MaxReturn : -9.836053848266602\n",
      "Train_MinReturn : -135.1975860595703\n",
      "Train_AverageEpLen : 120.88235294117646\n",
      "Actor Loss : 53.87400817871094\n",
      "Baseline Loss : 1042.3187255859375\n",
      "Train_EnvstepsSoFar : 123217\n",
      "TimeSinceStart : 35.27055621147156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -70.91323852539062\n",
      "Eval_StdReturn : 7.905328273773193\n",
      "Eval_MaxReturn : -63.84278869628906\n",
      "Eval_MinReturn : -81.94832611083984\n",
      "Eval_AverageEpLen : 183.33333333333334\n",
      "Train_AverageReturn : 9.7358980178833\n",
      "Train_StdReturn : 42.2045783996582\n",
      "Train_MaxReturn : 86.68916320800781\n",
      "Train_MinReturn : -47.47224426269531\n",
      "Train_AverageEpLen : 258.125\n",
      "Actor Loss : 29.128896713256836\n",
      "Baseline Loss : 1086.5\n",
      "Train_EnvstepsSoFar : 190761\n",
      "TimeSinceStart : 78.33038115501404\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 138.77203369140625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 138.77203369140625\n",
      "Eval_MinReturn : 138.77203369140625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 94.04019927978516\n",
      "Train_StdReturn : 20.239044189453125\n",
      "Train_MaxReturn : 114.27924346923828\n",
      "Train_MinReturn : 73.80115509033203\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 7.12746524810791\n",
      "Baseline Loss : 213.59408569335938\n",
      "Train_EnvstepsSoFar : 257554\n",
      "TimeSinceStart : 140.18974661827087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 74.23838806152344\n",
      "Eval_StdReturn : 49.86216735839844\n",
      "Eval_MaxReturn : 124.10055541992188\n",
      "Eval_MinReturn : 24.376220703125\n",
      "Eval_AverageEpLen : 621.0\n",
      "Train_AverageReturn : 55.87849044799805\n",
      "Train_StdReturn : 89.48418426513672\n",
      "Train_MaxReturn : 164.1473388671875\n",
      "Train_MinReturn : -61.53158187866211\n",
      "Train_AverageEpLen : 545.0\n",
      "Actor Loss : 1.4560855627059937\n",
      "Baseline Loss : 1083.6905517578125\n",
      "Train_EnvstepsSoFar : 325268\n",
      "TimeSinceStart : 214.08041167259216\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 25.92102813720703\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 25.92102813720703\n",
      "Eval_MinReturn : 25.92102813720703\n",
      "Eval_AverageEpLen : 429.0\n",
      "Train_AverageReturn : 114.9522705078125\n",
      "Train_StdReturn : 5.388980865478516\n",
      "Train_MaxReturn : 120.34125518798828\n",
      "Train_MinReturn : 109.56329345703125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 4.4535441398620605\n",
      "Baseline Loss : 223.81781005859375\n",
      "Train_EnvstepsSoFar : 391485\n",
      "TimeSinceStart : 278.9657709598541\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 165.16226196289062\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 165.16226196289062\n",
      "Eval_MinReturn : 165.16226196289062\n",
      "Eval_AverageEpLen : 604.0\n",
      "Train_AverageReturn : 81.20045471191406\n",
      "Train_StdReturn : 22.688121795654297\n",
      "Train_MaxReturn : 103.88858032226562\n",
      "Train_MinReturn : 58.51233673095703\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 1.9497654438018799\n",
      "Baseline Loss : 147.90353393554688\n",
      "Train_EnvstepsSoFar : 457377\n",
      "TimeSinceStart : 359.27741384506226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 214.08233642578125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 214.08233642578125\n",
      "Eval_MinReturn : 214.08233642578125\n",
      "Eval_AverageEpLen : 603.0\n",
      "Train_AverageReturn : 80.46940612792969\n",
      "Train_StdReturn : 116.88353729248047\n",
      "Train_MaxReturn : 204.09303283691406\n",
      "Train_MinReturn : -76.37156677246094\n",
      "Train_AverageEpLen : 685.6666666666666\n",
      "Actor Loss : -9.577081680297852\n",
      "Baseline Loss : 1036.0059814453125\n",
      "Train_EnvstepsSoFar : 526352\n",
      "TimeSinceStart : 412.8663966655731\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 238.14114379882812\n",
      "Eval_StdReturn : 12.193626403808594\n",
      "Eval_MaxReturn : 250.33477783203125\n",
      "Eval_MinReturn : 225.94752502441406\n",
      "Eval_AverageEpLen : 354.0\n",
      "Train_AverageReturn : 193.70053100585938\n",
      "Train_StdReturn : 112.8449478149414\n",
      "Train_MaxReturn : 288.13983154296875\n",
      "Train_MinReturn : -15.019214630126953\n",
      "Train_AverageEpLen : 278.375\n",
      "Actor Loss : 8.922185897827148\n",
      "Baseline Loss : 2304.59912109375\n",
      "Train_EnvstepsSoFar : 592897\n",
      "TimeSinceStart : 474.19921875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 134.34596252441406\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 134.34596252441406\n",
      "Eval_MinReturn : 134.34596252441406\n",
      "Eval_AverageEpLen : 516.0\n",
      "Train_AverageReturn : 31.05191993713379\n",
      "Train_StdReturn : 140.028076171875\n",
      "Train_MaxReturn : 245.11685180664062\n",
      "Train_MinReturn : -97.50358581542969\n",
      "Train_AverageEpLen : 364.1666666666667\n",
      "Actor Loss : 7.475884914398193\n",
      "Baseline Loss : 2749.49462890625\n",
      "Train_EnvstepsSoFar : 658811\n",
      "TimeSinceStart : 555.3092257976532\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lambda_ = [0, 0.95, 0.98, 0.99, 1]\n",
    "for la in lambda_:\n",
    "    %run run_hw2.py \\\n",
    "        --env_name LunarLander-v2 --ep_len 1000 \\\n",
    "        --discount 0.99 -n 300 -l 3 -s 128 -b 2000 -lr 0.001 \\\n",
    "        --use_reward_to_go --use_baseline --gae_lambda \"$la\" \\\n",
    "        --exp_name lunar_lander_lambda\"$la\" \\\n",
    "        --video_log_freq 100\n",
    "    print('*' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Provide a single plot with the learning curves for the LunarLander-v2 experiments that you tried. Describe in words how λ affected task performance. The run with the best performance should achieve an average score close to 200 (180+).  \n",
    "  The result of λ = 0.99 is close to λ = 1, and λ = 0.98 works best in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consider the parameter λ. What does λ = 0 correspond to? What about λ = 1? Relate this to the task performance in LunarLander-v2 in one or two sentences.  \n",
    "  When `λ = 1`, it corresponds to Monte Carlo method; when `λ = 0`, it corresponds to 1-step TD method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../../run_logs/exp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Sample Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4 (InvertedPendulum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, train a policy for the inverted pendulum problem without GAE and with otherwise default settings. Use five different seeds to get a good estimate of average performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_default_s1', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=5000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_default_s1_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 10.947368621826172\n",
      "Eval_StdReturn : 5.830714225769043\n",
      "Eval_MaxReturn : 27.0\n",
      "Eval_MinReturn : 5.0\n",
      "Eval_AverageEpLen : 10.947368421052632\n",
      "Train_AverageReturn : 8.70138931274414\n",
      "Train_StdReturn : 5.1184983253479\n",
      "Train_MaxReturn : 35.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 8.70138888888889\n",
      "Actor Loss : -0.06156244874000549\n",
      "Baseline Loss : 51.670318603515625\n",
      "Train_EnvstepsSoFar : 5012\n",
      "TimeSinceStart : 1.032536506652832\n",
      "Initial_DataCollection_AverageReturn : 8.70138931274414\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 52.5\n",
      "Eval_StdReturn : 22.276668548583984\n",
      "Eval_MaxReturn : 89.0\n",
      "Eval_MinReturn : 24.0\n",
      "Eval_AverageEpLen : 52.5\n",
      "Train_AverageReturn : 51.845359802246094\n",
      "Train_StdReturn : 19.63498878479004\n",
      "Train_MaxReturn : 118.0\n",
      "Train_MinReturn : 14.0\n",
      "Train_AverageEpLen : 51.845360824742265\n",
      "Actor Loss : 0.0021522080060094595\n",
      "Baseline Loss : 502.0694580078125\n",
      "Train_EnvstepsSoFar : 50204\n",
      "TimeSinceStart : 9.644773483276367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 87.80000305175781\n",
      "Eval_StdReturn : 21.01808738708496\n",
      "Eval_MaxReturn : 118.0\n",
      "Eval_MinReturn : 67.0\n",
      "Eval_AverageEpLen : 87.8\n",
      "Train_AverageReturn : 86.44827270507812\n",
      "Train_StdReturn : 32.56938171386719\n",
      "Train_MaxReturn : 228.0\n",
      "Train_MinReturn : 32.0\n",
      "Train_AverageEpLen : 86.44827586206897\n",
      "Actor Loss : -0.005078974179923534\n",
      "Baseline Loss : 1422.2154541015625\n",
      "Train_EnvstepsSoFar : 100867\n",
      "TimeSinceStart : 18.65973734855652\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 163.5\n",
      "Eval_StdReturn : 69.0669937133789\n",
      "Eval_MaxReturn : 265.0\n",
      "Eval_MinReturn : 70.0\n",
      "Eval_AverageEpLen : 163.5\n",
      "Train_AverageReturn : 109.0851058959961\n",
      "Train_StdReturn : 43.22815704345703\n",
      "Train_MaxReturn : 234.0\n",
      "Train_MinReturn : 26.0\n",
      "Train_AverageEpLen : 109.08510638297872\n",
      "Actor Loss : -0.02322208881378174\n",
      "Baseline Loss : 1711.0374755859375\n",
      "Train_EnvstepsSoFar : 151589\n",
      "TimeSinceStart : 27.57637310028076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 312.0\n",
      "Eval_StdReturn : 13.0\n",
      "Eval_MaxReturn : 325.0\n",
      "Eval_MinReturn : 299.0\n",
      "Eval_AverageEpLen : 312.0\n",
      "Train_AverageReturn : 181.20689392089844\n",
      "Train_StdReturn : 72.2817153930664\n",
      "Train_MaxReturn : 327.0\n",
      "Train_MinReturn : 64.0\n",
      "Train_AverageEpLen : 181.20689655172413\n",
      "Actor Loss : -0.030763499438762665\n",
      "Baseline Loss : 5413.380859375\n",
      "Train_EnvstepsSoFar : 202518\n",
      "TimeSinceStart : 36.507242918014526\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 557.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 557.0\n",
      "Eval_MinReturn : 557.0\n",
      "Eval_AverageEpLen : 557.0\n",
      "Train_AverageReturn : 366.6428527832031\n",
      "Train_StdReturn : 136.86573791503906\n",
      "Train_MaxReturn : 591.0\n",
      "Train_MinReturn : 64.0\n",
      "Train_AverageEpLen : 366.64285714285717\n",
      "Actor Loss : -0.004362284205853939\n",
      "Baseline Loss : 28274.525390625\n",
      "Train_EnvstepsSoFar : 254303\n",
      "TimeSinceStart : 45.57299852371216\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 755.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 755.0\n",
      "Eval_MinReturn : 755.0\n",
      "Eval_AverageEpLen : 755.0\n",
      "Train_AverageReturn : 377.9285583496094\n",
      "Train_StdReturn : 216.1611328125\n",
      "Train_MaxReturn : 706.0\n",
      "Train_MinReturn : 32.0\n",
      "Train_AverageEpLen : 377.92857142857144\n",
      "Actor Loss : -0.013541972264647484\n",
      "Baseline Loss : 40448.1640625\n",
      "Train_EnvstepsSoFar : 307784\n",
      "TimeSinceStart : 55.10131859779358\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 590.5\n",
      "Eval_StdReturn : 409.5\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 181.0\n",
      "Eval_AverageEpLen : 590.5\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.012821204960346222\n",
      "Baseline Loss : 179264.03125\n",
      "Train_EnvstepsSoFar : 360748\n",
      "TimeSinceStart : 64.89675188064575\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 249.0\n",
      "Eval_StdReturn : 142.0\n",
      "Eval_MaxReturn : 391.0\n",
      "Eval_MinReturn : 107.0\n",
      "Eval_AverageEpLen : 249.0\n",
      "Train_AverageReturn : 482.6363525390625\n",
      "Train_StdReturn : 365.344482421875\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 46.0\n",
      "Train_AverageEpLen : 482.6363636363636\n",
      "Actor Loss : -0.0046707927249372005\n",
      "Baseline Loss : 106789.7109375\n",
      "Train_EnvstepsSoFar : 413843\n",
      "TimeSinceStart : 74.50176525115967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 874.8333129882812\n",
      "Train_StdReturn : 279.88116455078125\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 249.0\n",
      "Train_AverageEpLen : 874.8333333333334\n",
      "Actor Loss : -0.00689292186871171\n",
      "Baseline Loss : 145979.75\n",
      "Train_EnvstepsSoFar : 465160\n",
      "TimeSinceStart : 84.14123463630676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.016699356958270073\n",
      "Baseline Loss : 141188.125\n",
      "Train_EnvstepsSoFar : 516910\n",
      "TimeSinceStart : 93.88914847373962\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_default_s2', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=5000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=2, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_default_s2_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 10.774999618530273\n",
      "Eval_StdReturn : 7.319452285766602\n",
      "Eval_MaxReturn : 36.0\n",
      "Eval_MinReturn : 4.0\n",
      "Eval_AverageEpLen : 10.775\n",
      "Train_AverageReturn : 7.489521026611328\n",
      "Train_StdReturn : 3.848346471786499\n",
      "Train_MaxReturn : 40.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 7.4895209580838324\n",
      "Actor Loss : -0.0402647964656353\n",
      "Baseline Loss : 32.022701263427734\n",
      "Train_EnvstepsSoFar : 5003\n",
      "TimeSinceStart : 0.9559879302978516\n",
      "Initial_DataCollection_AverageReturn : 7.489521026611328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 61.14285659790039\n",
      "Eval_StdReturn : 17.496353149414062\n",
      "Eval_MaxReturn : 85.0\n",
      "Eval_MinReturn : 40.0\n",
      "Eval_AverageEpLen : 61.142857142857146\n",
      "Train_AverageReturn : 47.70476150512695\n",
      "Train_StdReturn : 25.569671630859375\n",
      "Train_MaxReturn : 135.0\n",
      "Train_MinReturn : 16.0\n",
      "Train_AverageEpLen : 47.7047619047619\n",
      "Actor Loss : -0.014275426976382732\n",
      "Baseline Loss : 744.8628540039062\n",
      "Train_EnvstepsSoFar : 50156\n",
      "TimeSinceStart : 9.061148166656494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 74.33333587646484\n",
      "Eval_StdReturn : 37.31249237060547\n",
      "Eval_MaxReturn : 147.0\n",
      "Eval_MinReturn : 36.0\n",
      "Eval_AverageEpLen : 74.33333333333333\n",
      "Train_AverageReturn : 81.08064270019531\n",
      "Train_StdReturn : 27.99498748779297\n",
      "Train_MaxReturn : 147.0\n",
      "Train_MinReturn : 36.0\n",
      "Train_AverageEpLen : 81.08064516129032\n",
      "Actor Loss : -0.007390689570456743\n",
      "Baseline Loss : 987.716552734375\n",
      "Train_EnvstepsSoFar : 100639\n",
      "TimeSinceStart : 17.986046314239502\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 162.3333282470703\n",
      "Eval_StdReturn : 13.888444900512695\n",
      "Eval_MaxReturn : 175.0\n",
      "Eval_MinReturn : 143.0\n",
      "Eval_AverageEpLen : 162.33333333333334\n",
      "Train_AverageReturn : 126.80000305175781\n",
      "Train_StdReturn : 63.098018646240234\n",
      "Train_MaxReturn : 420.0\n",
      "Train_MinReturn : 43.0\n",
      "Train_AverageEpLen : 126.8\n",
      "Actor Loss : -0.028181996196508408\n",
      "Baseline Loss : 4797.35888671875\n",
      "Train_EnvstepsSoFar : 151433\n",
      "TimeSinceStart : 26.936429738998413\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 163.0\n",
      "Eval_StdReturn : 55.898719787597656\n",
      "Eval_MaxReturn : 240.0\n",
      "Eval_MinReturn : 109.0\n",
      "Eval_AverageEpLen : 163.0\n",
      "Train_AverageReturn : 161.28125\n",
      "Train_StdReturn : 45.03695297241211\n",
      "Train_MaxReturn : 279.0\n",
      "Train_MinReturn : 77.0\n",
      "Train_AverageEpLen : 161.28125\n",
      "Actor Loss : -0.007942848838865757\n",
      "Baseline Loss : 2982.368408203125\n",
      "Train_EnvstepsSoFar : 202125\n",
      "TimeSinceStart : 35.814374923706055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 577.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 577.0\n",
      "Eval_MinReturn : 577.0\n",
      "Eval_AverageEpLen : 577.0\n",
      "Train_AverageReturn : 471.7272644042969\n",
      "Train_StdReturn : 302.7417297363281\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 112.0\n",
      "Train_AverageEpLen : 471.72727272727275\n",
      "Actor Loss : 0.010424868203699589\n",
      "Baseline Loss : 115028.890625\n",
      "Train_EnvstepsSoFar : 253632\n",
      "TimeSinceStart : 44.827006816864014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.00862214807420969\n",
      "Baseline Loss : 200205.421875\n",
      "Train_EnvstepsSoFar : 307162\n",
      "TimeSinceStart : 54.610612630844116\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.004941376391798258\n",
      "Baseline Loss : 176653.796875\n",
      "Train_EnvstepsSoFar : 359261\n",
      "TimeSinceStart : 64.68317317962646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.02478424645960331\n",
      "Baseline Loss : 160785.890625\n",
      "Train_EnvstepsSoFar : 409261\n",
      "TimeSinceStart : 74.17512249946594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 911.3333129882812\n",
      "Train_StdReturn : 198.2646942138672\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 468.0\n",
      "Train_AverageEpLen : 911.3333333333334\n",
      "Actor Loss : -0.0074359639547765255\n",
      "Baseline Loss : 137920.859375\n",
      "Train_EnvstepsSoFar : 461052\n",
      "TimeSinceStart : 84.01577067375183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.020351143553853035\n",
      "Baseline Loss : 140484.84375\n",
      "Train_EnvstepsSoFar : 511971\n",
      "TimeSinceStart : 93.50260019302368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_default_s3', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=5000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=3, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_default_s3_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 8.617021560668945\n",
      "Eval_StdReturn : 4.536158561706543\n",
      "Eval_MaxReturn : 19.0\n",
      "Eval_MinReturn : 4.0\n",
      "Eval_AverageEpLen : 8.617021276595745\n",
      "Train_AverageReturn : 7.720678806304932\n",
      "Train_StdReturn : 3.985398292541504\n",
      "Train_MaxReturn : 26.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 7.720679012345679\n",
      "Actor Loss : -0.06736429035663605\n",
      "Baseline Loss : 31.980758666992188\n",
      "Train_EnvstepsSoFar : 5003\n",
      "TimeSinceStart : 0.9547932147979736\n",
      "Initial_DataCollection_AverageReturn : 7.720678806304932\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 59.5\n",
      "Eval_StdReturn : 29.184757232666016\n",
      "Eval_MaxReturn : 96.0\n",
      "Eval_MinReturn : 14.0\n",
      "Eval_AverageEpLen : 59.5\n",
      "Train_AverageReturn : 60.530120849609375\n",
      "Train_StdReturn : 23.57777976989746\n",
      "Train_MaxReturn : 156.0\n",
      "Train_MinReturn : 16.0\n",
      "Train_AverageEpLen : 60.53012048192771\n",
      "Actor Loss : -0.009363304823637009\n",
      "Baseline Loss : 835.2066040039062\n",
      "Train_EnvstepsSoFar : 50150\n",
      "TimeSinceStart : 8.998359680175781\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 94.83333587646484\n",
      "Eval_StdReturn : 52.83123016357422\n",
      "Eval_MaxReturn : 174.0\n",
      "Eval_MinReturn : 32.0\n",
      "Eval_AverageEpLen : 94.83333333333333\n",
      "Train_AverageReturn : 98.5882339477539\n",
      "Train_StdReturn : 50.73930740356445\n",
      "Train_MaxReturn : 277.0\n",
      "Train_MinReturn : 34.0\n",
      "Train_AverageEpLen : 98.58823529411765\n",
      "Actor Loss : -0.014546995982527733\n",
      "Baseline Loss : 3079.819091796875\n",
      "Train_EnvstepsSoFar : 100616\n",
      "TimeSinceStart : 17.948729038238525\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 94.4000015258789\n",
      "Eval_StdReturn : 35.02342224121094\n",
      "Eval_MaxReturn : 126.0\n",
      "Eval_MinReturn : 32.0\n",
      "Eval_AverageEpLen : 94.4\n",
      "Train_AverageReturn : 135.84210205078125\n",
      "Train_StdReturn : 71.83113098144531\n",
      "Train_MaxReturn : 356.0\n",
      "Train_MinReturn : 49.0\n",
      "Train_AverageEpLen : 135.8421052631579\n",
      "Actor Loss : -0.006107481662184\n",
      "Baseline Loss : 5374.23388671875\n",
      "Train_EnvstepsSoFar : 151237\n",
      "TimeSinceStart : 26.89343285560608\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 141.0\n",
      "Eval_StdReturn : 30.389142990112305\n",
      "Eval_MaxReturn : 193.0\n",
      "Eval_MinReturn : 116.0\n",
      "Eval_AverageEpLen : 141.0\n",
      "Train_AverageReturn : 162.96774291992188\n",
      "Train_StdReturn : 52.548274993896484\n",
      "Train_MaxReturn : 309.0\n",
      "Train_MinReturn : 44.0\n",
      "Train_AverageEpLen : 162.96774193548387\n",
      "Actor Loss : -0.01714753918349743\n",
      "Baseline Loss : 3599.680419921875\n",
      "Train_EnvstepsSoFar : 202150\n",
      "TimeSinceStart : 35.78832387924194\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 209.5\n",
      "Eval_StdReturn : 76.5\n",
      "Eval_MaxReturn : 286.0\n",
      "Eval_MinReturn : 133.0\n",
      "Eval_AverageEpLen : 209.5\n",
      "Train_AverageReturn : 224.3913116455078\n",
      "Train_StdReturn : 107.58686828613281\n",
      "Train_MaxReturn : 570.0\n",
      "Train_MinReturn : 110.0\n",
      "Train_AverageEpLen : 224.3913043478261\n",
      "Actor Loss : -0.011182147078216076\n",
      "Baseline Loss : 14558.6611328125\n",
      "Train_EnvstepsSoFar : 253260\n",
      "TimeSinceStart : 44.753368616104126\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 619.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 619.0\n",
      "Eval_MinReturn : 619.0\n",
      "Eval_AverageEpLen : 619.0\n",
      "Train_AverageReturn : 719.4285888671875\n",
      "Train_StdReturn : 329.1073913574219\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 248.0\n",
      "Train_AverageEpLen : 719.4285714285714\n",
      "Actor Loss : 0.014956354163587093\n",
      "Baseline Loss : 173406.78125\n",
      "Train_EnvstepsSoFar : 305078\n",
      "TimeSinceStart : 53.919063329696655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 654.4444580078125\n",
      "Train_StdReturn : 356.6065673828125\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 57.0\n",
      "Train_AverageEpLen : 654.4444444444445\n",
      "Actor Loss : 0.0075202398002147675\n",
      "Baseline Loss : 142798.0\n",
      "Train_EnvstepsSoFar : 360033\n",
      "TimeSinceStart : 64.00954246520996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 921.1666870117188\n",
      "Train_StdReturn : 176.2766876220703\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 527.0\n",
      "Train_AverageEpLen : 921.1666666666666\n",
      "Actor Loss : -0.005294563714414835\n",
      "Baseline Loss : 157238.3125\n",
      "Train_EnvstepsSoFar : 414182\n",
      "TimeSinceStart : 74.20454049110413\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.006859657820314169\n",
      "Baseline Loss : 156610.703125\n",
      "Train_EnvstepsSoFar : 466109\n",
      "TimeSinceStart : 83.93738985061646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 636.875\n",
      "Train_StdReturn : 376.2124938964844\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 120.0\n",
      "Train_AverageEpLen : 636.875\n",
      "Actor Loss : 0.0008846440468914807\n",
      "Baseline Loss : 115179.125\n",
      "Train_EnvstepsSoFar : 518341\n",
      "TimeSinceStart : 93.5210292339325\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_default_s4', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=5000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=4, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_default_s4_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 10.175000190734863\n",
      "Eval_StdReturn : 7.052969455718994\n",
      "Eval_MaxReturn : 36.0\n",
      "Eval_MinReturn : 4.0\n",
      "Eval_AverageEpLen : 10.175\n",
      "Train_AverageReturn : 7.729520797729492\n",
      "Train_StdReturn : 4.137450218200684\n",
      "Train_MaxReturn : 32.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 7.72952086553323\n",
      "Actor Loss : -0.05181959271430969\n",
      "Baseline Loss : 34.54132843017578\n",
      "Train_EnvstepsSoFar : 5001\n",
      "TimeSinceStart : 0.978503942489624\n",
      "Initial_DataCollection_AverageReturn : 7.729520797729492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 33.30769348144531\n",
      "Eval_StdReturn : 10.542110443115234\n",
      "Eval_MaxReturn : 55.0\n",
      "Eval_MinReturn : 19.0\n",
      "Eval_AverageEpLen : 33.30769230769231\n",
      "Train_AverageReturn : 34.342464447021484\n",
      "Train_StdReturn : 15.073081016540527\n",
      "Train_MaxReturn : 94.0\n",
      "Train_MinReturn : 10.0\n",
      "Train_AverageEpLen : 34.342465753424655\n",
      "Actor Loss : -0.002981244120746851\n",
      "Baseline Loss : 241.63853454589844\n",
      "Train_EnvstepsSoFar : 50145\n",
      "TimeSinceStart : 9.122442245483398\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 64.85713958740234\n",
      "Eval_StdReturn : 16.234254837036133\n",
      "Eval_MaxReturn : 93.0\n",
      "Eval_MinReturn : 41.0\n",
      "Eval_AverageEpLen : 64.85714285714286\n",
      "Train_AverageReturn : 67.70270538330078\n",
      "Train_StdReturn : 33.799781799316406\n",
      "Train_MaxReturn : 188.0\n",
      "Train_MinReturn : 13.0\n",
      "Train_AverageEpLen : 67.70270270270271\n",
      "Actor Loss : -0.02331702783703804\n",
      "Baseline Loss : 1026.9373779296875\n",
      "Train_EnvstepsSoFar : 100349\n",
      "TimeSinceStart : 17.94626474380493\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 102.0\n",
      "Eval_StdReturn : 66.53194427490234\n",
      "Eval_MaxReturn : 187.0\n",
      "Eval_MinReturn : 9.0\n",
      "Eval_AverageEpLen : 102.0\n",
      "Train_AverageReturn : 102.44898223876953\n",
      "Train_StdReturn : 37.63089370727539\n",
      "Train_MaxReturn : 273.0\n",
      "Train_MinReturn : 57.0\n",
      "Train_AverageEpLen : 102.44897959183673\n",
      "Actor Loss : -0.007855446077883244\n",
      "Baseline Loss : 1680.3095703125\n",
      "Train_EnvstepsSoFar : 150647\n",
      "TimeSinceStart : 26.72165536880493\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 134.6666717529297\n",
      "Eval_StdReturn : 66.56492614746094\n",
      "Eval_MaxReturn : 215.0\n",
      "Eval_MinReturn : 52.0\n",
      "Eval_AverageEpLen : 134.66666666666666\n",
      "Train_AverageReturn : 126.30000305175781\n",
      "Train_StdReturn : 38.90707015991211\n",
      "Train_MaxReturn : 246.0\n",
      "Train_MinReturn : 54.0\n",
      "Train_AverageEpLen : 126.3\n",
      "Actor Loss : -0.008276358246803284\n",
      "Baseline Loss : 1463.7872314453125\n",
      "Train_EnvstepsSoFar : 201041\n",
      "TimeSinceStart : 35.553719997406006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 355.0\n",
      "Eval_StdReturn : 135.0\n",
      "Eval_MaxReturn : 490.0\n",
      "Eval_MinReturn : 220.0\n",
      "Eval_AverageEpLen : 355.0\n",
      "Train_AverageReturn : 253.76190185546875\n",
      "Train_StdReturn : 140.05368041992188\n",
      "Train_MaxReturn : 584.0\n",
      "Train_MinReturn : 31.0\n",
      "Train_AverageEpLen : 253.76190476190476\n",
      "Actor Loss : -0.007435632403939962\n",
      "Baseline Loss : 18118.515625\n",
      "Train_EnvstepsSoFar : 252608\n",
      "TimeSinceStart : 44.5173978805542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 564.0\n",
      "Train_StdReturn : 293.9428405761719\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 108.0\n",
      "Train_AverageEpLen : 564.0\n",
      "Actor Loss : 0.017548248171806335\n",
      "Baseline Loss : 107427.4921875\n",
      "Train_EnvstepsSoFar : 305891\n",
      "TimeSinceStart : 54.127626180648804\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 868.8333129882812\n",
      "Train_StdReturn : 293.2975769042969\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 213.0\n",
      "Train_AverageEpLen : 868.8333333333334\n",
      "Actor Loss : 0.004670934285968542\n",
      "Baseline Loss : 171447.09375\n",
      "Train_EnvstepsSoFar : 358317\n",
      "TimeSinceStart : 63.94465255737305\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 565.3333129882812\n",
      "Train_StdReturn : 340.50225830078125\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 70.0\n",
      "Train_AverageEpLen : 565.3333333333334\n",
      "Actor Loss : 0.0004390995600260794\n",
      "Baseline Loss : 103182.6953125\n",
      "Train_EnvstepsSoFar : 411055\n",
      "TimeSinceStart : 73.58062434196472\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.010469033382833004\n",
      "Baseline Loss : 153150.421875\n",
      "Train_EnvstepsSoFar : 464129\n",
      "TimeSinceStart : 83.42877769470215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 788.5714111328125\n",
      "Train_StdReturn : 270.1607360839844\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 339.0\n",
      "Train_AverageEpLen : 788.5714285714286\n",
      "Actor Loss : -0.004711643327027559\n",
      "Baseline Loss : 113057.796875\n",
      "Train_EnvstepsSoFar : 514817\n",
      "TimeSinceStart : 93.11609601974487\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_default_s5', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=5, gae_lambda=None, normalize_advantages=True, batch_size=5000, eval_batch_size=400, discount=1.0, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=5, no_gpu=False, which_gpu=0, video_log_freq=-1, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_default_s5_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 11.685714721679688\n",
      "Eval_StdReturn : 5.402493953704834\n",
      "Eval_MaxReturn : 25.0\n",
      "Eval_MinReturn : 4.0\n",
      "Eval_AverageEpLen : 11.685714285714285\n",
      "Train_AverageReturn : 8.992818832397461\n",
      "Train_StdReturn : 5.57307767868042\n",
      "Train_MaxReturn : 54.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 8.992818671454218\n",
      "Actor Loss : -0.04734150692820549\n",
      "Baseline Loss : 61.54039764404297\n",
      "Train_EnvstepsSoFar : 5009\n",
      "TimeSinceStart : 0.9505035877227783\n",
      "Initial_DataCollection_AverageReturn : 8.992818832397461\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 57.875\n",
      "Eval_StdReturn : 18.66438865661621\n",
      "Eval_MaxReturn : 83.0\n",
      "Eval_MinReturn : 31.0\n",
      "Eval_AverageEpLen : 57.875\n",
      "Train_AverageReturn : 53.287235260009766\n",
      "Train_StdReturn : 28.042377471923828\n",
      "Train_MaxReturn : 157.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 53.287234042553195\n",
      "Actor Loss : -0.023885473608970642\n",
      "Baseline Loss : 953.6395263671875\n",
      "Train_EnvstepsSoFar : 50220\n",
      "TimeSinceStart : 9.022494554519653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 70.66666412353516\n",
      "Eval_StdReturn : 9.122621536254883\n",
      "Eval_MaxReturn : 82.0\n",
      "Eval_MinReturn : 54.0\n",
      "Eval_AverageEpLen : 70.66666666666667\n",
      "Train_AverageReturn : 80.85713958740234\n",
      "Train_StdReturn : 38.95127487182617\n",
      "Train_MaxReturn : 199.0\n",
      "Train_MinReturn : 30.0\n",
      "Train_AverageEpLen : 80.85714285714286\n",
      "Actor Loss : 0.00012794634676538408\n",
      "Baseline Loss : 1612.880126953125\n",
      "Train_EnvstepsSoFar : 100718\n",
      "TimeSinceStart : 17.868722438812256\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 104.5\n",
      "Eval_StdReturn : 45.653587341308594\n",
      "Eval_MaxReturn : 166.0\n",
      "Eval_MinReturn : 42.0\n",
      "Eval_AverageEpLen : 104.5\n",
      "Train_AverageReturn : 131.92105102539062\n",
      "Train_StdReturn : 37.0762939453125\n",
      "Train_MaxReturn : 225.0\n",
      "Train_MinReturn : 60.0\n",
      "Train_AverageEpLen : 131.92105263157896\n",
      "Actor Loss : -0.01878710649907589\n",
      "Baseline Loss : 2014.82421875\n",
      "Train_EnvstepsSoFar : 151696\n",
      "TimeSinceStart : 27.055975198745728\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 249.0\n",
      "Eval_StdReturn : 11.0\n",
      "Eval_MaxReturn : 260.0\n",
      "Eval_MinReturn : 238.0\n",
      "Eval_AverageEpLen : 249.0\n",
      "Train_AverageReturn : 203.9600067138672\n",
      "Train_StdReturn : 60.580841064453125\n",
      "Train_MaxReturn : 305.0\n",
      "Train_MinReturn : 115.0\n",
      "Train_AverageEpLen : 203.96\n",
      "Actor Loss : 0.013602327555418015\n",
      "Baseline Loss : 5490.4326171875\n",
      "Train_EnvstepsSoFar : 202628\n",
      "TimeSinceStart : 35.99697542190552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 452.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 452.0\n",
      "Eval_MinReturn : 452.0\n",
      "Eval_AverageEpLen : 452.0\n",
      "Train_AverageReturn : 282.0\n",
      "Train_StdReturn : 52.247276306152344\n",
      "Train_MaxReturn : 367.0\n",
      "Train_MinReturn : 143.0\n",
      "Train_AverageEpLen : 282.0\n",
      "Actor Loss : -0.002920124912634492\n",
      "Baseline Loss : 9044.431640625\n",
      "Train_EnvstepsSoFar : 254118\n",
      "TimeSinceStart : 45.04225182533264\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 285.5\n",
      "Eval_StdReturn : 108.5\n",
      "Eval_MaxReturn : 394.0\n",
      "Eval_MinReturn : 177.0\n",
      "Eval_AverageEpLen : 285.5\n",
      "Train_AverageReturn : 873.5\n",
      "Train_StdReturn : 141.58006286621094\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 609.0\n",
      "Train_AverageEpLen : 873.5\n",
      "Actor Loss : -0.012700155377388\n",
      "Baseline Loss : 159223.75\n",
      "Train_EnvstepsSoFar : 307160\n",
      "TimeSinceStart : 54.80096364021301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 450.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 450.0\n",
      "Eval_MinReturn : 450.0\n",
      "Eval_AverageEpLen : 450.0\n",
      "Train_AverageReturn : 744.0\n",
      "Train_StdReturn : 349.2420349121094\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 143.0\n",
      "Train_AverageEpLen : 744.0\n",
      "Actor Loss : 0.003961949143558741\n",
      "Baseline Loss : 153318.109375\n",
      "Train_EnvstepsSoFar : 358641\n",
      "TimeSinceStart : 64.4778220653534\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 950.0\n",
      "Train_StdReturn : 111.80339813232422\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 700.0\n",
      "Train_AverageEpLen : 950.0\n",
      "Actor Loss : 0.004038507118821144\n",
      "Baseline Loss : 150937.921875\n",
      "Train_EnvstepsSoFar : 412817\n",
      "TimeSinceStart : 74.5434193611145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.0036321624647825956\n",
      "Baseline Loss : 151205.40625\n",
      "Train_EnvstepsSoFar : 465878\n",
      "TimeSinceStart : 84.54077363014221\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.002980377059429884\n",
      "Baseline Loss : 142986.421875\n",
      "Train_EnvstepsSoFar : 519430\n",
      "TimeSinceStart : 94.53447890281677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "seeds = [1, 2, 3, 4, 5]\n",
    "for seed in seeds:\n",
    "    %run run_hw2.py --env_name InvertedPendulum-v4 -n 100 \\\n",
    "        --exp_name pendulum_default_s\"$seed\" \\\n",
    "        -rtg --use_baseline -na \\\n",
    "        --batch_size 5000 \\\n",
    "        --seed \"$seed\"\n",
    "    print('*' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Your task is to tune hyperparameters so that your implementation reaches maximum performance (score of 1000) in fewer **environment steps** than these default settings (note: this is **not** the same as minimizing number of policy gradient iterations: one policy gradient iteration corresponds to batch size environment steps). Try and find hyperparameters that reach a score of 1000 in as few steps as possible!\n",
    "  * Provide a set of hyperparameters that achieve high return on InvertedPendulum-v4 in as few environment steps as possible.\n",
    "  * Show learning curves for the average returns with your hyperparameters and with the default settings, with environment steps on the x-axis. Returns should be averaged over 5 seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do grid search\n",
    "# from itertools import product\n",
    "\n",
    "# params = {\n",
    "#     'baseline_gradient_steps': [5, 10],\n",
    "#     'gae_lambda': [None, 0.98],\n",
    "#     'batch_size': [100, 500, 1000, 2000, 5000],\n",
    "#     'discount': [1, 0.99],\n",
    "#     'layer_size': [32, 64], \n",
    "    \n",
    "# }\n",
    "\n",
    "# grid_params = [dict(zip(params, vals)) for vals in product(*params.values())]\n",
    "\n",
    "# for param in grid_params:\n",
    "#     bg_steps = param['baseline_gradient_steps']\n",
    "#     gae_lambda = param['gae_lambda']\n",
    "#     batch_size = param['batch_size']\n",
    "#     discount = param['discount']\n",
    "#     layer_size = param['layer_size']\n",
    "\n",
    "#     if gae_lambda is not None:\n",
    "#         %run run_hw2.py --env_name InvertedPendulum-v4 -n 100 \\\n",
    "#             --exp_name pendulum_bg_steps=\"$bg_steps\"_gae=\"$gae_lambda\"_bsize=\"$batch_size\"_dis=\"$discount\"_size=\"$layer_size\" \\\n",
    "#             -rtg --use_baseline -na \\\n",
    "#             --baseline_gradient_steps \"$bg_steps\" --gae_lambda \"$gae_lambda\" --batch_size \"$batch_size\" --discount \"$discount\" --layer_size \"$layer_size\"\n",
    "#     else:\n",
    "#         %run run_hw2.py --env_name InvertedPendulum-v4 -n 100 \\\n",
    "#             --exp_name pendulum_bg_steps=\"$bg_steps\"_gae=\"$gae_lambda\"_bsize=\"$batch_size\"_dis=\"$discount\"_size=\"$layer_size\" \\\n",
    "#             -rtg --use_baseline -na \\\n",
    "#             --baseline_gradient_steps \"$bg_steps\" --batch_size \"$batch_size\" --discount \"$discount\" --layer_size \"$layer_size\"\n",
    "    \n",
    "#     print('*' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_tuned_s1', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=10, gae_lambda=0.98, normalize_advantages=True, batch_size=300, eval_batch_size=400, discount=0.99, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=1, no_gpu=False, which_gpu=0, video_log_freq=50, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_tuned_s1_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 7.44444465637207\n",
      "Eval_StdReturn : 3.6243345737457275\n",
      "Eval_MaxReturn : 16.0\n",
      "Eval_MinReturn : 3.0\n",
      "Eval_AverageEpLen : 7.444444444444445\n",
      "Train_AverageReturn : 9.20588207244873\n",
      "Train_StdReturn : 4.992815017700195\n",
      "Train_MaxReturn : 22.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 9.205882352941176\n",
      "Actor Loss : 0.0824519619345665\n",
      "Baseline Loss : 24.259078979492188\n",
      "Train_EnvstepsSoFar : 313\n",
      "TimeSinceStart : 0.73732590675354\n",
      "Initial_DataCollection_AverageReturn : 9.20588207244873\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 28.64285659790039\n",
      "Eval_StdReturn : 13.567855834960938\n",
      "Eval_MaxReturn : 64.0\n",
      "Eval_MinReturn : 18.0\n",
      "Eval_AverageEpLen : 28.642857142857142\n",
      "Train_AverageReturn : 26.58333396911621\n",
      "Train_StdReturn : 9.123763084411621\n",
      "Train_MaxReturn : 43.0\n",
      "Train_MinReturn : 14.0\n",
      "Train_AverageEpLen : 26.583333333333332\n",
      "Actor Loss : 0.036663126200437546\n",
      "Baseline Loss : 73.5030517578125\n",
      "Train_EnvstepsSoFar : 3105\n",
      "TimeSinceStart : 2.47147274017334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 40.0\n",
      "Eval_StdReturn : 19.473058700561523\n",
      "Eval_MaxReturn : 64.0\n",
      "Eval_MinReturn : 16.0\n",
      "Eval_AverageEpLen : 40.0\n",
      "Train_AverageReturn : 48.57143020629883\n",
      "Train_StdReturn : 23.34741973876953\n",
      "Train_MaxReturn : 95.0\n",
      "Train_MinReturn : 9.0\n",
      "Train_AverageEpLen : 48.57142857142857\n",
      "Actor Loss : -0.0003008155326824635\n",
      "Baseline Loss : 115.75991821289062\n",
      "Train_EnvstepsSoFar : 6367\n",
      "TimeSinceStart : 4.2153332233428955\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 58.28571319580078\n",
      "Eval_StdReturn : 14.169528007507324\n",
      "Eval_MaxReturn : 74.0\n",
      "Eval_MinReturn : 39.0\n",
      "Eval_AverageEpLen : 58.285714285714285\n",
      "Train_AverageReturn : 77.5\n",
      "Train_StdReturn : 15.644487380981445\n",
      "Train_MaxReturn : 101.0\n",
      "Train_MinReturn : 57.0\n",
      "Train_AverageEpLen : 77.5\n",
      "Actor Loss : -0.010753093287348747\n",
      "Baseline Loss : 100.43412017822266\n",
      "Train_EnvstepsSoFar : 9783\n",
      "TimeSinceStart : 6.035010576248169\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 56.125\n",
      "Eval_StdReturn : 22.110164642333984\n",
      "Eval_MaxReturn : 87.0\n",
      "Eval_MinReturn : 12.0\n",
      "Eval_AverageEpLen : 56.125\n",
      "Train_AverageReturn : 48.0\n",
      "Train_StdReturn : 15.212776184082031\n",
      "Train_MaxReturn : 74.0\n",
      "Train_MinReturn : 19.0\n",
      "Train_AverageEpLen : 48.0\n",
      "Actor Loss : 0.02793685905635357\n",
      "Baseline Loss : 65.66825103759766\n",
      "Train_EnvstepsSoFar : 12992\n",
      "TimeSinceStart : 7.7664265632629395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 74.33333587646484\n",
      "Eval_StdReturn : 15.007405281066895\n",
      "Eval_MaxReturn : 95.0\n",
      "Eval_MinReturn : 58.0\n",
      "Eval_AverageEpLen : 74.33333333333333\n",
      "Train_AverageReturn : 56.5\n",
      "Train_StdReturn : 11.456439018249512\n",
      "Train_MaxReturn : 71.0\n",
      "Train_MinReturn : 37.0\n",
      "Train_AverageEpLen : 56.5\n",
      "Actor Loss : 0.010449680499732494\n",
      "Baseline Loss : 140.7564239501953\n",
      "Train_EnvstepsSoFar : 16400\n",
      "TimeSinceStart : 9.538927555084229\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 103.0\n",
      "Eval_StdReturn : 30.561412811279297\n",
      "Eval_MaxReturn : 141.0\n",
      "Eval_MinReturn : 52.0\n",
      "Eval_AverageEpLen : 103.0\n",
      "Train_AverageReturn : 110.25\n",
      "Train_StdReturn : 38.25163269042969\n",
      "Train_MaxReturn : 156.0\n",
      "Train_MinReturn : 50.0\n",
      "Train_AverageEpLen : 110.25\n",
      "Actor Loss : -0.007981567643582821\n",
      "Baseline Loss : 259.2500305175781\n",
      "Train_EnvstepsSoFar : 20058\n",
      "TimeSinceStart : 13.244765043258667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 205.3333282470703\n",
      "Eval_StdReturn : 58.15114212036133\n",
      "Eval_MaxReturn : 270.0\n",
      "Eval_MinReturn : 129.0\n",
      "Eval_AverageEpLen : 205.33333333333334\n",
      "Train_AverageReturn : 329.0\n",
      "Train_StdReturn : 147.0\n",
      "Train_MaxReturn : 476.0\n",
      "Train_MinReturn : 182.0\n",
      "Train_AverageEpLen : 329.0\n",
      "Actor Loss : -0.010792798362672329\n",
      "Baseline Loss : 334.82647705078125\n",
      "Train_EnvstepsSoFar : 24299\n",
      "TimeSinceStart : 15.378891468048096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 640.5\n",
      "Eval_StdReturn : 359.5\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 281.0\n",
      "Eval_AverageEpLen : 640.5\n",
      "Train_AverageReturn : 288.6666564941406\n",
      "Train_StdReturn : 198.53182983398438\n",
      "Train_MaxReturn : 569.0\n",
      "Train_MinReturn : 135.0\n",
      "Train_AverageEpLen : 288.6666666666667\n",
      "Actor Loss : -0.04837089404463768\n",
      "Baseline Loss : 680.276123046875\n",
      "Train_EnvstepsSoFar : 30203\n",
      "TimeSinceStart : 18.63561701774597\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 650.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 650.0\n",
      "Train_MinReturn : 650.0\n",
      "Train_AverageEpLen : 650.0\n",
      "Actor Loss : -0.06927467882633209\n",
      "Baseline Loss : 512.166259765625\n",
      "Train_EnvstepsSoFar : 38823\n",
      "TimeSinceStart : 22.486125946044922\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.0355813093483448\n",
      "Baseline Loss : 390.58709716796875\n",
      "Train_EnvstepsSoFar : 48021\n",
      "TimeSinceStart : 26.594595432281494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_tuned_s2', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=10, gae_lambda=0.98, normalize_advantages=True, batch_size=300, eval_batch_size=400, discount=0.99, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=2, no_gpu=False, which_gpu=0, video_log_freq=50, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_tuned_s2_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 9.95121955871582\n",
      "Eval_StdReturn : 4.9136505126953125\n",
      "Eval_MaxReturn : 27.0\n",
      "Eval_MinReturn : 4.0\n",
      "Eval_AverageEpLen : 9.951219512195122\n",
      "Train_AverageReturn : 8.628571510314941\n",
      "Train_StdReturn : 5.626903533935547\n",
      "Train_MaxReturn : 28.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 8.628571428571428\n",
      "Actor Loss : -0.053780581802129745\n",
      "Baseline Loss : 32.571022033691406\n",
      "Train_EnvstepsSoFar : 302\n",
      "TimeSinceStart : 0.7139887809753418\n",
      "Initial_DataCollection_AverageReturn : 8.628571510314941\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 37.3636360168457\n",
      "Eval_StdReturn : 15.557942390441895\n",
      "Eval_MaxReturn : 66.0\n",
      "Eval_MinReturn : 16.0\n",
      "Eval_AverageEpLen : 37.36363636363637\n",
      "Train_AverageReturn : 38.11111068725586\n",
      "Train_StdReturn : 13.461421966552734\n",
      "Train_MaxReturn : 58.0\n",
      "Train_MinReturn : 21.0\n",
      "Train_AverageEpLen : 38.111111111111114\n",
      "Actor Loss : 0.05922551453113556\n",
      "Baseline Loss : 118.24688720703125\n",
      "Train_EnvstepsSoFar : 3129\n",
      "TimeSinceStart : 2.2735016345977783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 41.70000076293945\n",
      "Eval_StdReturn : 21.128416061401367\n",
      "Eval_MaxReturn : 74.0\n",
      "Eval_MinReturn : 16.0\n",
      "Eval_AverageEpLen : 41.7\n",
      "Train_AverageReturn : 40.0\n",
      "Train_StdReturn : 12.884099006652832\n",
      "Train_MaxReturn : 57.0\n",
      "Train_MinReturn : 21.0\n",
      "Train_AverageEpLen : 40.0\n",
      "Actor Loss : -0.009136306121945381\n",
      "Baseline Loss : 94.92508697509766\n",
      "Train_EnvstepsSoFar : 6306\n",
      "TimeSinceStart : 4.028724908828735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 102.75\n",
      "Eval_StdReturn : 45.91500473022461\n",
      "Eval_MaxReturn : 177.0\n",
      "Eval_MinReturn : 53.0\n",
      "Eval_AverageEpLen : 102.75\n",
      "Train_AverageReturn : 75.0\n",
      "Train_StdReturn : 13.56466007232666\n",
      "Train_MaxReturn : 88.0\n",
      "Train_MinReturn : 49.0\n",
      "Train_AverageEpLen : 75.0\n",
      "Actor Loss : -0.08070243149995804\n",
      "Baseline Loss : 84.56769561767578\n",
      "Train_EnvstepsSoFar : 9773\n",
      "TimeSinceStart : 5.904474973678589\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 55.0\n",
      "Eval_StdReturn : 21.0\n",
      "Eval_MaxReturn : 93.0\n",
      "Eval_MinReturn : 16.0\n",
      "Eval_AverageEpLen : 55.0\n",
      "Train_AverageReturn : 62.0\n",
      "Train_StdReturn : 17.216270446777344\n",
      "Train_MaxReturn : 89.0\n",
      "Train_MinReturn : 43.0\n",
      "Train_AverageEpLen : 62.0\n",
      "Actor Loss : -0.07913228124380112\n",
      "Baseline Loss : 83.37736511230469\n",
      "Train_EnvstepsSoFar : 13240\n",
      "TimeSinceStart : 7.760026931762695\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 144.0\n",
      "Eval_StdReturn : 59.671321868896484\n",
      "Eval_MaxReturn : 193.0\n",
      "Eval_MinReturn : 60.0\n",
      "Eval_AverageEpLen : 144.0\n",
      "Train_AverageReturn : 113.66666412353516\n",
      "Train_StdReturn : 63.907920837402344\n",
      "Train_MaxReturn : 204.0\n",
      "Train_MinReturn : 66.0\n",
      "Train_AverageEpLen : 113.66666666666667\n",
      "Actor Loss : -0.04923642799258232\n",
      "Baseline Loss : 570.6795043945312\n",
      "Train_EnvstepsSoFar : 16911\n",
      "TimeSinceStart : 9.66721796989441\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 208.5\n",
      "Eval_StdReturn : 71.5\n",
      "Eval_MaxReturn : 280.0\n",
      "Eval_MinReturn : 137.0\n",
      "Eval_AverageEpLen : 208.5\n",
      "Train_AverageReturn : 92.5\n",
      "Train_StdReturn : 54.674034118652344\n",
      "Train_MaxReturn : 171.0\n",
      "Train_MinReturn : 26.0\n",
      "Train_AverageEpLen : 92.5\n",
      "Actor Loss : -0.07967690378427505\n",
      "Baseline Loss : 823.5922241210938\n",
      "Train_EnvstepsSoFar : 21498\n",
      "TimeSinceStart : 15.593619585037231\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 497.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 497.0\n",
      "Train_MinReturn : 497.0\n",
      "Train_AverageEpLen : 497.0\n",
      "Actor Loss : -0.017157072201371193\n",
      "Baseline Loss : 609.4075927734375\n",
      "Train_EnvstepsSoFar : 27230\n",
      "TimeSinceStart : 18.365413665771484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.010326395742595196\n",
      "Baseline Loss : 396.2775573730469\n",
      "Train_EnvstepsSoFar : 34609\n",
      "TimeSinceStart : 21.825052976608276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 679.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 679.0\n",
      "Eval_MinReturn : 679.0\n",
      "Eval_AverageEpLen : 679.0\n",
      "Train_AverageReturn : 211.5\n",
      "Train_StdReturn : 34.5\n",
      "Train_MaxReturn : 246.0\n",
      "Train_MinReturn : 177.0\n",
      "Train_AverageEpLen : 211.5\n",
      "Actor Loss : 0.02024495042860508\n",
      "Baseline Loss : 287.60821533203125\n",
      "Train_EnvstepsSoFar : 41586\n",
      "TimeSinceStart : 24.962624073028564\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.001988086849451065\n",
      "Baseline Loss : 396.6766662597656\n",
      "Train_EnvstepsSoFar : 51040\n",
      "TimeSinceStart : 28.965219259262085\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_tuned_s3', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=10, gae_lambda=0.98, normalize_advantages=True, batch_size=300, eval_batch_size=400, discount=0.99, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=3, no_gpu=False, which_gpu=0, video_log_freq=50, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_tuned_s3_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 8.395833015441895\n",
      "Eval_StdReturn : 3.8823297023773193\n",
      "Eval_MaxReturn : 25.0\n",
      "Eval_MinReturn : 3.0\n",
      "Eval_AverageEpLen : 8.395833333333334\n",
      "Train_AverageReturn : 7.692307472229004\n",
      "Train_StdReturn : 3.390437602996826\n",
      "Train_MaxReturn : 17.0\n",
      "Train_MinReturn : 4.0\n",
      "Train_AverageEpLen : 7.6923076923076925\n",
      "Actor Loss : -0.0529928095638752\n",
      "Baseline Loss : 11.816205978393555\n",
      "Train_EnvstepsSoFar : 300\n",
      "TimeSinceStart : 0.4088621139526367\n",
      "Initial_DataCollection_AverageReturn : 7.692307472229004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 66.83333587646484\n",
      "Eval_StdReturn : 20.144617080688477\n",
      "Eval_MaxReturn : 93.0\n",
      "Eval_MinReturn : 40.0\n",
      "Eval_AverageEpLen : 66.83333333333333\n",
      "Train_AverageReturn : 44.85714340209961\n",
      "Train_StdReturn : 18.380334854125977\n",
      "Train_MaxReturn : 66.0\n",
      "Train_MinReturn : 10.0\n",
      "Train_AverageEpLen : 44.857142857142854\n",
      "Actor Loss : -0.04191698133945465\n",
      "Baseline Loss : 182.84402465820312\n",
      "Train_EnvstepsSoFar : 3105\n",
      "TimeSinceStart : 1.979038953781128\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 100.25\n",
      "Eval_StdReturn : 25.163217544555664\n",
      "Eval_MaxReturn : 139.0\n",
      "Eval_MinReturn : 76.0\n",
      "Eval_AverageEpLen : 100.25\n",
      "Train_AverageReturn : 58.33333206176758\n",
      "Train_StdReturn : 10.434983253479004\n",
      "Train_MaxReturn : 76.0\n",
      "Train_MinReturn : 43.0\n",
      "Train_AverageEpLen : 58.333333333333336\n",
      "Actor Loss : 0.002268032403662801\n",
      "Baseline Loss : 81.38307189941406\n",
      "Train_EnvstepsSoFar : 6398\n",
      "TimeSinceStart : 3.725416660308838\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 183.6666717529297\n",
      "Eval_StdReturn : 51.860923767089844\n",
      "Eval_MaxReturn : 233.0\n",
      "Eval_MinReturn : 112.0\n",
      "Eval_AverageEpLen : 183.66666666666666\n",
      "Train_AverageReturn : 160.0\n",
      "Train_StdReturn : 41.0\n",
      "Train_MaxReturn : 201.0\n",
      "Train_MinReturn : 119.0\n",
      "Train_AverageEpLen : 160.0\n",
      "Actor Loss : 0.05437604710459709\n",
      "Baseline Loss : 417.814697265625\n",
      "Train_EnvstepsSoFar : 10012\n",
      "TimeSinceStart : 5.67519998550415\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 147.0\n",
      "Eval_StdReturn : 18.547237396240234\n",
      "Eval_MaxReturn : 163.0\n",
      "Eval_MinReturn : 121.0\n",
      "Eval_AverageEpLen : 147.0\n",
      "Train_AverageReturn : 111.66666412353516\n",
      "Train_StdReturn : 30.180936813354492\n",
      "Train_MaxReturn : 134.0\n",
      "Train_MinReturn : 69.0\n",
      "Train_AverageEpLen : 111.66666666666667\n",
      "Actor Loss : -0.047578636556863785\n",
      "Baseline Loss : 172.85850524902344\n",
      "Train_EnvstepsSoFar : 13577\n",
      "TimeSinceStart : 7.553677558898926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 473.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 473.0\n",
      "Eval_MinReturn : 473.0\n",
      "Eval_AverageEpLen : 473.0\n",
      "Train_AverageReturn : 147.3333282470703\n",
      "Train_StdReturn : 113.2617416381836\n",
      "Train_MaxReturn : 306.0\n",
      "Train_MinReturn : 49.0\n",
      "Train_AverageEpLen : 147.33333333333334\n",
      "Actor Loss : 0.00815805234014988\n",
      "Baseline Loss : 758.9965209960938\n",
      "Train_EnvstepsSoFar : 17271\n",
      "TimeSinceStart : 9.50304913520813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 584.0\n",
      "Eval_StdReturn : 416.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 168.0\n",
      "Eval_AverageEpLen : 584.0\n",
      "Train_AverageReturn : 405.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 405.0\n",
      "Train_MinReturn : 405.0\n",
      "Train_AverageEpLen : 405.0\n",
      "Actor Loss : 0.002126173349097371\n",
      "Baseline Loss : 681.064453125\n",
      "Train_EnvstepsSoFar : 23692\n",
      "TimeSinceStart : 26.27706265449524\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.025314826518297195\n",
      "Baseline Loss : 395.01971435546875\n",
      "Train_EnvstepsSoFar : 33067\n",
      "TimeSinceStart : 30.242417097091675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 628.0\n",
      "Train_StdReturn : 372.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 256.0\n",
      "Train_AverageEpLen : 628.0\n",
      "Actor Loss : 0.0104352543130517\n",
      "Baseline Loss : 565.4632568359375\n",
      "Train_EnvstepsSoFar : 42651\n",
      "TimeSinceStart : 34.28525710105896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 392.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 392.0\n",
      "Train_MinReturn : 392.0\n",
      "Train_AverageEpLen : 392.0\n",
      "Actor Loss : -0.012460027821362019\n",
      "Baseline Loss : 748.6702880859375\n",
      "Train_EnvstepsSoFar : 51678\n",
      "TimeSinceStart : 38.418801069259644\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 688.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 688.0\n",
      "Train_MinReturn : 688.0\n",
      "Train_AverageEpLen : 688.0\n",
      "Actor Loss : -0.04826854169368744\n",
      "Baseline Loss : 504.2287292480469\n",
      "Train_EnvstepsSoFar : 60510\n",
      "TimeSinceStart : 42.44420909881592\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_tuned_s4', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=10, gae_lambda=0.98, normalize_advantages=True, batch_size=300, eval_batch_size=400, discount=0.99, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=4, no_gpu=False, which_gpu=0, video_log_freq=50, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_tuned_s4_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 8.739130020141602\n",
      "Eval_StdReturn : 5.268589496612549\n",
      "Eval_MaxReturn : 35.0\n",
      "Eval_MinReturn : 3.0\n",
      "Eval_AverageEpLen : 8.73913043478261\n",
      "Train_AverageReturn : 8.685714721679688\n",
      "Train_StdReturn : 5.097738742828369\n",
      "Train_MaxReturn : 23.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 8.685714285714285\n",
      "Actor Loss : 0.002096267882734537\n",
      "Baseline Loss : 24.6317195892334\n",
      "Train_EnvstepsSoFar : 304\n",
      "TimeSinceStart : 0.3038787841796875\n",
      "Initial_DataCollection_AverageReturn : 8.685714721679688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 22.94444465637207\n",
      "Eval_StdReturn : 8.501452445983887\n",
      "Eval_MaxReturn : 43.0\n",
      "Eval_MinReturn : 12.0\n",
      "Eval_AverageEpLen : 22.944444444444443\n",
      "Train_AverageReturn : 22.5\n",
      "Train_StdReturn : 13.102617263793945\n",
      "Train_MaxReturn : 62.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 22.5\n",
      "Actor Loss : 0.006823067553341389\n",
      "Baseline Loss : 112.3158950805664\n",
      "Train_EnvstepsSoFar : 3119\n",
      "TimeSinceStart : 1.8742167949676514\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 54.625\n",
      "Eval_StdReturn : 16.874074935913086\n",
      "Eval_MaxReturn : 89.0\n",
      "Eval_MinReturn : 29.0\n",
      "Eval_AverageEpLen : 54.625\n",
      "Train_AverageReturn : 49.28571319580078\n",
      "Train_StdReturn : 7.0043721199035645\n",
      "Train_MaxReturn : 61.0\n",
      "Train_MinReturn : 39.0\n",
      "Train_AverageEpLen : 49.285714285714285\n",
      "Actor Loss : -1.3497256077243946e-05\n",
      "Baseline Loss : 56.35932540893555\n",
      "Train_EnvstepsSoFar : 6262\n",
      "TimeSinceStart : 3.73980450630188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 86.80000305175781\n",
      "Eval_StdReturn : 19.145755767822266\n",
      "Eval_MaxReturn : 110.0\n",
      "Eval_MinReturn : 57.0\n",
      "Eval_AverageEpLen : 86.8\n",
      "Train_AverageReturn : 62.599998474121094\n",
      "Train_StdReturn : 44.18415832519531\n",
      "Train_MaxReturn : 147.0\n",
      "Train_MinReturn : 18.0\n",
      "Train_AverageEpLen : 62.6\n",
      "Actor Loss : -0.014930755831301212\n",
      "Baseline Loss : 364.3577575683594\n",
      "Train_EnvstepsSoFar : 9552\n",
      "TimeSinceStart : 5.536388874053955\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 67.16666412353516\n",
      "Eval_StdReturn : 31.02910804748535\n",
      "Eval_MaxReturn : 126.0\n",
      "Eval_MinReturn : 40.0\n",
      "Eval_AverageEpLen : 67.16666666666667\n",
      "Train_AverageReturn : 76.5\n",
      "Train_StdReturn : 33.7675895690918\n",
      "Train_MaxReturn : 127.0\n",
      "Train_MinReturn : 32.0\n",
      "Train_AverageEpLen : 76.5\n",
      "Actor Loss : -0.006389352958649397\n",
      "Baseline Loss : 205.35867309570312\n",
      "Train_EnvstepsSoFar : 13018\n",
      "TimeSinceStart : 7.386133670806885\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 142.6666717529297\n",
      "Eval_StdReturn : 25.746627807617188\n",
      "Eval_MaxReturn : 177.0\n",
      "Eval_MinReturn : 115.0\n",
      "Eval_AverageEpLen : 142.66666666666666\n",
      "Train_AverageReturn : 135.3333282470703\n",
      "Train_StdReturn : 56.405277252197266\n",
      "Train_MaxReturn : 209.0\n",
      "Train_MinReturn : 72.0\n",
      "Train_AverageEpLen : 135.33333333333334\n",
      "Actor Loss : -0.0024739489890635014\n",
      "Baseline Loss : 419.383544921875\n",
      "Train_EnvstepsSoFar : 16846\n",
      "TimeSinceStart : 9.362091064453125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 237.0\n",
      "Eval_StdReturn : 39.0\n",
      "Eval_MaxReturn : 276.0\n",
      "Eval_MinReturn : 198.0\n",
      "Eval_AverageEpLen : 237.0\n",
      "Train_AverageReturn : 197.0\n",
      "Train_StdReturn : 18.0\n",
      "Train_MaxReturn : 215.0\n",
      "Train_MinReturn : 179.0\n",
      "Train_AverageEpLen : 197.0\n",
      "Actor Loss : 0.0037195561453700066\n",
      "Baseline Loss : 115.37944030761719\n",
      "Train_EnvstepsSoFar : 20844\n",
      "TimeSinceStart : 15.320099353790283\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 140.75\n",
      "Eval_StdReturn : 56.579036712646484\n",
      "Eval_MaxReturn : 225.0\n",
      "Eval_MinReturn : 69.0\n",
      "Eval_AverageEpLen : 140.75\n",
      "Train_AverageReturn : 67.80000305175781\n",
      "Train_StdReturn : 22.32845687866211\n",
      "Train_MaxReturn : 103.0\n",
      "Train_MinReturn : 33.0\n",
      "Train_AverageEpLen : 67.8\n",
      "Actor Loss : -0.08098099380731583\n",
      "Baseline Loss : 483.14141845703125\n",
      "Train_EnvstepsSoFar : 24960\n",
      "TimeSinceStart : 17.42976403236389\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 487.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 487.0\n",
      "Eval_MinReturn : 487.0\n",
      "Eval_AverageEpLen : 487.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.009447809308767319\n",
      "Baseline Loss : 500.09136962890625\n",
      "Train_EnvstepsSoFar : 33413\n",
      "TimeSinceStart : 21.14803147315979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : 0.010454627685248852\n",
      "Baseline Loss : 400.4726257324219\n",
      "Train_EnvstepsSoFar : 39793\n",
      "TimeSinceStart : 24.327532291412354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.030305668711662292\n",
      "Baseline Loss : 404.91741943359375\n",
      "Train_EnvstepsSoFar : 49380\n",
      "TimeSinceStart : 28.42283058166504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Namespace(env_name='InvertedPendulum-v4', exp_name='pendulum_tuned_s5', n_iter=100, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=10, gae_lambda=0.98, normalize_advantages=True, batch_size=300, eval_batch_size=400, discount=0.99, learning_rate=0.005, n_layers=2, layer_size=64, ep_len=None, seed=5, no_gpu=False, which_gpu=0, video_log_freq=50, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp4\\q2_pg_pendulum_tuned_s5_InvertedPendulum-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 6.396825313568115\n",
      "Eval_StdReturn : 2.8258426189422607\n",
      "Eval_MaxReturn : 18.0\n",
      "Eval_MinReturn : 3.0\n",
      "Eval_AverageEpLen : 6.396825396825397\n",
      "Train_AverageReturn : 7.560975551605225\n",
      "Train_StdReturn : 3.7614786624908447\n",
      "Train_MaxReturn : 24.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 7.560975609756097\n",
      "Actor Loss : -0.012897463515400887\n",
      "Baseline Loss : 14.881889343261719\n",
      "Train_EnvstepsSoFar : 310\n",
      "TimeSinceStart : 0.43993306159973145\n",
      "Initial_DataCollection_AverageReturn : 7.560975551605225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 20.799999237060547\n",
      "Eval_StdReturn : 13.581604957580566\n",
      "Eval_MaxReturn : 59.0\n",
      "Eval_MinReturn : 8.0\n",
      "Eval_AverageEpLen : 20.8\n",
      "Train_AverageReturn : 18.75\n",
      "Train_StdReturn : 7.701460838317871\n",
      "Train_MaxReturn : 35.0\n",
      "Train_MinReturn : 6.0\n",
      "Train_AverageEpLen : 18.75\n",
      "Actor Loss : -0.06801201403141022\n",
      "Baseline Loss : 43.45685577392578\n",
      "Train_EnvstepsSoFar : 3109\n",
      "TimeSinceStart : 2.115892171859741\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 50.75\n",
      "Eval_StdReturn : 15.522161483764648\n",
      "Eval_MaxReturn : 82.0\n",
      "Eval_MinReturn : 34.0\n",
      "Eval_AverageEpLen : 50.75\n",
      "Train_AverageReturn : 33.55555725097656\n",
      "Train_StdReturn : 11.963937759399414\n",
      "Train_MaxReturn : 56.0\n",
      "Train_MinReturn : 15.0\n",
      "Train_AverageEpLen : 33.55555555555556\n",
      "Actor Loss : -0.027978995814919472\n",
      "Baseline Loss : 101.74870300292969\n",
      "Train_EnvstepsSoFar : 6358\n",
      "TimeSinceStart : 3.937533140182495\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 78.33333587646484\n",
      "Eval_StdReturn : 25.545167922973633\n",
      "Eval_MaxReturn : 118.0\n",
      "Eval_MinReturn : 52.0\n",
      "Eval_AverageEpLen : 78.33333333333333\n",
      "Train_AverageReturn : 65.4000015258789\n",
      "Train_StdReturn : 20.34305763244629\n",
      "Train_MaxReturn : 106.0\n",
      "Train_MinReturn : 53.0\n",
      "Train_AverageEpLen : 65.4\n",
      "Actor Loss : 0.058983609080314636\n",
      "Baseline Loss : 85.51349639892578\n",
      "Train_EnvstepsSoFar : 9782\n",
      "TimeSinceStart : 5.821105718612671\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 94.80000305175781\n",
      "Eval_StdReturn : 47.20338821411133\n",
      "Eval_MaxReturn : 158.0\n",
      "Eval_MinReturn : 42.0\n",
      "Eval_AverageEpLen : 94.8\n",
      "Train_AverageReturn : 79.25\n",
      "Train_StdReturn : 25.63566780090332\n",
      "Train_MaxReturn : 102.0\n",
      "Train_MinReturn : 39.0\n",
      "Train_AverageEpLen : 79.25\n",
      "Actor Loss : -0.02722138725221157\n",
      "Baseline Loss : 208.42034912109375\n",
      "Train_EnvstepsSoFar : 13123\n",
      "TimeSinceStart : 7.776065111160278\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 205.0\n",
      "Eval_StdReturn : 26.0\n",
      "Eval_MaxReturn : 231.0\n",
      "Eval_MinReturn : 179.0\n",
      "Eval_AverageEpLen : 205.0\n",
      "Train_AverageReturn : 268.5\n",
      "Train_StdReturn : 6.5\n",
      "Train_MaxReturn : 275.0\n",
      "Train_MinReturn : 262.0\n",
      "Train_AverageEpLen : 268.5\n",
      "Actor Loss : -0.030521634966135025\n",
      "Baseline Loss : 379.9000244140625\n",
      "Train_EnvstepsSoFar : 17147\n",
      "TimeSinceStart : 9.822842121124268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 392.0\n",
      "Eval_StdReturn : 252.0\n",
      "Eval_MaxReturn : 644.0\n",
      "Eval_MinReturn : 140.0\n",
      "Eval_AverageEpLen : 392.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.004274316597729921\n",
      "Baseline Loss : 687.697509765625\n",
      "Train_EnvstepsSoFar : 21511\n",
      "TimeSinceStart : 15.526879787445068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 638.5\n",
      "Eval_StdReturn : 361.5\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 277.0\n",
      "Eval_AverageEpLen : 638.5\n",
      "Train_AverageReturn : 556.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 556.0\n",
      "Train_MinReturn : 556.0\n",
      "Train_AverageEpLen : 556.0\n",
      "Actor Loss : 0.020657170563936234\n",
      "Baseline Loss : 599.9954223632812\n",
      "Train_EnvstepsSoFar : 28311\n",
      "TimeSinceStart : 18.689213275909424\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.04776802659034729\n",
      "Baseline Loss : 394.3141174316406\n",
      "Train_EnvstepsSoFar : 37887\n",
      "TimeSinceStart : 22.87862491607666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 471.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 471.0\n",
      "Eval_MinReturn : 471.0\n",
      "Eval_AverageEpLen : 471.0\n",
      "Train_AverageReturn : 784.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 784.0\n",
      "Train_MinReturn : 784.0\n",
      "Train_AverageEpLen : 784.0\n",
      "Actor Loss : 0.00249302014708519\n",
      "Baseline Loss : 241.26248168945312\n",
      "Train_EnvstepsSoFar : 47893\n",
      "TimeSinceStart : 26.939995288848877\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Actor Loss : -0.013765010982751846\n",
      "Baseline Loss : 394.5435485839844\n",
      "Train_EnvstepsSoFar : 57466\n",
      "TimeSinceStart : 30.916054487228394\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# use following hyperparameters: baseline_gradient_steps=10, gae_lambda=0.98, batch_size=300, discount=0.99\n",
    "\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "for seed in seeds:\n",
    "    %run run_hw2.py --env_name InvertedPendulum-v4 -n 100 \\\n",
    "        --exp_name pendulum_tuned_s\"$seed\" \\\n",
    "        -rtg --use_baseline -na \\\n",
    "        --baseline_gradient_steps 10 --gae_lambda 0.98 --batch_size 300 --discount 0.99 \\\n",
    "        --seed \"$seed\" \\\n",
    "        --video_log_freq 50\n",
    "    print('*' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make review easier, I print out every 10 iterations, and based on the print outs use `Train_EnvstepsSoFar` where it reached 1000 scores at the first time to evaluate. The result is:\n",
    "* Default hyperparameters\n",
    "  | seed | iters | Train_EnvstepsSoFar |\n",
    "  |------|-------|---------------------|\n",
    "  | 1    | 90    | 465160              | \n",
    "  | 2    | 60    | 307162              | \n",
    "  | 3    | 70    | 360033              | \n",
    "  | 4    | 60    | 305891              | \n",
    "  | 5    | 80    | 412817              | \n",
    "  | Avg. |       | 370212.6            |\n",
    "\n",
    "* Tuned hyperparameters\n",
    "  | seed | iters | Train_EnvstepsSoFar |\n",
    "  |------|-------|---------------------|\n",
    "  | 1    | 90    | 38823               | \n",
    "  | 2    | 70    | 27230               | \n",
    "  | 3    | 70    | 33067               | \n",
    "  | 4    | 90    | 39793               | \n",
    "  | 5    | 80    | 37887               | \n",
    "  | Avg. |       | 35360               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../../run_logs/exp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit: Humanoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5 (Humanoid-v4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s put everything we’ve learned together to learn a policy for a more complicated environment! The Humanoid-v4 environment in Gym trains a humanoid to walk, from scratch!\n",
    "\n",
    "If you’ve implemented everything correctly, you shouldn’t have to do anything new for this section. Just run the following command - we’ve filled in some hyperparameters that should work:\n",
    "\n",
    "* It should achieve a final return of at least 600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(env_name='Humanoid-v4', exp_name='humanoid', n_iter=1000, use_reward_to_go=True, use_baseline=True, baseline_learning_rate=0.005, baseline_gradient_steps=50, gae_lambda=0.97, normalize_advantages=True, batch_size=50000, eval_batch_size=400, discount=0.99, learning_rate=0.001, n_layers=3, layer_size=256, ep_len=1000, seed=1, no_gpu=False, which_gpu=0, video_log_freq=50, scalar_log_freq=1, action_noise_std=0)\n",
      "########################\n",
      "logging outputs to  C:\\Users\\user\\Colab\\Berkeley_DeepRL_Fall2023\\hw2\\cs285\\scripts\\../../run_logs\\exp5\\q2_pg_humanoid_Humanoid-v4\n",
      "########################\n",
      "Using CPU.\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 112.75259399414062\n",
      "Eval_StdReturn : 41.44998550415039\n",
      "Eval_MaxReturn : 217.75685119628906\n",
      "Eval_MinReturn : 76.37547302246094\n",
      "Eval_AverageEpLen : 22.94736842105263\n",
      "Train_AverageReturn : 109.63346862792969\n",
      "Train_StdReturn : 33.54679870605469\n",
      "Train_MaxReturn : 331.59039306640625\n",
      "Train_MinReturn : 71.57970428466797\n",
      "Train_AverageEpLen : 22.513282305267897\n",
      "Actor Loss : -0.007861638441681862\n",
      "Baseline Loss : 1263.3665771484375\n",
      "Train_EnvstepsSoFar : 50002\n",
      "TimeSinceStart : 39.93314790725708\n",
      "Initial_DataCollection_AverageReturn : 109.63346862792969\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 685.6915283203125\n",
      "Eval_StdReturn : 122.16046142578125\n",
      "Eval_MaxReturn : 870.2973022460938\n",
      "Eval_MinReturn : 533.2860107421875\n",
      "Eval_AverageEpLen : 124.0\n",
      "Train_AverageReturn : 657.3763427734375\n",
      "Train_StdReturn : 145.42095947265625\n",
      "Train_MaxReturn : 1113.109375\n",
      "Train_MinReturn : 343.1331481933594\n",
      "Train_AverageEpLen : 121.41162227602905\n",
      "Actor Loss : -0.0021756526548415422\n",
      "Baseline Loss : 706.5780639648438\n",
      "Train_EnvstepsSoFar : 5005440\n",
      "TimeSinceStart : 4371.063323259354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 739.7683715820312\n",
      "Eval_StdReturn : 34.704345703125\n",
      "Eval_MaxReturn : 779.9266967773438\n",
      "Eval_MinReturn : 695.2542114257812\n",
      "Eval_AverageEpLen : 137.0\n",
      "Train_AverageReturn : 693.3440551757812\n",
      "Train_StdReturn : 152.58895874023438\n",
      "Train_MaxReturn : 1562.8017578125\n",
      "Train_MinReturn : 460.2041320800781\n",
      "Train_AverageEpLen : 125.225\n",
      "Actor Loss : -0.027072109282016754\n",
      "Baseline Loss : 929.4812622070312\n",
      "Train_EnvstepsSoFar : 10011367\n",
      "TimeSinceStart : 8920.157211303711\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 667.3656616210938\n",
      "Eval_StdReturn : 32.18560028076172\n",
      "Eval_MaxReturn : 722.2407836914062\n",
      "Eval_MinReturn : 639.9525756835938\n",
      "Eval_AverageEpLen : 120.25\n",
      "Train_AverageReturn : 722.8049926757812\n",
      "Train_StdReturn : 144.12074279785156\n",
      "Train_MaxReturn : 1367.665283203125\n",
      "Train_MinReturn : 487.2033386230469\n",
      "Train_AverageEpLen : 129.31007751937983\n",
      "Actor Loss : -0.005798250902444124\n",
      "Baseline Loss : 890.8983154296875\n",
      "Train_EnvstepsSoFar : 15018593\n",
      "TimeSinceStart : 13494.028938770294\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 720.69873046875\n",
      "Eval_StdReturn : 63.88249969482422\n",
      "Eval_MaxReturn : 798.2440185546875\n",
      "Eval_MinReturn : 631.1577758789062\n",
      "Eval_AverageEpLen : 128.5\n",
      "Train_AverageReturn : 771.4006958007812\n",
      "Train_StdReturn : 155.96249389648438\n",
      "Train_MaxReturn : 1450.3212890625\n",
      "Train_MinReturn : 504.4825744628906\n",
      "Train_AverageEpLen : 138.28729281767957\n",
      "Actor Loss : 0.03141821548342705\n",
      "Baseline Loss : 1086.9224853515625\n",
      "Train_EnvstepsSoFar : 20024957\n",
      "TimeSinceStart : 18131.511080265045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 500 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 833.5380249023438\n",
      "Eval_StdReturn : 43.5869026184082\n",
      "Eval_MaxReturn : 875.3214111328125\n",
      "Eval_MinReturn : 773.399169921875\n",
      "Eval_AverageEpLen : 154.0\n",
      "Train_AverageReturn : 762.7042236328125\n",
      "Train_StdReturn : 161.42010498046875\n",
      "Train_MaxReturn : 1494.5126953125\n",
      "Train_MinReturn : 472.5068664550781\n",
      "Train_AverageEpLen : 139.36211699164346\n",
      "Actor Loss : 0.010455392301082611\n",
      "Baseline Loss : 1093.9927978515625\n",
      "Train_EnvstepsSoFar : 25032778\n",
      "TimeSinceStart : 22793.07902240753\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 600 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 795.6004028320312\n",
      "Eval_StdReturn : 42.483882904052734\n",
      "Eval_MaxReturn : 843.4979248046875\n",
      "Eval_MinReturn : 740.239990234375\n",
      "Eval_AverageEpLen : 144.66666666666666\n",
      "Train_AverageReturn : 841.4886474609375\n",
      "Train_StdReturn : 180.92369079589844\n",
      "Train_MaxReturn : 1884.227783203125\n",
      "Train_MinReturn : 525.5903930664062\n",
      "Train_AverageEpLen : 151.23867069486406\n",
      "Actor Loss : -0.010727985762059689\n",
      "Baseline Loss : 1077.07421875\n",
      "Train_EnvstepsSoFar : 30039720\n",
      "TimeSinceStart : 27471.87339615822\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 700 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 853.88037109375\n",
      "Eval_StdReturn : 15.101802825927734\n",
      "Eval_MaxReturn : 864.9368896484375\n",
      "Eval_MinReturn : 832.5277099609375\n",
      "Eval_AverageEpLen : 162.66666666666666\n",
      "Train_AverageReturn : 848.504638671875\n",
      "Train_StdReturn : 210.0726318359375\n",
      "Train_MaxReturn : 1954.2230224609375\n",
      "Train_MinReturn : 480.1923522949219\n",
      "Train_AverageEpLen : 154.59259259259258\n",
      "Actor Loss : -0.005566068459302187\n",
      "Baseline Loss : 1640.9793701171875\n",
      "Train_EnvstepsSoFar : 35048118\n",
      "TimeSinceStart : 32235.964170217514\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 800 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 979.607177734375\n",
      "Eval_StdReturn : 129.87879943847656\n",
      "Eval_MaxReturn : 1163.129638671875\n",
      "Eval_MinReturn : 881.3357543945312\n",
      "Eval_AverageEpLen : 174.66666666666666\n",
      "Train_AverageReturn : 766.9151000976562\n",
      "Train_StdReturn : 155.73255920410156\n",
      "Train_MaxReturn : 1268.9659423828125\n",
      "Train_MinReturn : 454.0807189941406\n",
      "Train_AverageEpLen : 139.4289693593315\n",
      "Actor Loss : -0.002328659174963832\n",
      "Baseline Loss : 985.3017578125\n",
      "Train_EnvstepsSoFar : 40055585\n",
      "TimeSinceStart : 36932.2216527462\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 900 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 796.9186401367188\n",
      "Eval_StdReturn : 23.231081008911133\n",
      "Eval_MaxReturn : 829.7714233398438\n",
      "Eval_MinReturn : 780.27880859375\n",
      "Eval_AverageEpLen : 145.33333333333334\n",
      "Train_AverageReturn : 814.824951171875\n",
      "Train_StdReturn : 154.40415954589844\n",
      "Train_MaxReturn : 1303.9625244140625\n",
      "Train_MinReturn : 465.0023193359375\n",
      "Train_AverageEpLen : 147.59587020648968\n",
      "Actor Loss : -0.0014931345358490944\n",
      "Baseline Loss : 1115.694091796875\n",
      "Train_EnvstepsSoFar : 45062366\n",
      "TimeSinceStart : 41619.215368032455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 803.76953125\n",
      "Eval_StdReturn : 119.37196350097656\n",
      "Eval_MaxReturn : 890.98583984375\n",
      "Eval_MinReturn : 634.9835205078125\n",
      "Eval_AverageEpLen : 145.0\n",
      "Train_AverageReturn : 852.5276489257812\n",
      "Train_StdReturn : 172.21517944335938\n",
      "Train_MaxReturn : 1806.419189453125\n",
      "Train_MinReturn : 521.7476806640625\n",
      "Train_AverageEpLen : 152.92660550458714\n",
      "Actor Loss : -0.018559159711003304\n",
      "Baseline Loss : 1348.836181640625\n",
      "Train_EnvstepsSoFar : 50069537\n",
      "TimeSinceStart : 46321.61032509804\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Collecting video rollouts...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# executing for about 12.8 hr (run on localhost CPU, i5-12400)\n",
    "%run run_hw2.py \\\n",
    "    --env_name Humanoid-v4 --ep_len 1000 \\\n",
    "    --discount 0.99 -n 1000 -l 3 -s 256 -b 50000 -lr 0.001 \\\n",
    "    --baseline_gradient_steps 50 \\\n",
    "    -na --use_reward_to_go --use_baseline --gae_lambda 0.97 \\\n",
    "    --exp_name humanoid --video_log_freq 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../../run_logs/exp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
