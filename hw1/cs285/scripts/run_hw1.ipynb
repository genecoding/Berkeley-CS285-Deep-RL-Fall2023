{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLXw6zd-k3Xd"
   },
   "source": [
    "## Setup\n",
    "\n",
    "You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4HBPnmbIPPyl"
   },
   "outputs": [],
   "source": [
    "# #@title mount your Google Drive\n",
    "# #@markdown Your work will be stored in a folder called `cs285_f2023` by default to prevent Colab instance timeouts from deleting your edits.\n",
    "\n",
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1693168282918,
     "user": {
      "displayName": "Joey Hong",
      "userId": "05817993003903533998"
     },
     "user_tz": 420
    },
    "id": "OuCfTLJIx5nQ"
   },
   "outputs": [],
   "source": [
    "# #@title set up mount symlink\n",
    "\n",
    "# DRIVE_PATH = '/content/gdrive/My\\ Drive/cs285_f2023'\n",
    "# DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "# if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "#   %mkdir $DRIVE_PATH\n",
    "\n",
    "# ## the space in `My Drive` causes some issues,\n",
    "# ## make a symlink to avoid this\n",
    "# SYM_PATH = '/content/cs285_f2023'\n",
    "# if not os.path.exists(SYM_PATH):\n",
    "#   !ln -s $DRIVE_PATH $SYM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8694,
     "status": "ok",
     "timestamp": 1693168293394,
     "user": {
      "displayName": "Joey Hong",
      "userId": "05817993003903533998"
     },
     "user_tz": 420
    },
    "id": "XTtWDO-Bkqnn",
    "outputId": "051f55fa-77d6-420f-f28f-65d2581bea6d"
   },
   "outputs": [],
   "source": [
    "# #@title apt install requirements\n",
    "\n",
    "# #@markdown Run each section with Shift+Enter\n",
    "\n",
    "# #@markdown Double-click on section headers to show code.\n",
    "\n",
    "# !apt update\n",
    "# !apt install -y --no-install-recommends \\\n",
    "#         build-essential \\\n",
    "#         curl \\\n",
    "#         git \\\n",
    "#         gnupg2 \\\n",
    "#         make \\\n",
    "#         cmake \\\n",
    "#         ffmpeg \\\n",
    "#         swig \\\n",
    "#         libz-dev \\\n",
    "#         unzip \\\n",
    "#         zlib1g-dev \\\n",
    "#         libglfw3 \\\n",
    "#         libglfw3-dev \\\n",
    "#         libxrandr2 \\\n",
    "#         libxinerama-dev \\\n",
    "#         libxi6 \\\n",
    "#         libxcursor-dev \\\n",
    "#         libgl1-mesa-dev \\\n",
    "#         libgl1-mesa-glx \\\n",
    "#         libglew-dev \\\n",
    "#         libosmesa6-dev \\\n",
    "#         lsb-release \\\n",
    "#         ack-grep \\\n",
    "#         patchelf \\\n",
    "#         wget \\\n",
    "#         xpra \\\n",
    "#         xserver-xorg-dev \\\n",
    "#         ffmpeg\n",
    "# !apt-get install python-opengl -y\n",
    "# !apt install xvfb -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11889,
     "status": "ok",
     "timestamp": 1693168307436,
     "user": {
      "displayName": "Joey Hong",
      "userId": "05817993003903533998"
     },
     "user_tz": 420
    },
    "id": "X_aXQac0f3pr",
    "outputId": "56842496-0ab7-4810-e612-838cb1e511d6"
   },
   "outputs": [],
   "source": [
    "# #@title clone homework repo\n",
    "\n",
    "# %cd $SYM_PATH\n",
    "# !git clone https://github.com/berkeleydeeprlcourse/homework_fall2023.git\n",
    "# %cd hw1\n",
    "# %pip install -r requirements_colab.txt\n",
    "# %pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1693168311112,
     "user": {
      "displayName": "Joey Hong",
      "userId": "05817993003903533998"
     },
     "user_tz": 420
    },
    "id": "8y_M1tGxmGhT",
    "outputId": "02f2f252-af17-48a9-e74e-4281d367b208"
   },
   "outputs": [],
   "source": [
    "# #@title set up virtual display\n",
    "\n",
    "# from pyvirtualdisplay import Display\n",
    "\n",
    "# display = Display(visible=0, size=(1400, 900))\n",
    "# display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "executionInfo": {
     "elapsed": 21359,
     "status": "ok",
     "timestamp": 1693168332469,
     "user": {
      "displayName": "Joey Hong",
      "userId": "05817993003903533998"
     },
     "user_tz": 420
    },
    "id": "y7cywOEgo4a8",
    "outputId": "683fb60a-0024-464b-9126-04349506d84c"
   },
   "outputs": [],
   "source": [
    "#@title test virtual display\n",
    "\n",
    "#@markdown If you see a video of a four-legged ant fumbling about, setup is complete!\n",
    "\n",
    "import gym\n",
    "from cs285.infrastructure.colab_utils import (\n",
    "    wrap_env,\n",
    "    show_video\n",
    ")\n",
    "\n",
    "env = wrap_env(gym.make(\"Ant-v4\", render_mode='rgb_array'))\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    obs, rew, term, _ = env.step(env.action_space.sample() )\n",
    "    if term:\n",
    "      break;\n",
    "\n",
    "env.close()\n",
    "print('Loading video...')\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioxyP78KPUjB"
   },
   "source": [
    "## Editing Code\n",
    "\n",
    "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`cs285_f2020/...`). Double click a file to open an editor. You will need to edit code in the following files:\n",
    "```markdown\n",
    "* cs285/policies/MLP_policy.py\n",
    "* cs285/infrastructure/utils.py\n",
    "* cs285/scripts/run_hw1.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UunygyDXrx7k"
   },
   "source": [
    "## Run Behavior Cloning (Problem 1)\n",
    "\n",
    "Note that there is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "TOaW1-rpPUjC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\IPython\\extensions\\autoreload.py:121: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "#@title imports\n",
    "\n",
    "import time\n",
    "\n",
    "from cs285.scripts.run_hw1 import run_training_loop\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1693169213803,
     "user": {
      "displayName": "Joey Hong",
      "userId": "05817993003903533998"
     },
     "user_tz": 420
    },
    "id": "imnAkQ6jryL7"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  #@markdown expert data\n",
    "  # expert_policy_file = 'cs285/policies/experts/Ant.pkl' #@param\n",
    "  expert_policy_file = '../../cs285/policies/experts/Ant.pkl' #@param\n",
    "  # expert_data = 'cs285/expert_data/expert_data_Ant-v4.pkl' #@param\n",
    "  expert_data = '../../cs285/expert_data/expert_data_Ant-v4.pkl' #@param\n",
    "  env_name = 'Ant-v4' #@param ['Ant-v4', 'Walker2d-v4', 'HalfCheetah-v4', 'Hopper-v4']\n",
    "  exp_name = 'bc_ant' #@param\n",
    "  do_dagger = False #@param {type: \"boolean\"}\n",
    "  ep_len = 1000 #@param {type: \"integer\"}\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1000 #@param {type: \"integer\"})\n",
    "  n_iter = 1 #@param {type: \"integer\"})\n",
    "\n",
    "  #@markdown batches & buffers\n",
    "  batch_size_initial = 2000 #@param {type: \"integer\"})\n",
    "  batch_size = 1000 #@param {type: \"integer\"})\n",
    "  eval_batch_size = 5000 #@param {type: \"integer\"}\n",
    "  train_batch_size = 100 #@param {type: \"integer\"}\n",
    "  max_replay_buffer_size = 1000000 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown network\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "  learning_rate = 5e-3 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  video_log_freq = 5 #@param {type: \"integer\"}\n",
    "  scalar_log_freq = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown gpu & run-time settings\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1693169216362,
     "user": {
      "displayName": "Joey Hong",
      "userId": "05817993003903533998"
     },
     "user_tz": 420
    },
    "id": "7UkzHBfxsxH8"
   },
   "outputs": [],
   "source": [
    "#@title create directory for logging\n",
    "\n",
    "import os\n",
    "\n",
    "def create_log_dir(args, part=''):\n",
    "    if args.do_dagger:\n",
    "        logdir_prefix = 'q2_'  # The autograder uses the prefix `q2_`\n",
    "        assert args.n_iter>1, ('DAgger needs more than 1 iteration (n_iter>1) of training, to iteratively query the expert and train (after 1st warmstarting from behavior cloning).')\n",
    "    else:\n",
    "        logdir_prefix = 'q1_'  # The autograder uses the prefix `q1_`\n",
    "        assert args.n_iter==1, ('Vanilla behavior cloning collects expert data just once (n_iter=1)')\n",
    "    \n",
    "    # data_path ='/content/cs285_f2023/hw1/data'\n",
    "    data_path = '../../data'\n",
    "    if not (os.path.exists(data_path)):\n",
    "        os.makedirs(data_path)\n",
    "    logdir = logdir_prefix + args.exp_name + '_' + args.env_name #+ \\\n",
    "             # '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    logdir = os.path.join(data_path, part, logdir)\n",
    "    args['logdir'] = logdir\n",
    "    if not(os.path.exists(logdir)):\n",
    "        os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (3.2) Experiment with one set of hyperparameters that affects the performance of the behavioral cloning agent, such as the amount of training steps, the amount of expert data provided, or something that you come up with yourself. For one of the tasks used in the previous question, show a graph of how the BC agentâ€™s performance varies with the value of this hyperparameter. In the caption for the graph, state the hyperparameter and a brief rationale for why you chose it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To experiment different values of hyperparameters that affects the performance, I choose \n",
    "* num_agent_train_steps_per_iter\n",
    "* train_batch_size\n",
    "* size\n",
    "\n",
    "with different values as below.  \n",
    "Because num_agent_train_steps_per_iter is one of key hyperparameters that affects the performance most significantly, and also try another two hyperparameters (train_batch_size & size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'num_agent_train_steps_per_iter': 1000,\n",
       "  'train_batch_size': 200,\n",
       "  'size': 64,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=1000-tb_size=200-size=64'},\n",
       " {'num_agent_train_steps_per_iter': 1000,\n",
       "  'train_batch_size': 200,\n",
       "  'size': 128,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=1000-tb_size=200-size=128'},\n",
       " {'num_agent_train_steps_per_iter': 1000,\n",
       "  'train_batch_size': 200,\n",
       "  'size': 256,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=1000-tb_size=200-size=256'},\n",
       " {'num_agent_train_steps_per_iter': 1000,\n",
       "  'train_batch_size': 500,\n",
       "  'size': 64,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=1000-tb_size=500-size=64'},\n",
       " {'num_agent_train_steps_per_iter': 1000,\n",
       "  'train_batch_size': 500,\n",
       "  'size': 128,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=1000-tb_size=500-size=128'},\n",
       " {'num_agent_train_steps_per_iter': 1000,\n",
       "  'train_batch_size': 500,\n",
       "  'size': 256,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=1000-tb_size=500-size=256'},\n",
       " {'num_agent_train_steps_per_iter': 1000,\n",
       "  'train_batch_size': 1000,\n",
       "  'size': 64,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=1000-tb_size=1000-size=64'},\n",
       " {'num_agent_train_steps_per_iter': 1000,\n",
       "  'train_batch_size': 1000,\n",
       "  'size': 128,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=1000-tb_size=1000-size=128'},\n",
       " {'num_agent_train_steps_per_iter': 1000,\n",
       "  'train_batch_size': 1000,\n",
       "  'size': 256,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=1000-tb_size=1000-size=256'},\n",
       " {'num_agent_train_steps_per_iter': 5000,\n",
       "  'train_batch_size': 200,\n",
       "  'size': 64,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=5000-tb_size=200-size=64'},\n",
       " {'num_agent_train_steps_per_iter': 5000,\n",
       "  'train_batch_size': 200,\n",
       "  'size': 128,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=5000-tb_size=200-size=128'},\n",
       " {'num_agent_train_steps_per_iter': 5000,\n",
       "  'train_batch_size': 200,\n",
       "  'size': 256,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=5000-tb_size=200-size=256'},\n",
       " {'num_agent_train_steps_per_iter': 5000,\n",
       "  'train_batch_size': 500,\n",
       "  'size': 64,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=5000-tb_size=500-size=64'},\n",
       " {'num_agent_train_steps_per_iter': 5000,\n",
       "  'train_batch_size': 500,\n",
       "  'size': 128,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=5000-tb_size=500-size=128'},\n",
       " {'num_agent_train_steps_per_iter': 5000,\n",
       "  'train_batch_size': 500,\n",
       "  'size': 256,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=5000-tb_size=500-size=256'},\n",
       " {'num_agent_train_steps_per_iter': 5000,\n",
       "  'train_batch_size': 1000,\n",
       "  'size': 64,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=5000-tb_size=1000-size=64'},\n",
       " {'num_agent_train_steps_per_iter': 5000,\n",
       "  'train_batch_size': 1000,\n",
       "  'size': 128,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=5000-tb_size=1000-size=128'},\n",
       " {'num_agent_train_steps_per_iter': 5000,\n",
       "  'train_batch_size': 1000,\n",
       "  'size': 256,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=5000-tb_size=1000-size=256'},\n",
       " {'num_agent_train_steps_per_iter': 10000,\n",
       "  'train_batch_size': 200,\n",
       "  'size': 64,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=10000-tb_size=200-size=64'},\n",
       " {'num_agent_train_steps_per_iter': 10000,\n",
       "  'train_batch_size': 200,\n",
       "  'size': 128,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=10000-tb_size=200-size=128'},\n",
       " {'num_agent_train_steps_per_iter': 10000,\n",
       "  'train_batch_size': 200,\n",
       "  'size': 256,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=10000-tb_size=200-size=256'},\n",
       " {'num_agent_train_steps_per_iter': 10000,\n",
       "  'train_batch_size': 500,\n",
       "  'size': 64,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=10000-tb_size=500-size=64'},\n",
       " {'num_agent_train_steps_per_iter': 10000,\n",
       "  'train_batch_size': 500,\n",
       "  'size': 128,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=10000-tb_size=500-size=128'},\n",
       " {'num_agent_train_steps_per_iter': 10000,\n",
       "  'train_batch_size': 500,\n",
       "  'size': 256,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=10000-tb_size=500-size=256'},\n",
       " {'num_agent_train_steps_per_iter': 10000,\n",
       "  'train_batch_size': 1000,\n",
       "  'size': 64,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=10000-tb_size=1000-size=64'},\n",
       " {'num_agent_train_steps_per_iter': 10000,\n",
       "  'train_batch_size': 1000,\n",
       "  'size': 128,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=10000-tb_size=1000-size=128'},\n",
       " {'num_agent_train_steps_per_iter': 10000,\n",
       "  'train_batch_size': 1000,\n",
       "  'size': 256,\n",
       "  'video_log_freq': -1,\n",
       "  'exp_name': 't_step=10000-tb_size=1000-size=256'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do grid search\n",
    "from itertools import product\n",
    "\n",
    "params = {\n",
    "    'num_agent_train_steps_per_iter': [1000, 5000, 10000],\n",
    "    'train_batch_size': [200, 500, 1000],\n",
    "    'size': [64, 128, 256],\n",
    "    'video_log_freq': [-1],\n",
    "}\n",
    "\n",
    "grid_params = [dict(zip(params, vals)) for vals in product(*params.values())]\n",
    "\n",
    "# set exp_name by parameter combination\n",
    "for param in grid_params:\n",
    "    param['exp_name'] = f\"t_step={param['num_agent_train_steps_per_iter']}-tb_size={param['train_batch_size']}-size={param['size']}\"\n",
    "\n",
    "grid_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qQb789_syt0"
   },
   "outputs": [],
   "source": [
    "# ## run training\n",
    "# print(args.logdir)\n",
    "# run_training_loop(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=1000-tb_size=200-size=64', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=1000-tb_size=200-size=64_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=1000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=1000-tb_size=200-size=64_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval_AverageReturn : 1503.58984375\n",
      "Eval_StdReturn : 515.1749267578125\n",
      "Eval_MaxReturn : 2144.9677734375\n",
      "Eval_MinReturn : 696.6118774414062\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.03301353007555008\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 2.8112382888793945\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=1000-tb_size=200-size=128', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=1000-tb_size=200-size=128_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=1000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=128', 'train_batch_size=200', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=1000-tb_size=200-size=128_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\tensorboardX\\summary.py:153: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  scalar = float(scalar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1274.5731201171875\n",
      "Eval_StdReturn : 641.5538330078125\n",
      "Eval_MaxReturn : 2241.037353515625\n",
      "Eval_MinReturn : 546.1234130859375\n",
      "Eval_AverageEpLen : 855.2857142857143\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.03430361673235893\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 3.5667097568511963\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=1000-tb_size=200-size=256', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=1000-tb_size=200-size=256_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=1000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=256', 'train_batch_size=200', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=1000-tb_size=200-size=256_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1995.3675537109375\n",
      "Eval_StdReturn : 751.0162963867188\n",
      "Eval_MaxReturn : 2918.705322265625\n",
      "Eval_MinReturn : 1085.1279296875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.034015968441963196\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 3.5541224479675293\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=1000-tb_size=500-size=64', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=1000-tb_size=500-size=64_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=1000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=500', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=1000-tb_size=500-size=64_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2530.023681640625\n",
      "Eval_StdReturn : 859.6403198242188\n",
      "Eval_MaxReturn : 3303.13818359375\n",
      "Eval_MinReturn : 798.5865478515625\n",
      "Eval_AverageEpLen : 877.6666666666666\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.033328283578157425\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 3.1322336196899414\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=1000-tb_size=500-size=128', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=1000-tb_size=500-size=128_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=1000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=128', 'train_batch_size=500', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=1000-tb_size=500-size=128_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2532.98388671875\n",
      "Eval_StdReturn : 815.5120849609375\n",
      "Eval_MaxReturn : 3466.814208984375\n",
      "Eval_MinReturn : 1229.54150390625\n",
      "Eval_AverageEpLen : 805.2857142857143\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.03326449915766716\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 3.6553311347961426\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=1000-tb_size=500-size=256', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=1000-tb_size=500-size=256_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=1000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=256', 'train_batch_size=500', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=1000-tb_size=500-size=256_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2225.140869140625\n",
      "Eval_StdReturn : 975.3052368164062\n",
      "Eval_MaxReturn : 3437.431640625\n",
      "Eval_MinReturn : 457.5809020996094\n",
      "Eval_AverageEpLen : 919.6666666666666\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.03405720368027687\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 4.3460376262664795\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=1000-tb_size=1000-size=64', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=1000-tb_size=1000-size=64_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=1000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=1000', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=1000-tb_size=1000-size=64_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2221.93701171875\n",
      "Eval_StdReturn : 1144.3944091796875\n",
      "Eval_MaxReturn : 3537.630859375\n",
      "Eval_MinReturn : 82.42831420898438\n",
      "Eval_AverageEpLen : 734.625\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0333610400557518\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 3.5672810077667236\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=1000-tb_size=1000-size=128', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=1000-tb_size=1000-size=128_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=1000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=128', 'train_batch_size=1000', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=1000-tb_size=1000-size=128_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2531.06689453125\n",
      "Eval_StdReturn : 893.7138671875\n",
      "Eval_MaxReturn : 3460.85009765625\n",
      "Eval_MinReturn : 716.4088134765625\n",
      "Eval_AverageEpLen : 814.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.03201372176408768\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 4.247661828994751\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=1000-tb_size=1000-size=256', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=1000-tb_size=1000-size=256_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=1000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=256', 'train_batch_size=1000', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=1000-tb_size=1000-size=256_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3352.595703125\n",
      "Eval_StdReturn : 180.70071411132812\n",
      "Eval_MaxReturn : 3537.4228515625\n",
      "Eval_MinReturn : 3091.8974609375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.03288381174206734\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 5.193329095840454\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=5000-tb_size=200-size=64', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=5000-tb_size=200-size=64_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=5000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=5000-tb_size=200-size=64_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4680.45458984375\n",
      "Eval_StdReturn : 64.55854797363281\n",
      "Eval_MaxReturn : 4754.46875\n",
      "Eval_MinReturn : 4576.509765625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0016654337523505092\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 6.124222993850708\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=5000-tb_size=200-size=128', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=5000-tb_size=200-size=128_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=5000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=128', 'train_batch_size=200', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=5000-tb_size=200-size=128_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4628.8134765625\n",
      "Eval_StdReturn : 127.12939453125\n",
      "Eval_MaxReturn : 4818.9931640625\n",
      "Eval_MinReturn : 4477.35693359375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0017569506308063865\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 7.174170017242432\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=5000-tb_size=200-size=256', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=5000-tb_size=200-size=256_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=5000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=256', 'train_batch_size=200', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=5000-tb_size=200-size=256_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4633.27392578125\n",
      "Eval_StdReturn : 54.716400146484375\n",
      "Eval_MaxReturn : 4690.2509765625\n",
      "Eval_MinReturn : 4537.73779296875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0018524151528254151\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 9.235991477966309\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=5000-tb_size=500-size=64', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=5000-tb_size=500-size=64_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=5000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=500', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=5000-tb_size=500-size=64_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4413.8408203125\n",
      "Eval_StdReturn : 436.40692138671875\n",
      "Eval_MaxReturn : 4700.390625\n",
      "Eval_MinReturn : 3547.38916015625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0016539612552151084\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 7.857207298278809\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=5000-tb_size=500-size=128', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=5000-tb_size=500-size=128_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=5000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=128', 'train_batch_size=500', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=5000-tb_size=500-size=128_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4613.10595703125\n",
      "Eval_StdReturn : 47.92306900024414\n",
      "Eval_MaxReturn : 4684.7109375\n",
      "Eval_MinReturn : 4551.087890625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0015490279765799642\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 8.782329320907593\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=5000-tb_size=500-size=256', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=5000-tb_size=500-size=256_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=5000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=256', 'train_batch_size=500', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=5000-tb_size=500-size=256_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4665.1728515625\n",
      "Eval_StdReturn : 72.72559356689453\n",
      "Eval_MaxReturn : 4776.3232421875\n",
      "Eval_MinReturn : 4547.68505859375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0015312617179006338\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 12.268126249313354\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=5000-tb_size=1000-size=64', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=5000-tb_size=1000-size=64_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=5000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=1000', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=5000-tb_size=1000-size=64_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4618.63232421875\n",
      "Eval_StdReturn : 61.32685852050781\n",
      "Eval_MaxReturn : 4680.7021484375\n",
      "Eval_MinReturn : 4510.71240234375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0014838846400380135\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 9.076753377914429\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=5000-tb_size=1000-size=128', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=5000-tb_size=1000-size=128_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=5000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=128', 'train_batch_size=1000', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=5000-tb_size=1000-size=128_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4380.01416015625\n",
      "Eval_StdReturn : 968.0684814453125\n",
      "Eval_MaxReturn : 5071.2783203125\n",
      "Eval_MinReturn : 2236.35888671875\n",
      "Eval_AverageEpLen : 915.5\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.001496700569987297\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 11.806089878082275\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=5000-tb_size=1000-size=256', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=5000-tb_size=1000-size=256_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=5000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=256', 'train_batch_size=1000', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=5000-tb_size=1000-size=256_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4657.48876953125\n",
      "Eval_StdReturn : 38.898948669433594\n",
      "Eval_MaxReturn : 4699.08154296875\n",
      "Eval_MinReturn : 4600.9033203125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0014325237134471536\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 17.967852115631104\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=10000-tb_size=200-size=64', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=10000-tb_size=200-size=64_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=10000-tb_size=200-size=64_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4715.142578125\n",
      "Eval_StdReturn : 20.994335174560547\n",
      "Eval_MaxReturn : 4748.7392578125\n",
      "Eval_MinReturn : 4685.734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0003112778067588806\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 10.285630941390991\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=10000-tb_size=200-size=128', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=10000-tb_size=200-size=128_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=128', 'train_batch_size=200', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=10000-tb_size=200-size=128_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4630.0986328125\n",
      "Eval_StdReturn : 104.50263977050781\n",
      "Eval_MaxReturn : 4807.37353515625\n",
      "Eval_MinReturn : 4534.857421875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0008699632599018514\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 12.532643556594849\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=10000-tb_size=200-size=256', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=10000-tb_size=200-size=256_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=256', 'train_batch_size=200', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=10000-tb_size=200-size=256_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 279.6523132324219\n",
      "Eval_StdReturn : 217.4387664794922\n",
      "Eval_MaxReturn : 767.4044189453125\n",
      "Eval_MinReturn : 12.689227104187012\n",
      "Eval_AverageEpLen : 279.22222222222223\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.009376483969390392\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 16.965144395828247\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=10000-tb_size=500-size=64', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=10000-tb_size=500-size=64_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=500', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=10000-tb_size=500-size=64_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4740.4541015625\n",
      "Eval_StdReturn : 24.29452133178711\n",
      "Eval_MaxReturn : 4771.39453125\n",
      "Eval_MinReturn : 4699.71826171875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0002143940655514598\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 13.478424310684204\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=10000-tb_size=500-size=128', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=10000-tb_size=500-size=128_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=128', 'train_batch_size=500', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=10000-tb_size=500-size=128_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4571.2138671875\n",
      "Eval_StdReturn : 97.43960571289062\n",
      "Eval_MaxReturn : 4685.0205078125\n",
      "Eval_MinReturn : 4452.4248046875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0004275818064343184\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 15.78486943244934\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=10000-tb_size=500-size=256', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=10000-tb_size=500-size=256_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=256', 'train_batch_size=500', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=10000-tb_size=500-size=256_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2325.844482421875\n",
      "Eval_StdReturn : 1239.794677734375\n",
      "Eval_MaxReturn : 3625.97509765625\n",
      "Eval_MinReturn : 322.49530029296875\n",
      "Eval_AverageEpLen : 716.7142857142857\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.004396993201225996\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 23.362157821655273\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=10000-tb_size=1000-size=64', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=10000-tb_size=1000-size=64_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=1000', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=10000-tb_size=1000-size=64_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4648.1806640625\n",
      "Eval_StdReturn : 145.96597290039062\n",
      "Eval_MaxReturn : 4931.095703125\n",
      "Eval_MinReturn : 4526.107421875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0002381536178290844\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 18.42623472213745\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=10000-tb_size=1000-size=128', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=10000-tb_size=1000-size=128_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=128', 'train_batch_size=1000', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=10000-tb_size=1000-size=128_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4060.51171875\n",
      "Eval_StdReturn : 1409.939208984375\n",
      "Eval_MaxReturn : 4776.611328125\n",
      "Eval_MinReturn : 910.4541625976562\n",
      "Eval_AverageEpLen : 868.8333333333334\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0002606819325592369\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 21.924498558044434\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=t_step=10000-tb_size=1000-size=256', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-2\\\\q1_t_step=10000-tb_size=1000-size=256_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=256', 'train_batch_size=1000', 'video_log_freq=-1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-2\\q1_t_step=10000-tb_size=1000-size=256_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3632.422607421875\n",
      "Eval_StdReturn : 263.2212219238281\n",
      "Eval_MaxReturn : 4114.150390625\n",
      "Eval_MinReturn : 3366.26708984375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.002681590151041746\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 34.71767568588257\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# run training with grid parameters\n",
    "for param in grid_params:\n",
    "    for p in param:\n",
    "        args[p] = param[p]\n",
    "    create_log_dir(args, part='3-2')\n",
    "    run_training_loop(args)\n",
    "    print('*' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (3.1) Run behavioral cloning (BC) and report results on two tasks: one where a behavioral cloning agent should achieve at least 30% of the performance of the expert, and one environment of your choosing where it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the results, I select one set of combination to apply and train the agent on all four tasks. The set of hyperparameters is:\n",
    "* num_agent_train_steps_per_iter = 10000 \n",
    "* train_batch_size = 200\n",
    "* size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args['num_agent_train_steps_per_iter'] = 10000\n",
    "args['train_batch_size'] = 200\n",
    "args['size'] = 64\n",
    "args['video_log_freq'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_Ant = {\n",
    "    'expert_policy_file': '../../cs285/policies/experts/Ant.pkl',\n",
    "    'expert_data': '../../cs285/expert_data/expert_data_Ant-v4.pkl',\n",
    "    'env_name': 'Ant-v4',\n",
    "    'exp_name': 'bc_ant',\n",
    "}\n",
    "\n",
    "for k in param_Ant:\n",
    "    args[k] = param_Ant[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=bc_ant', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-1\\\\q1_bc_ant_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-1\\q1_bc_ant_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4749.41015625\n",
      "Eval_StdReturn : 59.842185974121094\n",
      "Eval_MaxReturn : 4807.9140625\n",
      "Eval_MinReturn : 4649.9013671875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00029027400887571275\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 45.74547266960144\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run training on Ant\n",
    "create_log_dir(args, part='3-1')\n",
    "run_training_loop(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_HalfCheetah = {\n",
    "    'expert_policy_file': '../../cs285/policies/experts/HalfCheetah.pkl',\n",
    "    'expert_data': '../../cs285/expert_data/expert_data_HalfCheetah-v4.pkl',\n",
    "    'env_name': 'HalfCheetah-v4',\n",
    "    'exp_name': 'bc_halfcheetah',\n",
    "}\n",
    "\n",
    "for k in param_HalfCheetah:\n",
    "    args[k] = param_HalfCheetah[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=HalfCheetah-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=bc_halfcheetah', 'expert_data=../../cs285/expert_data/expert_data_HalfCheetah-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/HalfCheetah.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-1\\\\q1_bc_halfcheetah_HalfCheetah-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-1\\q1_bc_halfcheetah_HalfCheetah-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4080.100341796875\n",
      "Eval_StdReturn : 58.103641510009766\n",
      "Eval_MaxReturn : 4181.392578125\n",
      "Eval_MinReturn : 4019.67529296875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4034.7999834965067\n",
      "Train_StdReturn : 32.8677631311341\n",
      "Train_MaxReturn : 4067.6677466276406\n",
      "Train_MinReturn : 4001.9322203653724\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0007307063206098974\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 40.932074546813965\n",
      "Initial_DataCollection_AverageReturn : 4034.7999834965067\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run training on HalfCheetah\n",
    "create_log_dir(args, part='3-1')\n",
    "run_training_loop(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_Hopper = {\n",
    "    'expert_policy_file': '../../cs285/policies/experts/Hopper.pkl',\n",
    "    'expert_data': '../../cs285/expert_data/expert_data_Hopper-v4.pkl',\n",
    "    'env_name': 'Hopper-v4',\n",
    "    'exp_name': 'bc_hopper',\n",
    "}\n",
    "\n",
    "for k in param_Hopper:\n",
    "    args[k] = param_Hopper[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Hopper-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=bc_hopper', 'expert_data=../../cs285/expert_data/expert_data_Hopper-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Hopper.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-1\\\\q1_bc_hopper_Hopper-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-1\\q1_bc_hopper_Hopper-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2664.45654296875\n",
      "Eval_StdReturn : 714.4451293945312\n",
      "Eval_MaxReturn : 3287.18896484375\n",
      "Eval_MinReturn : 1402.0440673828125\n",
      "Eval_AverageEpLen : 764.8571428571429\n",
      "Train_AverageReturn : 3717.5129936182307\n",
      "Train_StdReturn : 0.3530361779417035\n",
      "Train_MaxReturn : 3717.8660297961724\n",
      "Train_MinReturn : 3717.159957440289\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0005491412593983114\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 44.841530561447144\n",
      "Initial_DataCollection_AverageReturn : 3717.5129936182307\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run training on Hopper\n",
    "create_log_dir(args, part='3-1')\n",
    "run_training_loop(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_Walker2d = {\n",
    "    'expert_policy_file': '../../cs285/policies/experts/Walker2d.pkl',\n",
    "    'expert_data': '../../cs285/expert_data/expert_data_Walker2d-v4.pkl',\n",
    "    'env_name': 'Walker2d-v4',\n",
    "    'exp_name': 'bc_walker2d',\n",
    "}\n",
    "\n",
    "for k in param_Walker2d:\n",
    "    args[k] = param_Walker2d[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=False', 'env_name=Walker2d-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=bc_walker2d', 'expert_data=../../cs285/expert_data/expert_data_Walker2d-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Walker2d.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\3-1\\\\q1_bc_walker2d_Walker2d-v4', 'max_replay_buffer_size=1000000', 'n_iter=1', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=1', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\3-1\\q1_bc_walker2d_Walker2d-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Walker2d.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 5266.7783203125\n",
      "Eval_StdReturn : 35.68962860107422\n",
      "Eval_MaxReturn : 5324.98583984375\n",
      "Eval_MinReturn : 5213.6298828125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 5383.310325177668\n",
      "Train_StdReturn : 54.15251563871789\n",
      "Train_MaxReturn : 5437.462840816386\n",
      "Train_MinReturn : 5329.1578095389505\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.002182147465646267\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 42.18763709068298\n",
      "Initial_DataCollection_AverageReturn : 5383.310325177668\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run training on Walker2d\n",
    "create_log_dir(args, part='3-1')\n",
    "run_training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\n",
    "|                              | Ant               | HalfCheetah       | Hopper            | Walker2d          |\n",
    "| ---------------------------- | ----------------- | ----------------- | ----------------- | ----------------- |\n",
    "| AverageReturn (Train / Eval) | 4681.89 / 4749.41 | 4034.79 / 4080.10 | 3717.51 / 2664.45 | 5383.31 / 5266.77 |\n",
    "| StdReturn (Train / Eval)     | 30.70 / 59.84     | 32.86 / 58.10     | 0.35 / 714.44     | 54.15 / 35.68     |\n",
    "\n",
    "It looks like four tasks all achieve at least 30% of the performance of the expert(=Train). ï¼š)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "75M0MlR5tUIb"
   },
   "outputs": [],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir /content/cs285_f2023/hw1/data\n",
    "# %tensorboard --logdir ../../data/3-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../../data/3-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff9onuUPfPEa"
   },
   "source": [
    "## Running DAgger (Problem 2)\n",
    "Modify the settings above:\n",
    "1. check the `do_dagger` box\n",
    "2. set `n_iters` to `10`\n",
    "3. set `exp_name` to `dagger_{env_name}`\n",
    "and then rerun the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (4.2) Run DAgger and report results on the two tasks you tested previously with behavioral cloning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same hyperparameters used in Behavior Cloning, and train on all four tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args['expert_policy_file'] = '../../cs285/policies/experts/Ant.pkl'\n",
    "args['expert_data'] = '../../cs285/expert_data/expert_data_Ant-v4.pkl'\n",
    "args['env_name'] = 'Ant-v4'\n",
    "args['exp_name'] = 'dagger_ant'\n",
    "\n",
    "# hyperparameters for DAgger\n",
    "args['do_dagger'] = True\n",
    "args['n_iter'] = 10\n",
    "\n",
    "# the same as Behavior Cloning; the others are as default\n",
    "args['num_agent_train_steps_per_iter'] = 10000\n",
    "args['train_batch_size'] = 200\n",
    "args['size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=True', 'env_name=Ant-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=dagger_ant', 'expert_data=../../cs285/expert_data/expert_data_Ant-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Ant.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\4-2\\\\q2_dagger_ant_Ant-v4', 'max_replay_buffer_size=1000000', 'n_iter=10', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=5', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\4-2\\q2_dagger_ant_Ant-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval_AverageReturn : 4767.443359375\n",
      "Eval_StdReturn : 30.58685874938965\n",
      "Eval_MaxReturn : 4818.509765625\n",
      "Eval_MinReturn : 4727.31982421875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4681.891673935816\n",
      "Train_StdReturn : 30.70862278765526\n",
      "Train_MaxReturn : 4712.600296723471\n",
      "Train_MinReturn : 4651.18305114816\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0003318272647447884\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 11.092876672744751\n",
      "Initial_DataCollection_AverageReturn : 4681.891673935816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\tensorboardX\\summary.py:153: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  scalar = float(scalar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4588.5986328125\n",
      "Eval_StdReturn : 83.02816009521484\n",
      "Eval_MaxReturn : 4708.1083984375\n",
      "Eval_MinReturn : 4482.7734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4774.0146484375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4774.0146484375\n",
      "Train_MinReturn : 4774.0146484375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00013036698510404676\n",
      "Train_EnvstepsSoFar : 1000\n",
      "TimeSinceStart : 22.77952289581299\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4729.0380859375\n",
      "Eval_StdReturn : 84.97787475585938\n",
      "Eval_MaxReturn : 4838.45654296875\n",
      "Eval_MinReturn : 4577.7060546875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4624.1484375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4624.1484375\n",
      "Train_MinReturn : 4624.1484375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00011926922888960689\n",
      "Train_EnvstepsSoFar : 2000\n",
      "TimeSinceStart : 34.311978578567505\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4756.96826171875\n",
      "Eval_StdReturn : 117.7965316772461\n",
      "Eval_MaxReturn : 4935.24755859375\n",
      "Eval_MinReturn : 4566.2919921875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4656.521484375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4656.521484375\n",
      "Train_MinReturn : 4656.521484375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00014126545283943415\n",
      "Train_EnvstepsSoFar : 3000\n",
      "TimeSinceStart : 45.859909772872925\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4732.171875\n",
      "Eval_StdReturn : 42.655364990234375\n",
      "Eval_MaxReturn : 4792.9619140625\n",
      "Eval_MinReturn : 4676.5458984375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4901.9560546875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4901.9560546875\n",
      "Train_MinReturn : 4901.9560546875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00011696351430146024\n",
      "Train_EnvstepsSoFar : 4000\n",
      "TimeSinceStart : 87.47571349143982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4751.0478515625\n",
      "Eval_StdReturn : 94.81939697265625\n",
      "Eval_MaxReturn : 4893.2666015625\n",
      "Eval_MinReturn : 4619.4677734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4795.8134765625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4795.8134765625\n",
      "Train_MinReturn : 4795.8134765625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00012447693734429777\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 99.41582107543945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4760.55615234375\n",
      "Eval_StdReturn : 34.99757766723633\n",
      "Eval_MaxReturn : 4806.056640625\n",
      "Eval_MinReturn : 4717.6533203125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4578.8076171875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4578.8076171875\n",
      "Train_MinReturn : 4578.8076171875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00012399509432725608\n",
      "Train_EnvstepsSoFar : 6000\n",
      "TimeSinceStart : 111.1764543056488\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4744.70556640625\n",
      "Eval_StdReturn : 154.10304260253906\n",
      "Eval_MaxReturn : 5018.494140625\n",
      "Eval_MinReturn : 4559.5810546875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4760.31787109375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4760.31787109375\n",
      "Train_MinReturn : 4760.31787109375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00010174274211749434\n",
      "Train_EnvstepsSoFar : 7000\n",
      "TimeSinceStart : 122.82350635528564\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4774.390625\n",
      "Eval_StdReturn : 83.60476684570312\n",
      "Eval_MaxReturn : 4857.265625\n",
      "Eval_MinReturn : 4637.001953125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4918.447265625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4918.447265625\n",
      "Train_MinReturn : 4918.447265625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00011033528426196426\n",
      "Train_EnvstepsSoFar : 8000\n",
      "TimeSinceStart : 134.95436239242554\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4660.49169921875\n",
      "Eval_StdReturn : 72.7898941040039\n",
      "Eval_MaxReturn : 4770.11767578125\n",
      "Eval_MinReturn : 4549.302734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4844.5439453125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4844.5439453125\n",
      "Train_MinReturn : 4844.5439453125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 9.893398964777589e-05\n",
      "Train_EnvstepsSoFar : 9000\n",
      "TimeSinceStart : 179.81302070617676\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run training on Ant\n",
    "create_log_dir(args, part='4-2')\n",
    "run_training_loop(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args['expert_policy_file'] = '../../cs285/policies/experts/HalfCheetah.pkl'\n",
    "args['expert_data'] = '../../cs285/expert_data/expert_data_HalfCheetah-v4.pkl'\n",
    "args['env_name'] = 'HalfCheetah-v4'\n",
    "args['exp_name'] = 'dagger_halfcheetah'\n",
    "\n",
    "# hyperparameters for DAgger\n",
    "args['do_dagger'] = True\n",
    "args['n_iter'] = 10\n",
    "\n",
    "# the same as Behavior Cloning; the others are as default\n",
    "args['num_agent_train_steps_per_iter'] = 10000\n",
    "args['train_batch_size'] = 200\n",
    "args['size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=True', 'env_name=HalfCheetah-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=dagger_halfcheetah', 'expert_data=../../cs285/expert_data/expert_data_HalfCheetah-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/HalfCheetah.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\4-2\\\\q2_dagger_halfcheetah_HalfCheetah-v4', 'max_replay_buffer_size=1000000', 'n_iter=10', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=5', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\4-2\\q2_dagger_halfcheetah_HalfCheetah-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4082.01611328125\n",
      "Eval_StdReturn : 44.430091857910156\n",
      "Eval_MaxReturn : 4124.9462890625\n",
      "Eval_MinReturn : 4002.56640625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4034.7999834965067\n",
      "Train_StdReturn : 32.8677631311341\n",
      "Train_MaxReturn : 4067.6677466276406\n",
      "Train_MinReturn : 4001.9322203653724\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0006747972802259028\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 10.442224979400635\n",
      "Initial_DataCollection_AverageReturn : 4034.7999834965067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4134.03759765625\n",
      "Eval_StdReturn : 90.16998291015625\n",
      "Eval_MaxReturn : 4230.548828125\n",
      "Eval_MinReturn : 3963.9716796875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3924.31201171875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3924.31201171875\n",
      "Train_MinReturn : 3924.31201171875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00046199376811273396\n",
      "Train_EnvstepsSoFar : 1000\n",
      "TimeSinceStart : 20.527252197265625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4056.150390625\n",
      "Eval_StdReturn : 87.14698791503906\n",
      "Eval_MaxReturn : 4153.595703125\n",
      "Eval_MinReturn : 3904.82470703125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4122.90234375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4122.90234375\n",
      "Train_MinReturn : 4122.90234375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0004893813747912645\n",
      "Train_EnvstepsSoFar : 2000\n",
      "TimeSinceStart : 30.347774744033813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3990.39501953125\n",
      "Eval_StdReturn : 63.37395477294922\n",
      "Eval_MaxReturn : 4077.5205078125\n",
      "Eval_MinReturn : 3886.13916015625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3869.205078125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3869.205078125\n",
      "Train_MinReturn : 3869.205078125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0005880955141037703\n",
      "Train_EnvstepsSoFar : 3000\n",
      "TimeSinceStart : 40.74598813056946\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4078.128173828125\n",
      "Eval_StdReturn : 32.231082916259766\n",
      "Eval_MaxReturn : 4136.26806640625\n",
      "Eval_MinReturn : 4046.839111328125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4071.13427734375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4071.13427734375\n",
      "Train_MinReturn : 4071.13427734375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0006050645606592298\n",
      "Train_EnvstepsSoFar : 4000\n",
      "TimeSinceStart : 80.00006580352783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4028.710205078125\n",
      "Eval_StdReturn : 38.22739791870117\n",
      "Eval_MaxReturn : 4074.0810546875\n",
      "Eval_MinReturn : 3963.2314453125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4045.044677734375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4045.044677734375\n",
      "Train_MinReturn : 4045.044677734375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00046896954881958663\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 89.76571106910706\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4044.61474609375\n",
      "Eval_StdReturn : 58.11669158935547\n",
      "Eval_MaxReturn : 4152.208984375\n",
      "Eval_MinReturn : 3983.27783203125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4219.66162109375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4219.66162109375\n",
      "Train_MinReturn : 4219.66162109375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0005044312565587461\n",
      "Train_EnvstepsSoFar : 6000\n",
      "TimeSinceStart : 99.31842255592346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4111.34619140625\n",
      "Eval_StdReturn : 94.35130310058594\n",
      "Eval_MaxReturn : 4192.03955078125\n",
      "Eval_MinReturn : 3928.34423828125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4083.001953125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4083.001953125\n",
      "Train_MinReturn : 4083.001953125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.000304627901641652\n",
      "Train_EnvstepsSoFar : 7000\n",
      "TimeSinceStart : 109.08011627197266\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4066.549560546875\n",
      "Eval_StdReturn : 80.12073516845703\n",
      "Eval_MaxReturn : 4179.0966796875\n",
      "Eval_MinReturn : 3956.999267578125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4069.320556640625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4069.320556640625\n",
      "Train_MinReturn : 4069.320556640625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0004270173085387796\n",
      "Train_EnvstepsSoFar : 8000\n",
      "TimeSinceStart : 118.76601791381836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4117.9462890625\n",
      "Eval_StdReturn : 71.78826904296875\n",
      "Eval_MaxReturn : 4198.55322265625\n",
      "Eval_MinReturn : 3996.178955078125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4136.015625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4136.015625\n",
      "Train_MinReturn : 4136.015625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.000672779802698642\n",
      "Train_EnvstepsSoFar : 9000\n",
      "TimeSinceStart : 157.0273470878601\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run training on HalfCheetah\n",
    "create_log_dir(args, part='4-2')\n",
    "run_training_loop(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args['expert_policy_file'] = '../../cs285/policies/experts/Hopper.pkl'\n",
    "args['expert_data'] = '../../cs285/expert_data/expert_data_Hopper-v4.pkl'\n",
    "args['env_name'] = 'Hopper-v4'\n",
    "args['exp_name'] = 'dagger_hopper'\n",
    "\n",
    "# hyperparameters for DAgger\n",
    "args['do_dagger'] = True\n",
    "args['n_iter'] = 10\n",
    "\n",
    "# the same as Behavior Cloning; the others are as default\n",
    "args['num_agent_train_steps_per_iter'] = 10000\n",
    "args['train_batch_size'] = 200\n",
    "args['size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=True', 'env_name=Hopper-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=dagger_hopper', 'expert_data=../../cs285/expert_data/expert_data_Hopper-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Hopper.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\4-2\\\\q2_dagger_hopper_Hopper-v4', 'max_replay_buffer_size=1000000', 'n_iter=10', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=5', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\4-2\\q2_dagger_hopper_Hopper-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3642.48291015625\n",
      "Eval_StdReturn : 121.13954162597656\n",
      "Eval_MaxReturn : 3711.84375\n",
      "Eval_MinReturn : 3400.86767578125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3717.5129936182307\n",
      "Train_StdReturn : 0.3530361779417035\n",
      "Train_MaxReturn : 3717.8660297961724\n",
      "Train_MinReturn : 3717.159957440289\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00038724878686480224\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 9.459464073181152\n",
      "Initial_DataCollection_AverageReturn : 3717.5129936182307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3727.60693359375\n",
      "Eval_StdReturn : 5.092636585235596\n",
      "Eval_MaxReturn : 3735.3681640625\n",
      "Eval_MinReturn : 3720.54150390625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3712.056640625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3712.056640625\n",
      "Train_MinReturn : 3712.056640625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0004415930307004601\n",
      "Train_EnvstepsSoFar : 1000\n",
      "TimeSinceStart : 19.043670892715454\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3710.823486328125\n",
      "Eval_StdReturn : 3.313716173171997\n",
      "Eval_MaxReturn : 3715.41015625\n",
      "Eval_MinReturn : 3705.6572265625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3725.68359375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3725.68359375\n",
      "Train_MinReturn : 3725.68359375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0005541928112506866\n",
      "Train_EnvstepsSoFar : 2000\n",
      "TimeSinceStart : 28.596972703933716\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3715.272705078125\n",
      "Eval_StdReturn : 2.1171376705169678\n",
      "Eval_MaxReturn : 3718.69091796875\n",
      "Eval_MinReturn : 3712.162109375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3712.586181640625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3712.586181640625\n",
      "Train_MinReturn : 3712.586181640625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0005645487108267844\n",
      "Train_EnvstepsSoFar : 3000\n",
      "TimeSinceStart : 38.36814260482788\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3723.610595703125\n",
      "Eval_StdReturn : 2.074644088745117\n",
      "Eval_MaxReturn : 3726.866943359375\n",
      "Eval_MinReturn : 3721.64453125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3721.851318359375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3721.851318359375\n",
      "Train_MinReturn : 3721.851318359375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00034794831299223006\n",
      "Train_EnvstepsSoFar : 4000\n",
      "TimeSinceStart : 83.45724201202393\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3714.36474609375\n",
      "Eval_StdReturn : 3.1528615951538086\n",
      "Eval_MaxReturn : 3717.346923828125\n",
      "Eval_MinReturn : 3708.525146484375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3725.86962890625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3725.86962890625\n",
      "Train_MinReturn : 3725.86962890625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00042601852328516543\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 92.97914242744446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3720.093017578125\n",
      "Eval_StdReturn : 5.252020359039307\n",
      "Eval_MaxReturn : 3729.56201171875\n",
      "Eval_MinReturn : 3715.3408203125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3720.603759765625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3720.603759765625\n",
      "Train_MinReturn : 3720.603759765625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0005833947798237205\n",
      "Train_EnvstepsSoFar : 6000\n",
      "TimeSinceStart : 102.34407043457031\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3721.596923828125\n",
      "Eval_StdReturn : 4.175520896911621\n",
      "Eval_MaxReturn : 3729.199951171875\n",
      "Eval_MinReturn : 3717.39013671875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3723.03173828125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3723.03173828125\n",
      "Train_MinReturn : 3723.03173828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0003748805320356041\n",
      "Train_EnvstepsSoFar : 7000\n",
      "TimeSinceStart : 111.88128733634949\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3718.253173828125\n",
      "Eval_StdReturn : 0.7381013035774231\n",
      "Eval_MaxReturn : 3719.302001953125\n",
      "Eval_MinReturn : 3717.0986328125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3719.2978515625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3719.2978515625\n",
      "Train_MinReturn : 3719.2978515625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00029777915915474296\n",
      "Train_EnvstepsSoFar : 8000\n",
      "TimeSinceStart : 121.46538710594177\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3713.668701171875\n",
      "Eval_StdReturn : 1.7745980024337769\n",
      "Eval_MaxReturn : 3716.28466796875\n",
      "Eval_MinReturn : 3711.21875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3716.81396484375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3716.81396484375\n",
      "Train_MinReturn : 3716.81396484375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.00044638453982770443\n",
      "Train_EnvstepsSoFar : 9000\n",
      "TimeSinceStart : 165.9042432308197\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run training on Hopper\n",
    "create_log_dir(args, part='4-2')\n",
    "run_training_loop(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args['expert_policy_file'] = '../../cs285/policies/experts/Walker2d.pkl'\n",
    "args['expert_data'] = '../../cs285/expert_data/expert_data_Walker2d-v4.pkl'\n",
    "args['env_name'] = 'Walker2d-v4'\n",
    "args['exp_name'] = 'dagger_walker2d'\n",
    "\n",
    "# hyperparameters for DAgger\n",
    "args['do_dagger'] = True\n",
    "args['n_iter'] = 10\n",
    "\n",
    "# the same as Behavior Cloning; the others are as default\n",
    "args['num_agent_train_steps_per_iter'] = 10000\n",
    "args['train_batch_size'] = 200\n",
    "args['size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size=1000', 'batch_size_initial=2000', 'do_dagger=True', 'env_name=Walker2d-v4', 'ep_len=1000', 'eval_batch_size=5000', 'exp_name=dagger_walker2d', 'expert_data=../../cs285/expert_data/expert_data_Walker2d-v4.pkl', 'expert_policy_file=../../cs285/policies/experts/Walker2d.pkl', 'learning_rate=0.005', 'logdir=../../data\\\\4-2\\\\q2_dagger_walker2d_Walker2d-v4', 'max_replay_buffer_size=1000000', 'n_iter=10', 'n_layers=2', 'no_gpu=False', 'num_agent_train_steps_per_iter=10000', 'save_params=False', 'scalar_log_freq=1', 'seed=1', 'size=64', 'train_batch_size=200', 'video_log_freq=5', 'which_gpu=0']\n",
      "\n",
      "########################\n",
      "logging outputs to  ../../data\\4-2\\q2_dagger_walker2d_Walker2d-v4\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... ../../cs285/policies/experts/Walker2d.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval_AverageReturn : 4927.95849609375\n",
      "Eval_StdReturn : 677.58935546875\n",
      "Eval_MaxReturn : 5363.66552734375\n",
      "Eval_MinReturn : 3425.5888671875\n",
      "Eval_AverageEpLen : 950.0\n",
      "Train_AverageReturn : 5383.310325177668\n",
      "Train_StdReturn : 54.15251563871789\n",
      "Train_MaxReturn : 5437.462840816386\n",
      "Train_MinReturn : 5329.1578095389505\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.003240828402340412\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 10.413546323776245\n",
      "Initial_DataCollection_AverageReturn : 5383.310325177668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\tensorboardX\\summary.py:153: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  scalar = float(scalar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4694.85791015625\n",
      "Eval_StdReturn : 1556.56591796875\n",
      "Eval_MaxReturn : 5450.8916015625\n",
      "Eval_MinReturn : 1215.750732421875\n",
      "Eval_AverageEpLen : 883.5\n",
      "Train_AverageReturn : 5283.61474609375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 5283.61474609375\n",
      "Train_MinReturn : 5283.61474609375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0028004648629575968\n",
      "Train_EnvstepsSoFar : 1000\n",
      "TimeSinceStart : 21.13810133934021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 5327.6884765625\n",
      "Eval_StdReturn : 42.142616271972656\n",
      "Eval_MaxReturn : 5386.86083984375\n",
      "Eval_MinReturn : 5271.61279296875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 5404.24951171875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 5404.24951171875\n",
      "Train_MinReturn : 5404.24951171875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0025948467664420605\n",
      "Train_EnvstepsSoFar : 2000\n",
      "TimeSinceStart : 31.912943601608276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 5380.7275390625\n",
      "Eval_StdReturn : 41.33746337890625\n",
      "Eval_MaxReturn : 5435.2802734375\n",
      "Eval_MinReturn : 5326.6455078125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 5384.482421875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 5384.482421875\n",
      "Train_MinReturn : 5384.482421875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0021750577725470066\n",
      "Train_EnvstepsSoFar : 3000\n",
      "TimeSinceStart : 42.52363872528076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 5381.42724609375\n",
      "Eval_StdReturn : 38.432857513427734\n",
      "Eval_MaxReturn : 5447.4462890625\n",
      "Eval_MinReturn : 5334.1787109375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 5392.83740234375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 5392.83740234375\n",
      "Train_MinReturn : 5392.83740234375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0014680755557492375\n",
      "Train_EnvstepsSoFar : 4000\n",
      "TimeSinceStart : 89.11248660087585\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 5430.9033203125\n",
      "Eval_StdReturn : 20.93875503540039\n",
      "Eval_MaxReturn : 5462.21484375\n",
      "Eval_MinReturn : 5406.53369140625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 5386.66796875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 5386.66796875\n",
      "Train_MinReturn : 5386.66796875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.002107796957716346\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 100.04879760742188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4681.91943359375\n",
      "Eval_StdReturn : 1525.4539794921875\n",
      "Eval_MaxReturn : 5422.3916015625\n",
      "Eval_MinReturn : 1272.10009765625\n",
      "Eval_AverageEpLen : 884.8333333333334\n",
      "Train_AverageReturn : 5496.4052734375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 5496.4052734375\n",
      "Train_MinReturn : 5496.4052734375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0016032772837206721\n",
      "Train_EnvstepsSoFar : 6000\n",
      "TimeSinceStart : 110.71746897697449\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4828.37939453125\n",
      "Eval_StdReturn : 1115.4190673828125\n",
      "Eval_MaxReturn : 5434.87060546875\n",
      "Eval_MinReturn : 2340.16650390625\n",
      "Eval_AverageEpLen : 919.0\n",
      "Train_AverageReturn : 5293.68359375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 5293.68359375\n",
      "Train_MinReturn : 5293.68359375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.001999006839469075\n",
      "Train_EnvstepsSoFar : 7000\n",
      "TimeSinceStart : 121.91742396354675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 5372.5224609375\n",
      "Eval_StdReturn : 41.28501510620117\n",
      "Eval_MaxReturn : 5405.5068359375\n",
      "Eval_MinReturn : 5293.955078125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 5336.7822265625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 5336.7822265625\n",
      "Train_MinReturn : 5336.7822265625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0014028801815584302\n",
      "Train_EnvstepsSoFar : 8000\n",
      "TimeSinceStart : 132.9027931690216\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 5432.3193359375\n",
      "Eval_StdReturn : 32.73652267456055\n",
      "Eval_MaxReturn : 5488.00927734375\n",
      "Eval_MinReturn : 5387.29296875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 5371.935546875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 5371.935546875\n",
      "Train_MinReturn : 5371.935546875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : 0.0011145212920382619\n",
      "Train_EnvstepsSoFar : 9000\n",
      "TimeSinceStart : 177.97464609146118\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run training on Walker2d\n",
    "create_log_dir(args, part='4-2')\n",
    "run_training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the last iteration:\n",
    "|                              | Ant               | HalfCheetah       | Hopper            | Walker2d          |\n",
    "| ---------------------------- | ----------------- | ----------------- | ----------------- | ----------------- |\n",
    "| AverageReturn (Train / Eval) | 4844.54 / 4660.49 | 4136.01 / 4117.94 | 3716.81 / 3713.66 | 5371.93 / 5432.31 |\n",
    "| StdReturn (Train / Eval)     | 0.0 / 72.78       | 0.0 / 71.78       | 0.0  / 1.77       | 0.0 / 32.73       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../../data/4-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Behavior Cloning vs. DAgger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mLXw6zd-k3Xd",
    "UunygyDXrx7k"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
